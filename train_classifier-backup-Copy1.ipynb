{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef079762-b6bc-4770-bc8d-5bbb61042b77",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2712541e-ed17-4fae-a498-a962c80808fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860e1151-9f26-4fb0-9c0a-956f89ce8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_metric\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from commom import load_jsonl, save_jsonl\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    "    BertTokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorForTokenClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c824df94-f793-411f-b1ed-a656998763f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, data_list, max_length=512, model_name=\"bert-base-multilingual-cased\"):\n",
    "        self.d_list = data_list\n",
    "        self.len = len(self.d_list)\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.label2index = {\n",
    "            'SPAM': 0,\n",
    "            'EDM': 1,\n",
    "            'HAM': 2\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.d_list[index]\n",
    "        context = data['context']\n",
    "        label = data['label']\n",
    "        \n",
    "        processed_sample = dict()\n",
    "        processed_sample['labels'] = torch.tensor(self.label2index[label])\n",
    "        tokenized_input = self.tokenizer(context,\n",
    "                                         max_length=self.max_length,\n",
    "                                         padding='max_length', \n",
    "                                         truncation=True,\n",
    "                                         return_tensors=\"pt\")\n",
    "        \n",
    "        input_items = {key: val.squeeze() for key, val in tokenized_input.items()}\n",
    "        processed_sample.update(input_items)\n",
    "        return processed_sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b24c7d03-5cd9-4746-afa5-4d2f79506d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Jsonl: datasets/train.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1322it [00:00, 13219.41it/s]\u001b[A\n",
      "3001it [00:00, 15312.23it/s]\u001b[A\n",
      "4718it [00:00, 16159.26it/s]\u001b[A\n",
      "6408it [00:00, 16448.60it/s]\u001b[A\n",
      "8053it [00:00, 16057.64it/s]\u001b[A\n",
      "9661it [00:00, 16043.60it/s]\u001b[A\n",
      "11320it [00:00, 16214.45it/s]\u001b[A\n",
      "12943it [00:00, 16151.01it/s]\u001b[A\n",
      "14560it [00:00, 16156.59it/s]\u001b[A\n",
      "16186it [00:01, 16186.86it/s]\u001b[A\n",
      "17860it [00:01, 16355.03it/s]\u001b[A\n",
      "19497it [00:01, 16356.94it/s]\u001b[A\n",
      "21133it [00:01, 16084.23it/s]\u001b[A\n",
      "22798it [00:01, 16252.15it/s]\u001b[A\n",
      "24447it [00:01, 16322.04it/s]\u001b[A\n",
      "26080it [00:01, 16203.01it/s]\u001b[A\n",
      "27701it [00:01, 15938.71it/s]\u001b[A\n",
      "29297it [00:01, 15925.30it/s]\u001b[A\n",
      "30891it [00:01, 15924.30it/s]\u001b[A\n",
      "32487it [00:02, 15933.94it/s]\u001b[A\n",
      "34081it [00:02, 15588.84it/s]\u001b[A\n",
      "35765it [00:02, 15946.72it/s]\u001b[A\n",
      "37362it [00:02, 15687.84it/s]\u001b[A\n",
      "38997it [00:02, 15880.35it/s]\u001b[A\n",
      "40673it [00:02, 16135.34it/s]\u001b[A\n",
      "42289it [00:02, 16120.65it/s]\u001b[A\n",
      "43922it [00:02, 16149.04it/s]\u001b[A\n",
      "45538it [00:02, 15764.59it/s]\u001b[A\n",
      "47117it [00:02, 15770.31it/s]\u001b[A\n",
      "48696it [00:03, 15732.62it/s]\u001b[A\n",
      "50271it [00:03, 15319.11it/s]\u001b[A\n",
      "51806it [00:03, 15270.26it/s]\u001b[A\n",
      "53339it [00:03, 15285.73it/s]\u001b[A\n",
      "54869it [00:03, 15163.80it/s]\u001b[A\n",
      "56423it [00:03, 15264.35it/s]\u001b[A\n",
      "57951it [00:03, 14622.89it/s]\u001b[A\n",
      "59419it [00:03, 14607.25it/s]\u001b[A\n",
      "60974it [00:03, 14873.90it/s]\u001b[A\n",
      "62465it [00:03, 14765.82it/s]\u001b[A\n",
      "64015it [00:04, 14980.77it/s]\u001b[A\n",
      "65516it [00:04, 14968.37it/s]\u001b[A\n",
      "67123it [00:04, 15292.29it/s]\u001b[A\n",
      "68707it [00:04, 15441.51it/s]\u001b[A\n",
      "70331it [00:04, 15678.43it/s]\u001b[A\n",
      "72047it [00:04, 16119.12it/s]\u001b[A\n",
      "73660it [00:04, 15711.44it/s]\u001b[A\n",
      "75234it [00:04, 15695.96it/s]\u001b[A\n",
      "76806it [00:04, 15479.48it/s]\u001b[A\n",
      "78356it [00:05, 14834.66it/s]\u001b[A\n",
      "79904it [00:05, 15017.85it/s]\u001b[A\n",
      "81571it [00:05, 15491.65it/s]\u001b[A\n",
      "83151it [00:05, 15573.49it/s]\u001b[A\n",
      "84796it [00:05, 15831.97it/s]\u001b[A\n",
      "86383it [00:05, 15698.63it/s]\u001b[A\n",
      "87956it [00:05, 15606.93it/s]\u001b[A\n",
      "89519it [00:05, 15034.05it/s]\u001b[A\n",
      "91028it [00:05, 15042.89it/s]\u001b[A\n",
      "92536it [00:05, 14915.38it/s]\u001b[A\n",
      "94057it [00:06, 15000.38it/s]\u001b[A\n",
      "95559it [00:06, 14959.69it/s]\u001b[A\n",
      "97057it [00:06, 14708.07it/s]\u001b[A\n",
      "98530it [00:06, 14660.41it/s]\u001b[A\n",
      "100156it [00:06, 15128.34it/s]\u001b[A\n",
      "101718it [00:06, 15267.71it/s]\u001b[A\n",
      "103247it [00:06, 15184.76it/s]\u001b[A\n",
      "104767it [00:06, 15012.16it/s]\u001b[A\n",
      "106372it [00:06, 15317.62it/s]\u001b[A\n",
      "107913it [00:06, 15344.37it/s]\u001b[A\n",
      "109490it [00:07, 15470.65it/s]\u001b[A\n",
      "111057it [00:07, 15521.57it/s]\u001b[A\n",
      "112682it [00:07, 15738.96it/s]\u001b[A\n",
      "114257it [00:07, 15518.27it/s]\u001b[A\n",
      "115810it [00:07, 15389.52it/s]\u001b[A\n",
      "117403it [00:07, 15538.94it/s]\u001b[A\n",
      "118968it [00:07, 15571.47it/s]\u001b[A\n",
      "120526it [00:07, 15524.62it/s]\u001b[A\n",
      "122125it [00:07, 15662.93it/s]\u001b[A\n",
      "123692it [00:07, 15412.35it/s]\u001b[A\n",
      "125327it [00:08, 15689.02it/s]\u001b[A\n",
      "126958it [00:08, 15870.64it/s]\u001b[A\n",
      "128596it [00:08, 16022.00it/s]\u001b[A\n",
      "130200it [00:08, 15593.68it/s]\u001b[A\n",
      "131763it [00:08, 15535.27it/s]\u001b[A\n",
      "133338it [00:08, 15597.41it/s]\u001b[A\n",
      "134948it [00:08, 15745.94it/s]\u001b[A\n",
      "136524it [00:08, 15702.19it/s]\u001b[A\n",
      "138096it [00:08, 15697.95it/s]\u001b[A\n",
      "139667it [00:08, 15643.68it/s]\u001b[A\n",
      "141232it [00:09, 15588.84it/s]\u001b[A\n",
      "142797it [00:09, 15604.16it/s]\u001b[A\n",
      "144358it [00:09, 15567.47it/s]\u001b[A\n",
      "145976it [00:09, 15749.09it/s]\u001b[A\n",
      "147552it [00:09, 15480.22it/s]\u001b[A\n",
      "149167it [00:09, 15677.04it/s]\u001b[A\n",
      "150788it [00:09, 15834.82it/s]\u001b[A\n",
      "152373it [00:09, 15780.83it/s]\u001b[A\n",
      "153952it [00:09, 15642.16it/s]\u001b[A\n",
      "155517it [00:09, 15630.33it/s]\u001b[A\n",
      "157201it [00:10, 15989.57it/s]\u001b[A\n",
      "158801it [00:10, 15698.99it/s]\u001b[A\n",
      "160389it [00:10, 15751.39it/s]\u001b[A\n",
      "162049it [00:10, 15990.37it/s]\u001b[A\n",
      "163708it [00:10, 16167.63it/s]\u001b[A\n",
      "165422it [00:10, 16455.65it/s]\u001b[A\n",
      "167125it [00:10, 16626.29it/s]\u001b[A\n",
      "168789it [00:10, 16103.62it/s]\u001b[A\n",
      "170533it [00:10, 16492.53it/s]\u001b[A\n",
      "172188it [00:11, 16507.53it/s]\u001b[A\n",
      "173842it [00:11, 16115.45it/s]\u001b[A\n",
      "175458it [00:11, 16094.60it/s]\u001b[A\n",
      "177070it [00:11, 15765.60it/s]\u001b[A\n",
      "178690it [00:11, 15890.11it/s]\u001b[A\n",
      "180357it [00:11, 16118.54it/s]\u001b[A\n",
      "182057it [00:11, 16373.77it/s]\u001b[A\n",
      "183747it [00:11, 16518.05it/s]\u001b[A\n",
      "185470it [00:11, 16721.29it/s]\u001b[A\n",
      "187144it [00:11, 16474.60it/s]\u001b[A\n",
      "188795it [00:12, 16483.27it/s]\u001b[A\n",
      "190445it [00:12, 16264.16it/s]\u001b[A\n",
      "192160it [00:12, 16523.75it/s]\u001b[A\n",
      "193814it [00:12, 16508.30it/s]\u001b[A\n",
      "195483it [00:12, 16553.58it/s]\u001b[A\n",
      "197146it [00:12, 16576.12it/s]\u001b[A\n",
      "198940it [00:12, 16981.45it/s]\u001b[A\n",
      "200639it [00:12, 16948.13it/s]\u001b[A\n",
      "202335it [00:12, 16772.35it/s]\u001b[A\n",
      "204013it [00:12, 16764.08it/s]\u001b[A\n",
      "205690it [00:13, 16516.93it/s]\u001b[A\n",
      "207343it [00:13, 16405.67it/s]\u001b[A\n",
      "208985it [00:13, 16088.12it/s]\u001b[A\n",
      "210731it [00:13, 16486.32it/s]\u001b[A\n",
      "212522it [00:13, 16897.79it/s]\u001b[A\n",
      "214214it [00:13, 16852.82it/s]\u001b[A\n",
      "215966it [00:13, 17047.17it/s]\u001b[A\n",
      "217672it [00:13, 17017.08it/s]\u001b[A\n",
      "219375it [00:13, 16908.99it/s]\u001b[A\n",
      "221067it [00:13, 16695.34it/s]\u001b[A\n",
      "222848it [00:14, 17021.23it/s]\u001b[A\n",
      "224565it [00:14, 17059.80it/s]\u001b[A\n",
      "226272it [00:14, 16695.12it/s]\u001b[A\n",
      "227947it [00:14, 16701.94it/s]\u001b[A\n",
      "229623it [00:14, 16718.17it/s]\u001b[A\n",
      "231296it [00:14, 16679.50it/s]\u001b[A\n",
      "233021it [00:14, 16848.77it/s]\u001b[A\n",
      "234707it [00:14, 16794.24it/s]\u001b[A\n",
      "236402it [00:14, 16834.32it/s]\u001b[A\n",
      "238110it [00:14, 16895.59it/s]\u001b[A\n",
      "239800it [00:15, 16839.89it/s]\u001b[A\n",
      "241518it [00:15, 16938.13it/s]\u001b[A\n",
      "243213it [00:15, 16841.78it/s]\u001b[A\n",
      "244898it [00:15, 16807.19it/s]\u001b[A\n",
      "246610it [00:15, 16899.96it/s]\u001b[A\n",
      "248301it [00:15, 16826.95it/s]\u001b[A\n",
      "249984it [00:15, 16489.41it/s]\u001b[A\n",
      "251650it [00:15, 16537.58it/s]\u001b[A\n",
      "253308it [00:15, 16549.62it/s]\u001b[A\n",
      "254964it [00:15, 16027.38it/s]\u001b[A\n",
      "256571it [00:16, 16039.28it/s]\u001b[A\n",
      "259516it [00:16, 15938.43it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Jsonl: datasets/dev.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1699it [00:00, 16984.99it/s]\u001b[A\n",
      "3398it [00:00, 16881.82it/s]\u001b[A\n",
      "5087it [00:00, 16731.73it/s]\u001b[A\n",
      "6761it [00:00, 16473.59it/s]\u001b[A\n",
      "8461it [00:00, 16659.93it/s]\u001b[A\n",
      "10128it [00:00, 16467.00it/s]\u001b[A\n",
      "11776it [00:00, 16326.50it/s]\u001b[A\n",
      "13410it [00:00, 16011.98it/s]\u001b[A\n",
      "15021it [00:00, 16036.98it/s]\u001b[A\n",
      "16710it [00:01, 16290.97it/s]\u001b[A\n",
      "18396it [00:01, 16458.23it/s]\u001b[A\n",
      "20043it [00:01, 16451.90it/s]\u001b[A\n",
      "21816it [00:01, 16833.60it/s]\u001b[A\n",
      "23503it [00:01, 16843.57it/s]\u001b[A\n",
      "25188it [00:01, 16803.35it/s]\u001b[A\n",
      "26885it [00:01, 16851.07it/s]\u001b[A\n",
      "28571it [00:01, 16809.61it/s]\u001b[A\n",
      "30253it [00:01, 16797.86it/s]\u001b[A\n",
      "32440it [00:01, 16554.35it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "train_list = load_jsonl('datasets/train.jsonl')\n",
    "dev_list = load_jsonl('datasets/dev.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65ab448b-c47a-4535-954e-a15e02356fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NLIDataset(train_list)\n",
    "dev_dataset = NLIDataset(dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd9a753c-6545-45b4-b7b4-9eaae6c15f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': '237242',\n",
       " 'md5sum': 'b3155772342dd7122bec1df6c9bd33e1',\n",
       " 'label': 'EDM',\n",
       " 'context': 'Read email browser 訂閱 取消 訂閱 Subscribe Unsubscribe'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "260ed660-3f90-46e0-ae61-6b44e7650ae6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor(1),\n",
       " 'input_ids': tensor([  101, 37182, 79515, 91597,  7169,  8148,  2737,  5010,  7169,  8148,\n",
       "         24358, 31505, 47116, 10112, 11038, 12892, 19528, 99590, 11044,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad69d768-5bf4-41e4-9025-0d231d2c16a9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: 3 classes\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=3)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if torch.cuda.device_count() >1:\n",
    "    model = nn.DataParallel(model,device_ids=[0])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acfb8614-4c87-49aa-b377-14c024b0f8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6488\n",
      "811\n"
     ]
    }
   ],
   "source": [
    "train_batch_size=40\n",
    "learning_rate=2e-5 \n",
    "train_epochs=5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "dev_dataloader = DataLoader(dev_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "print(len(train_dataloader))\n",
    "print(len(dev_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95d5fb97-7401-412c-bc22-1e57e1f0846c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 2, 1, 0, 1, 1,\n",
      "        1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]), 'input_ids': tensor([[ 101, 8809, 8809,  ..., 5949, 4769,  102],\n",
      "        [ 101, 7860, 3457,  ..., 8127, 2358,  102],\n",
      "        [ 101, 4459, 2864,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 8148, 7154,  ...,    0,    0,    0],\n",
      "        [ 101, 6252, 2078,  ...,    0,    0,    0],\n",
      "        [ 101, 4458, 4333,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for batch_index, batch_dict in enumerate(train_dataloader):\n",
    "    print(batch_dict)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe74576b-9e2b-41ac-b359-668dbfead630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1961d9667e9d4b3ea6b5dc8b9e7dd039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32440 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0   loss:  tensor(1.0818, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0125, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.1076, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.1206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0386, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.9901664611590629\n",
      "f1:  0.982330044562603\n",
      "epoch:  1   loss:  tensor(0.0524, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.1700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0667, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.9928175092478422\n",
      "f1:  0.9871337698683215\n",
      "epoch:  2   loss:  tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0747, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.3335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.9938039457459926\n",
      "f1:  0.9889851180780581\n",
      "epoch:  3   loss:  tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(8.8502e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0644, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0037, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.993711467324291\n",
      "f1:  0.9890338807898044\n",
      "epoch:  4   loss:  tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0488, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(7.4150e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(8.5918e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(5.1732e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.9940813810110974\n",
      "f1:  0.9894556928411555\n"
     ]
    }
   ],
   "source": [
    "## 進度條\n",
    "num_training_steps = train_epochs * len(train_dataloader)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "## 設定warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "  \"linear\",\n",
    "  optimizer=optimizer,\n",
    "  num_warmup_steps=10,\n",
    "  num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "## start training\n",
    "for epoch in range(train_epochs):\n",
    "    model.train()\n",
    "    for batch_index, batch_dict in enumerate(train_dataloader):\n",
    "        \n",
    "        input_items = {key: val.to(device) for key, val in batch_dict.items()}\n",
    "#         del input_items['token_type_ids'] ## bart不需要這個\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**input_items)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        if torch.cuda.device_count() >1: ##多GPU的情況要對loss求平均\n",
    "            loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        if batch_index % 500 ==0:\n",
    "            print('epoch: ', epoch, '  loss: ', loss)\n",
    "            \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    with torch.no_grad():\n",
    "        for batch_index, batch_dict in enumerate(dev_dataloader):\n",
    "            input_items = {key: val.to(device) for key, val in batch_dict.items()}\n",
    "            outputs = model(**input_items)\n",
    "\n",
    "            predictions += outputs.logits.argmax(dim=-1).tolist()\n",
    "            references += batch_dict['labels'].tolist()\n",
    "\n",
    "    accuracy = accuracy_score(references, predictions)\n",
    "    f1 = f1_score(references, predictions,average='macro')\n",
    "    print('acc: ', accuracy)\n",
    "    print('f1: ', f1)\n",
    "    \n",
    "    ## save model\n",
    "    save_path = 'models/Mail_Classifier_11/epoch_' + str(epoch+1)\n",
    "    if torch.cuda.device_count() >1:\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(save_path)\n",
    "    else:\n",
    "        model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c2c796",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "916674e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Jsonl: datasets/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32440it [00:01, 17434.15it/s]\n"
     ]
    }
   ],
   "source": [
    "test_list = load_jsonl('datasets/test.jsonl')\n",
    "test_dataset = NLIDataset(test_list)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f36c4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: 3 classes\n",
    "model_path = 'models/Mail_Classifier_11/epoch_5'\n",
    "# model_path = 'models/Mail_Classifier/epoch_5'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=3)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1af166d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45fdda09efb41488d0258aca28e2d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.9931565967940814\n",
      "f1:  0.9879735482311608\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "references = []\n",
    "num_steps = len(test_dataloader)\n",
    "progress_bar = tqdm(range(num_steps))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_index, batch_dict in enumerate(test_dataloader):\n",
    "        input_items = {key: val.to(device) for key, val in batch_dict.items()}\n",
    "        outputs = model(**input_items)\n",
    "\n",
    "        predictions += outputs.logits.argmax(dim=-1).tolist()\n",
    "        references += batch_dict['labels'].tolist()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "accuracy = accuracy_score(references, predictions)\n",
    "f1 = f1_score(references, predictions,average='macro')\n",
    "print('acc: ', accuracy)\n",
    "print('f1: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57ec3d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/Mail_Classifier_11/epoch_5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97      3723\n",
      "           1       1.00      1.00      1.00     21875\n",
      "           2       0.99      0.99      0.99      6842\n",
      "\n",
      "    accuracy                           0.99     32440\n",
      "   macro avg       0.99      0.99      0.99     32440\n",
      "weighted avg       0.99      0.99      0.99     32440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_path)\n",
    "print(classification_report(references, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52fe47b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3723\n",
      "[0, 21, 24, 56, 61, 92, 97, 100, 101, 109, 111, 117, 125, 129, 148, 152, 158, 165, 169, 191, 193, 198, 203, 217, 220, 222, 232, 234, 251, 257, 264, 284, 294, 306, 312, 315, 321, 332, 335, 338, 340, 357, 375, 379, 380, 406, 422, 431, 445, 449, 452, 465, 496, 498, 503, 510, 514, 530, 544, 549, 553, 559, 561, 567, 582, 589, 593, 599, 612, 618, 623, 648, 660, 663, 665, 668, 671, 684, 709, 710, 718, 729, 736, 739, 759, 770, 777, 778, 780, 788, 789, 802, 819, 831, 857, 871, 873, 882, 883, 885, 890, 893, 907, 927, 934, 947, 952, 957, 959, 972, 998, 1010, 1024, 1027, 1035, 1039, 1045, 1054, 1070, 1072, 1073, 1085, 1095, 1099, 1101, 1107, 1110, 1111, 1113, 1121, 1123, 1126, 1133, 1138, 1140, 1146, 1148, 1156, 1157, 1162, 1165, 1170, 1171, 1174, 1188, 1196, 1200, 1209, 1213, 1216, 1239, 1267, 1268, 1285, 1291, 1295, 1305, 1323, 1326, 1329, 1344, 1361, 1375, 1380, 1393, 1395, 1396, 1408, 1414, 1416, 1419, 1421, 1422, 1424, 1431, 1435, 1443, 1451, 1458, 1480, 1497, 1503, 1514, 1518, 1526, 1531, 1536, 1544, 1553, 1554, 1575, 1578, 1589, 1592, 1605, 1607, 1610, 1611, 1617, 1623, 1630, 1633, 1637, 1644, 1652, 1659, 1675, 1678, 1687, 1702, 1728, 1733, 1734, 1756, 1767, 1774, 1783, 1784, 1787, 1795, 1799, 1802, 1809, 1816, 1817, 1822, 1824, 1828, 1832, 1840, 1844, 1850, 1858, 1860, 1863, 1872, 1878, 1883, 1904, 1943, 1947, 1957, 1986, 1996, 2005, 2010, 2011, 2013, 2022, 2053, 2072, 2076, 2104, 2115, 2117, 2120, 2128, 2135, 2139, 2149, 2198, 2199, 2201, 2204, 2230, 2233, 2240, 2243, 2257, 2264, 2275, 2277, 2278, 2286, 2294, 2299, 2308, 2323, 2343, 2360, 2362, 2365, 2375, 2380, 2385, 2387, 2406, 2408, 2411, 2419, 2429, 2439, 2452, 2461, 2466, 2468, 2469, 2480, 2492, 2515, 2522, 2523, 2527, 2530, 2537, 2551, 2570, 2572, 2587, 2589, 2590, 2597, 2608, 2622, 2658, 2663, 2698, 2707, 2723, 2726, 2727, 2737, 2753, 2755, 2761, 2763, 2765, 2772, 2783, 2790, 2797, 2800, 2810, 2815, 2824, 2827, 2828, 2848, 2870, 2872, 2879, 2883, 2887, 2888, 2896, 2899, 2934, 2935, 2952, 2970, 2975, 2978, 2979, 2989, 2995, 3003, 3012, 3017, 3027, 3031, 3049, 3074, 3075, 3095, 3108, 3126, 3143, 3146, 3163, 3176, 3187, 3197, 3201, 3213, 3226, 3248, 3251, 3262, 3265, 3271, 3275, 3280, 3298, 3301, 3303, 3318, 3322, 3342, 3344, 3356, 3358, 3362, 3363, 3372, 3373, 3387, 3398, 3410, 3411, 3415, 3440, 3446, 3450, 3460, 3471, 3472, 3473, 3483, 3496, 3506, 3526, 3530, 3537, 3547, 3558, 3562, 3566, 3588, 3593, 3595, 3604, 3620, 3622, 3631, 3633, 3638, 3654, 3657, 3661, 3667, 3679, 3689, 3700, 3702, 3732, 3733, 3734, 3735, 3738, 3739, 3749, 3761, 3764, 3765, 3778, 3798, 3825, 3827, 3845, 3850, 3856, 3862, 3883, 3886, 3907, 3910, 3911, 3915, 3923, 3929, 3931, 3935, 3952, 3961, 3982, 3990, 3994, 3998, 4006, 4010, 4031, 4036, 4046, 4047, 4077, 4094, 4097, 4106, 4114, 4124, 4150, 4151, 4153, 4166, 4200, 4203, 4211, 4223, 4224, 4231, 4245, 4251, 4252, 4282, 4292, 4298, 4303, 4313, 4319, 4322, 4327, 4334, 4339, 4341, 4344, 4345, 4346, 4348, 4369, 4380, 4382, 4408, 4428, 4431, 4456, 4464, 4484, 4486, 4493, 4495, 4499, 4515, 4521, 4540, 4564, 4569, 4580, 4595, 4596, 4597, 4598, 4600, 4603, 4608, 4614, 4618, 4638, 4650, 4653, 4659, 4669, 4677, 4685, 4686, 4695, 4699, 4700, 4722, 4725, 4731, 4741, 4742, 4768, 4776, 4778, 4789, 4790, 4794, 4798, 4807, 4831, 4832, 4875, 4878, 4888, 4896, 4908, 4910, 4921, 4943, 4952, 4959, 5004, 5017, 5018, 5030, 5039, 5048, 5050, 5059, 5061, 5063, 5071, 5077, 5086, 5107, 5109, 5110, 5112, 5129, 5136, 5143, 5153, 5159, 5164, 5172, 5188, 5193, 5196, 5213, 5215, 5220, 5255, 5261, 5262, 5287, 5303, 5305, 5318, 5331, 5343, 5344, 5347, 5354, 5370, 5405, 5407, 5410, 5420, 5444, 5447, 5461, 5462, 5475, 5489, 5497, 5501, 5511, 5523, 5554, 5556, 5559, 5575, 5583, 5592, 5597, 5606, 5609, 5624, 5638, 5646, 5653, 5656, 5668, 5671, 5688, 5690, 5691, 5698, 5706, 5709, 5715, 5717, 5732, 5741, 5747, 5760, 5763, 5765, 5772, 5784, 5818, 5824, 5836, 5849, 5850, 5852, 5873, 5874, 5890, 5897, 5902, 5938, 5942, 5965, 5975, 5980, 5981, 5998, 6004, 6014, 6015, 6026, 6032, 6035, 6036, 6044, 6052, 6067, 6082, 6110, 6142, 6146, 6155, 6157, 6163, 6165, 6166, 6176, 6177, 6181, 6189, 6192, 6194, 6205, 6226, 6227, 6229, 6245, 6299, 6301, 6312, 6313, 6316, 6334, 6352, 6361, 6365, 6372, 6377, 6380, 6385, 6386, 6387, 6403, 6405, 6411, 6424, 6428, 6440, 6443, 6448, 6456, 6465, 6473, 6481, 6482, 6488, 6490, 6522, 6525, 6529, 6534, 6538, 6543, 6544, 6548, 6549, 6567, 6584, 6614, 6621, 6622, 6632, 6661, 6668, 6673, 6675, 6677, 6679, 6688, 6698, 6699, 6704, 6708, 6709, 6710, 6713, 6739, 6758, 6799, 6801, 6819, 6848, 6858, 6863, 6875, 6901, 6908, 6910, 6911, 6915, 6923, 6926, 6944, 6947, 6948, 6954, 6957, 6972, 6978, 6980, 6996, 7007, 7008, 7015, 7017, 7019, 7020, 7034, 7035, 7042, 7054, 7056, 7058, 7076, 7083, 7085, 7095, 7102, 7116, 7122, 7127, 7136, 7140, 7145, 7153, 7156, 7165, 7166, 7180, 7183, 7188, 7190, 7206, 7208, 7212, 7214, 7229, 7236, 7241, 7248, 7252, 7253, 7259, 7271, 7286, 7287, 7315, 7323, 7331, 7370, 7371, 7380, 7389, 7405, 7423, 7433, 7434, 7459, 7465, 7466, 7468, 7471, 7474, 7478, 7488, 7504, 7508, 7509, 7514, 7532, 7534, 7537, 7539, 7540, 7543, 7550, 7551, 7553, 7564, 7567, 7569, 7572, 7582, 7584, 7591, 7614, 7620, 7633, 7639, 7640, 7649, 7659, 7660, 7665, 7667, 7671, 7689, 7690, 7691, 7698, 7701, 7706, 7712, 7719, 7725, 7740, 7745, 7747, 7755, 7758, 7759, 7763, 7776, 7778, 7786, 7792, 7793, 7796, 7809, 7813, 7820, 7833, 7840, 7845, 7876, 7879, 7886, 7887, 7898, 7906, 7919, 7920, 7936, 7938, 7942, 7951, 7952, 7966, 7986, 7987, 7992, 7997, 8006, 8010, 8023, 8030, 8045, 8060, 8065, 8068, 8074, 8086, 8090, 8099, 8100, 8114, 8134, 8148, 8152, 8155, 8157, 8162, 8166, 8171, 8176, 8177, 8189, 8193, 8196, 8204, 8206, 8207, 8219, 8226, 8236, 8237, 8238, 8246, 8261, 8263, 8277, 8297, 8300, 8314, 8332, 8335, 8357, 8360, 8361, 8363, 8384, 8388, 8407, 8411, 8412, 8425, 8453, 8459, 8474, 8494, 8507, 8513, 8514, 8526, 8543, 8551, 8574, 8576, 8577, 8581, 8585, 8590, 8600, 8608, 8615, 8619, 8635, 8636, 8638, 8641, 8659, 8663, 8670, 8674, 8683, 8690, 8695, 8699, 8700, 8704, 8708, 8709, 8711, 8717, 8719, 8738, 8740, 8745, 8748, 8762, 8768, 8773, 8782, 8801, 8802, 8804, 8828, 8846, 8858, 8867, 8877, 8881, 8891, 8893, 8907, 8913, 8927, 8929, 8933, 8949, 8950, 8972, 8994, 9004, 9023, 9033, 9056, 9061, 9066, 9067, 9079, 9093, 9095, 9111, 9115, 9123, 9126, 9129, 9131, 9152, 9154, 9169, 9185, 9194, 9198, 9206, 9213, 9217, 9230, 9233, 9234, 9242, 9258, 9286, 9288, 9291, 9294, 9321, 9325, 9328, 9331, 9337, 9341, 9366, 9367, 9384, 9400, 9414, 9419, 9437, 9450, 9454, 9456, 9461, 9467, 9478, 9479, 9500, 9503, 9511, 9521, 9522, 9526, 9528, 9543, 9561, 9567, 9569, 9578, 9580, 9581, 9592, 9606, 9609, 9623, 9636, 9645, 9649, 9675, 9689, 9695, 9699, 9701, 9710, 9713, 9716, 9755, 9759, 9767, 9789, 9815, 9817, 9819, 9832, 9842, 9846, 9849, 9853, 9855, 9858, 9870, 9875, 9881, 9888, 9889, 9913, 9928, 9934, 9956, 9958, 9959, 9992, 9993, 9997, 10017, 10019, 10020, 10021, 10023, 10025, 10027, 10032, 10036, 10051, 10054, 10064, 10065, 10086, 10101, 10107, 10110, 10111, 10116, 10123, 10129, 10136, 10141, 10147, 10160, 10196, 10203, 10209, 10216, 10228, 10241, 10248, 10255, 10272, 10276, 10287, 10296, 10301, 10304, 10308, 10316, 10321, 10327, 10338, 10339, 10363, 10368, 10381, 10383, 10387, 10389, 10392, 10439, 10443, 10447, 10449, 10472, 10493, 10494, 10498, 10502, 10511, 10512, 10514, 10523, 10524, 10535, 10536, 10560, 10565, 10567, 10591, 10620, 10629, 10633, 10637, 10647, 10649, 10650, 10651, 10683, 10686, 10699, 10700, 10705, 10712, 10728, 10734, 10760, 10762, 10772, 10777, 10785, 10793, 10814, 10823, 10824, 10833, 10859, 10864, 10868, 10887, 10890, 10893, 10918, 10925, 10927, 10933, 10944, 10953, 10958, 10968, 10980, 11008, 11017, 11029, 11033, 11038, 11039, 11042, 11053, 11065, 11099, 11106, 11107, 11116, 11120, 11125, 11126, 11133, 11142, 11155, 11157, 11163, 11166, 11172, 11178, 11182, 11187, 11191, 11194, 11195, 11198, 11211, 11216, 11226, 11248, 11252, 11276, 11294, 11295, 11313, 11319, 11324, 11333, 11334, 11348, 11359, 11364, 11372, 11374, 11378, 11393, 11398, 11402, 11419, 11426, 11436, 11438, 11439, 11441, 11470, 11474, 11478, 11496, 11502, 11504, 11515, 11520, 11522, 11543, 11548, 11557, 11595, 11596, 11605, 11617, 11626, 11632, 11642, 11670, 11672, 11682, 11688, 11701, 11702, 11703, 11705, 11738, 11745, 11746, 11756, 11763, 11766, 11771, 11775, 11776, 11781, 11798, 11802, 11811, 11835, 11841, 11847, 11858, 11868, 11869, 11872, 11892, 11901, 11905, 11911, 11931, 11944, 11947, 11953, 11956, 11957, 11959, 11960, 11963, 11971, 11984, 11997, 12002, 12019, 12022, 12026, 12028, 12035, 12054, 12071, 12082, 12088, 12091, 12094, 12099, 12105, 12112, 12118, 12126, 12134, 12142, 12146, 12158, 12165, 12166, 12168, 12191, 12220, 12222, 12230, 12245, 12246, 12253, 12257, 12263, 12265, 12267, 12276, 12280, 12281, 12308, 12316, 12320, 12334, 12337, 12339, 12340, 12342, 12354, 12366, 12380, 12382, 12402, 12407, 12417, 12430, 12431, 12434, 12437, 12438, 12452, 12457, 12458, 12468, 12489, 12491, 12500, 12525, 12539, 12541, 12542, 12546, 12550, 12553, 12572, 12576, 12577, 12580, 12581, 12585, 12593, 12595, 12605, 12619, 12633, 12636, 12651, 12659, 12675, 12679, 12684, 12689, 12697, 12699, 12701, 12710, 12730, 12741, 12765, 12772, 12774, 12780, 12784, 12790, 12791, 12793, 12796, 12802, 12807, 12808, 12824, 12834, 12839, 12852, 12865, 12867, 12904, 12921, 12946, 12951, 12961, 12963, 12964, 12967, 12973, 12974, 12983, 13004, 13008, 13010, 13011, 13023, 13025, 13032, 13034, 13045, 13046, 13048, 13052, 13060, 13061, 13072, 13093, 13094, 13108, 13115, 13122, 13127, 13128, 13140, 13158, 13159, 13171, 13174, 13182, 13189, 13193, 13200, 13203, 13205, 13209, 13211, 13216, 13230, 13231, 13267, 13280, 13285, 13287, 13290, 13293, 13294, 13296, 13307, 13311, 13312, 13314, 13327, 13329, 13335, 13343, 13344, 13345, 13373, 13375, 13386, 13398, 13408, 13417, 13422, 13424, 13435, 13441, 13451, 13452, 13463, 13468, 13478, 13488, 13492, 13511, 13513, 13545, 13563, 13570, 13575, 13581, 13591, 13606, 13616, 13621, 13623, 13630, 13633, 13636, 13637, 13670, 13691, 13692, 13693, 13706, 13711, 13712, 13720, 13723, 13748, 13749, 13757, 13766, 13768, 13774, 13775, 13781, 13784, 13818, 13830, 13831, 13834, 13839, 13844, 13848, 13854, 13856, 13868, 13880, 13888, 13894, 13901, 13912, 13914, 13916, 13921, 13927, 13929, 13931, 13933, 13944, 13951, 13974, 13977, 13978, 13984, 13986, 13989, 14018, 14020, 14022, 14041, 14046, 14051, 14064, 14087, 14088, 14111, 14116, 14121, 14135, 14151, 14157, 14164, 14169, 14172, 14181, 14182, 14225, 14228, 14245, 14252, 14283, 14290, 14307, 14314, 14321, 14343, 14353, 14376, 14381, 14390, 14418, 14419, 14423, 14434, 14438, 14442, 14456, 14470, 14489, 14491, 14502, 14524, 14537, 14544, 14549, 14556, 14565, 14575, 14577, 14597, 14637, 14643, 14646, 14654, 14674, 14685, 14693, 14697, 14709, 14719, 14720, 14722, 14723, 14724, 14738, 14760, 14779, 14784, 14788, 14792, 14799, 14812, 14821, 14822, 14835, 14846, 14851, 14855, 14857, 14858, 14868, 14870, 14874, 14876, 14894, 14901, 14905, 14929, 14938, 14947, 14966, 14968, 14978, 14988, 14991, 14992, 14999, 15003, 15004, 15010, 15018, 15060, 15062, 15081, 15093, 15118, 15128, 15131, 15138, 15148, 15154, 15159, 15162, 15169, 15172, 15185, 15228, 15235, 15252, 15258, 15259, 15274, 15307, 15310, 15336, 15345, 15352, 15369, 15376, 15389, 15390, 15400, 15401, 15402, 15449, 15450, 15453, 15476, 15481, 15485, 15493, 15494, 15508, 15509, 15519, 15523, 15556, 15571, 15572, 15581, 15585, 15589, 15598, 15608, 15614, 15616, 15617, 15627, 15639, 15649, 15657, 15667, 15668, 15670, 15671, 15672, 15679, 15687, 15696, 15709, 15719, 15743, 15744, 15747, 15767, 15770, 15772, 15782, 15784, 15787, 15803, 15804, 15813, 15828, 15839, 15843, 15844, 15846, 15858, 15867, 15879, 15882, 15884, 15889, 15891, 15907, 15916, 15923, 15933, 15944, 15950, 15953, 15968, 15976, 15983, 15989, 15992, 15997, 16015, 16025, 16029, 16034, 16041, 16049, 16050, 16066, 16070, 16074, 16089, 16090, 16098, 16129, 16132, 16152, 16153, 16157, 16178, 16189, 16204, 16207, 16209, 16214, 16218, 16225, 16230, 16238, 16240, 16242, 16251, 16261, 16303, 16308, 16313, 16315, 16316, 16332, 16335, 16336, 16344, 16349, 16352, 16363, 16366, 16377, 16381, 16401, 16411, 16422, 16424, 16426, 16447, 16473, 16498, 16516, 16520, 16523, 16524, 16530, 16531, 16548, 16549, 16559, 16565, 16567, 16579, 16586, 16587, 16594, 16599, 16601, 16606, 16607, 16608, 16617, 16625, 16636, 16637, 16662, 16666, 16667, 16675, 16685, 16696, 16722, 16733, 16738, 16749, 16752, 16788, 16795, 16798, 16802, 16809, 16810, 16811, 16815, 16823, 16835, 16842, 16843, 16845, 16885, 16889, 16893, 16928, 16936, 16958, 16959, 16963, 16977, 16978, 16984, 17001, 17002, 17003, 17005, 17009, 17014, 17026, 17032, 17034, 17039, 17043, 17045, 17049, 17060, 17065, 17076, 17077, 17079, 17091, 17099, 17119, 17125, 17126, 17131, 17132, 17135, 17158, 17160, 17167, 17169, 17175, 17181, 17188, 17195, 17200, 17202, 17232, 17255, 17263, 17267, 17275, 17277, 17281, 17282, 17284, 17293, 17314, 17319, 17327, 17347, 17353, 17354, 17361, 17366, 17372, 17380, 17393, 17395, 17402, 17409, 17415, 17417, 17451, 17454, 17461, 17469, 17477, 17481, 17485, 17488, 17490, 17502, 17512, 17524, 17533, 17542, 17543, 17550, 17553, 17561, 17569, 17576, 17582, 17590, 17600, 17601, 17603, 17610, 17620, 17635, 17643, 17645, 17663, 17675, 17686, 17687, 17692, 17698, 17723, 17725, 17734, 17740, 17741, 17767, 17770, 17772, 17775, 17776, 17779, 17780, 17808, 17809, 17831, 17832, 17843, 17845, 17847, 17848, 17854, 17872, 17882, 17888, 17889, 17894, 17895, 17912, 17921, 17934, 17943, 17971, 17985, 17986, 17990, 18003, 18017, 18020, 18027, 18035, 18036, 18037, 18038, 18045, 18055, 18057, 18060, 18078, 18079, 18084, 18088, 18089, 18107, 18125, 18127, 18134, 18138, 18146, 18151, 18154, 18158, 18168, 18175, 18182, 18205, 18214, 18217, 18223, 18231, 18238, 18254, 18255, 18261, 18265, 18269, 18272, 18284, 18297, 18322, 18335, 18342, 18343, 18345, 18348, 18351, 18361, 18370, 18374, 18376, 18388, 18404, 18409, 18413, 18422, 18438, 18439, 18455, 18465, 18483, 18484, 18505, 18510, 18516, 18520, 18522, 18532, 18547, 18548, 18566, 18570, 18603, 18618, 18652, 18654, 18666, 18677, 18683, 18684, 18711, 18716, 18719, 18721, 18734, 18753, 18758, 18770, 18772, 18777, 18785, 18793, 18809, 18868, 18869, 18883, 18895, 18920, 18921, 18930, 18931, 18945, 18947, 18951, 18973, 18975, 18981, 18985, 18986, 18994, 19009, 19026, 19030, 19037, 19040, 19059, 19069, 19074, 19093, 19097, 19114, 19119, 19133, 19149, 19150, 19152, 19164, 19171, 19181, 19215, 19217, 19226, 19231, 19233, 19234, 19244, 19253, 19259, 19260, 19273, 19274, 19310, 19319, 19335, 19336, 19343, 19345, 19347, 19348, 19349, 19382, 19398, 19405, 19406, 19412, 19420, 19431, 19432, 19446, 19449, 19452, 19457, 19483, 19487, 19492, 19501, 19503, 19505, 19509, 19526, 19554, 19569, 19570, 19581, 19588, 19596, 19597, 19606, 19619, 19620, 19625, 19643, 19645, 19648, 19654, 19666, 19670, 19672, 19682, 19688, 19696, 19708, 19722, 19730, 19731, 19736, 19744, 19750, 19751, 19766, 19774, 19787, 19797, 19798, 19819, 19822, 19827, 19836, 19840, 19848, 19859, 19864, 19865, 19871, 19879, 19900, 19903, 19910, 19928, 19936, 19938, 19954, 19957, 19977, 19978, 20003, 20013, 20021, 20023, 20026, 20050, 20068, 20069, 20070, 20085, 20089, 20090, 20110, 20122, 20129, 20137, 20153, 20160, 20172, 20210, 20215, 20231, 20239, 20266, 20272, 20274, 20284, 20287, 20307, 20314, 20316, 20317, 20346, 20349, 20354, 20362, 20372, 20398, 20403, 20416, 20426, 20428, 20435, 20436, 20457, 20465, 20466, 20470, 20481, 20483, 20485, 20491, 20494, 20496, 20507, 20512, 20517, 20521, 20524, 20533, 20543, 20548, 20549, 20554, 20556, 20565, 20585, 20586, 20587, 20592, 20599, 20601, 20604, 20667, 20674, 20675, 20686, 20690, 20732, 20733, 20736, 20746, 20754, 20755, 20760, 20769, 20777, 20790, 20795, 20807, 20811, 20816, 20825, 20847, 20861, 20865, 20867, 20880, 20881, 20886, 20924, 20926, 20936, 20947, 20949, 20951, 20954, 20966, 20985, 20987, 21013, 21015, 21019, 21025, 21026, 21044, 21047, 21049, 21079, 21093, 21099, 21129, 21135, 21147, 21154, 21162, 21167, 21177, 21188, 21205, 21211, 21225, 21232, 21243, 21263, 21264, 21270, 21274, 21275, 21303, 21312, 21316, 21336, 21337, 21342, 21363, 21368, 21370, 21391, 21393, 21399, 21404, 21409, 21411, 21422, 21438, 21456, 21479, 21493, 21508, 21531, 21532, 21533, 21537, 21557, 21558, 21561, 21570, 21573, 21597, 21624, 21636, 21643, 21655, 21668, 21696, 21700, 21709, 21722, 21745, 21752, 21810, 21814, 21823, 21831, 21833, 21842, 21844, 21847, 21849, 21850, 21851, 21871, 21879, 21882, 21890, 21891, 21897, 21899, 21905, 21914, 21931, 21937, 21953, 21971, 21982, 21987, 21989, 21995, 21998, 22033, 22035, 22040, 22056, 22060, 22069, 22072, 22073, 22077, 22078, 22079, 22082, 22097, 22103, 22123, 22124, 22169, 22178, 22179, 22193, 22200, 22204, 22205, 22209, 22214, 22251, 22254, 22259, 22269, 22275, 22285, 22307, 22313, 22321, 22329, 22341, 22347, 22350, 22353, 22391, 22396, 22408, 22411, 22420, 22421, 22425, 22440, 22454, 22468, 22470, 22474, 22483, 22484, 22485, 22500, 22502, 22508, 22512, 22529, 22550, 22553, 22557, 22581, 22595, 22606, 22610, 22625, 22633, 22635, 22637, 22639, 22647, 22649, 22652, 22668, 22677, 22684, 22711, 22756, 22761, 22768, 22775, 22781, 22782, 22807, 22810, 22818, 22819, 22822, 22826, 22829, 22837, 22839, 22841, 22851, 22852, 22862, 22865, 22876, 22898, 22917, 22924, 22934, 22941, 22945, 22948, 22949, 22962, 22964, 22971, 22995, 23003, 23007, 23030, 23033, 23043, 23050, 23056, 23066, 23076, 23078, 23079, 23085, 23095, 23097, 23105, 23122, 23172, 23184, 23196, 23199, 23200, 23206, 23208, 23218, 23232, 23233, 23239, 23241, 23248, 23249, 23261, 23263, 23269, 23282, 23288, 23293, 23294, 23308, 23312, 23314, 23316, 23328, 23341, 23355, 23380, 23382, 23385, 23395, 23398, 23399, 23404, 23405, 23426, 23432, 23436, 23460, 23461, 23470, 23472, 23484, 23486, 23498, 23506, 23518, 23520, 23524, 23553, 23556, 23562, 23582, 23620, 23622, 23626, 23633, 23662, 23663, 23666, 23674, 23688, 23689, 23690, 23691, 23699, 23706, 23710, 23740, 23743, 23760, 23762, 23837, 23868, 23873, 23877, 23878, 23881, 23883, 23886, 23888, 23893, 23895, 23907, 23923, 23926, 23929, 23944, 23952, 23965, 23966, 23967, 23970, 23973, 23988, 23990, 24018, 24027, 24044, 24047, 24050, 24067, 24084, 24094, 24096, 24101, 24116, 24121, 24124, 24126, 24129, 24138, 24150, 24151, 24178, 24181, 24216, 24218, 24228, 24241, 24250, 24270, 24271, 24273, 24284, 24285, 24296, 24301, 24303, 24314, 24327, 24331, 24332, 24333, 24341, 24347, 24360, 24364, 24365, 24370, 24380, 24412, 24419, 24423, 24434, 24435, 24460, 24461, 24468, 24472, 24475, 24485, 24495, 24515, 24518, 24521, 24524, 24567, 24568, 24576, 24583, 24586, 24590, 24603, 24610, 24632, 24636, 24637, 24638, 24646, 24650, 24662, 24671, 24678, 24688, 24696, 24701, 24726, 24730, 24737, 24743, 24745, 24751, 24789, 24810, 24813, 24817, 24820, 24832, 24849, 24876, 24879, 24916, 24922, 24934, 24938, 24946, 24952, 24965, 24996, 24997, 25007, 25008, 25021, 25023, 25035, 25037, 25050, 25056, 25065, 25067, 25068, 25083, 25090, 25097, 25108, 25112, 25115, 25118, 25120, 25123, 25138, 25151, 25168, 25201, 25209, 25218, 25222, 25225, 25231, 25239, 25240, 25246, 25268, 25273, 25275, 25282, 25286, 25287, 25300, 25315, 25320, 25331, 25342, 25343, 25354, 25361, 25363, 25370, 25379, 25386, 25400, 25412, 25429, 25434, 25456, 25463, 25466, 25469, 25470, 25482, 25499, 25512, 25526, 25537, 25542, 25548, 25557, 25560, 25563, 25565, 25567, 25571, 25576, 25584, 25598, 25610, 25617, 25651, 25663, 25664, 25679, 25680, 25685, 25699, 25715, 25718, 25735, 25737, 25748, 25751, 25756, 25764, 25779, 25781, 25794, 25809, 25834, 25856, 25862, 25864, 25871, 25872, 25873, 25879, 25907, 25910, 25914, 25918, 25920, 25928, 25935, 25943, 25966, 25967, 25980, 25982, 25986, 25990, 25998, 26001, 26002, 26008, 26020, 26021, 26022, 26027, 26043, 26052, 26057, 26058, 26101, 26109, 26110, 26111, 26140, 26158, 26161, 26185, 26207, 26210, 26212, 26221, 26241, 26250, 26251, 26254, 26258, 26265, 26266, 26278, 26287, 26294, 26300, 26310, 26315, 26321, 26331, 26366, 26367, 26368, 26375, 26377, 26381, 26384, 26390, 26402, 26404, 26405, 26407, 26419, 26427, 26433, 26444, 26461, 26475, 26477, 26482, 26490, 26496, 26500, 26501, 26532, 26540, 26541, 26548, 26550, 26567, 26578, 26580, 26582, 26616, 26617, 26621, 26636, 26642, 26652, 26653, 26660, 26670, 26687, 26690, 26712, 26743, 26745, 26756, 26766, 26772, 26782, 26789, 26791, 26805, 26807, 26814, 26821, 26839, 26854, 26855, 26859, 26861, 26868, 26872, 26877, 26885, 26890, 26895, 26907, 26912, 26913, 26933, 26937, 26944, 26987, 27002, 27013, 27020, 27027, 27033, 27039, 27053, 27062, 27081, 27095, 27096, 27103, 27115, 27126, 27127, 27154, 27158, 27159, 27162, 27176, 27179, 27185, 27210, 27211, 27215, 27217, 27222, 27239, 27240, 27241, 27255, 27258, 27270, 27276, 27280, 27285, 27297, 27302, 27316, 27335, 27337, 27341, 27343, 27350, 27355, 27360, 27366, 27393, 27396, 27402, 27408, 27410, 27418, 27451, 27467, 27468, 27483, 27485, 27511, 27518, 27524, 27538, 27547, 27557, 27567, 27568, 27571, 27586, 27587, 27592, 27610, 27614, 27629, 27630, 27636, 27637, 27644, 27658, 27660, 27663, 27666, 27695, 27701, 27714, 27741, 27750, 27752, 27763, 27765, 27770, 27789, 27798, 27807, 27810, 27814, 27815, 27822, 27828, 27830, 27833, 27846, 27849, 27859, 27867, 27877, 27885, 27909, 27921, 27923, 27937, 27948, 27951, 27955, 27958, 27962, 27992, 28025, 28027, 28041, 28047, 28052, 28053, 28067, 28088, 28099, 28101, 28104, 28152, 28158, 28163, 28168, 28173, 28182, 28186, 28204, 28213, 28217, 28222, 28233, 28236, 28241, 28242, 28244, 28279, 28280, 28291, 28297, 28311, 28325, 28349, 28351, 28358, 28361, 28364, 28369, 28377, 28389, 28428, 28442, 28447, 28454, 28463, 28475, 28476, 28500, 28503, 28523, 28563, 28587, 28594, 28595, 28599, 28601, 28611, 28612, 28619, 28624, 28661, 28684, 28702, 28711, 28716, 28741, 28751, 28776, 28788, 28802, 28808, 28809, 28810, 28814, 28818, 28843, 28854, 28871, 28872, 28873, 28886, 28899, 28906, 28907, 28914, 28934, 28954, 28955, 28965, 28974, 28977, 28980, 28984, 28991, 29005, 29011, 29014, 29024, 29031, 29036, 29038, 29046, 29057, 29066, 29082, 29089, 29101, 29107, 29108, 29150, 29158, 29160, 29171, 29189, 29192, 29195, 29201, 29206, 29210, 29218, 29232, 29246, 29274, 29284, 29288, 29290, 29302, 29322, 29334, 29337, 29338, 29341, 29344, 29347, 29348, 29351, 29353, 29354, 29366, 29400, 29405, 29415, 29425, 29435, 29445, 29457, 29465, 29511, 29525, 29526, 29537, 29552, 29561, 29564, 29567, 29572, 29581, 29595, 29597, 29599, 29609, 29617, 29619, 29646, 29657, 29658, 29659, 29660, 29662, 29666, 29679, 29689, 29690, 29700, 29712, 29723, 29761, 29771, 29773, 29776, 29780, 29785, 29788, 29802, 29810, 29818, 29823, 29828, 29853, 29873, 29881, 29888, 29895, 29939, 29950, 29966, 29980, 29998, 29999, 30009, 30011, 30016, 30022, 30027, 30039, 30051, 30063, 30075, 30092, 30099, 30101, 30102, 30119, 30123, 30129, 30132, 30147, 30153, 30173, 30178, 30190, 30193, 30194, 30197, 30211, 30214, 30225, 30237, 30253, 30256, 30261, 30271, 30288, 30291, 30292, 30297, 30315, 30333, 30334, 30354, 30357, 30369, 30372, 30378, 30381, 30390, 30394, 30408, 30422, 30424, 30429, 30435, 30441, 30442, 30444, 30452, 30453, 30465, 30486, 30488, 30497, 30526, 30529, 30538, 30553, 30554, 30555, 30574, 30600, 30616, 30627, 30632, 30638, 30640, 30649, 30653, 30656, 30658, 30666, 30679, 30684, 30702, 30703, 30713, 30717, 30724, 30736, 30737, 30739, 30752, 30761, 30782, 30786, 30794, 30797, 30804, 30840, 30841, 30863, 30900, 30902, 30903, 30910, 30914, 30926, 30940, 30942, 30945, 30964, 30986, 30990, 31012, 31022, 31032, 31033, 31037, 31055, 31063, 31094, 31096, 31100, 31101, 31102, 31107, 31109, 31125, 31135, 31143, 31162, 31165, 31173, 31174, 31176, 31187, 31188, 31192, 31206, 31207, 31212, 31216, 31220, 31222, 31227, 31232, 31239, 31243, 31255, 31266, 31272, 31281, 31296, 31315, 31341, 31348, 31352, 31357, 31366, 31368, 31374, 31382, 31383, 31385, 31390, 31418, 31435, 31447, 31460, 31470, 31477, 31482, 31504, 31505, 31508, 31520, 31548, 31556, 31559, 31575, 31580, 31588, 31593, 31596, 31601, 31602, 31643, 31672, 31690, 31693, 31695, 31698, 31699, 31712, 31715, 31742, 31759, 31760, 31786, 31792, 31803, 31816, 31824, 31825, 31841, 31854, 31856, 31861, 31863, 31864, 31874, 31877, 31881, 31889, 31903, 31906, 31922, 31952, 31953, 31968, 31978, 31992, 31995, 32008, 32009, 32023, 32029, 32048, 32050, 32061, 32062, 32066, 32070, 32076, 32083, 32085, 32091, 32093, 32096, 32098, 32107, 32112, 32119, 32127, 32133, 32145, 32146, 32149, 32151, 32152, 32156, 32157, 32160, 32167, 32172, 32183, 32189, 32203, 32206, 32212, 32214, 32233, 32240, 32244, 32262, 32270, 32285, 32288, 32302, 32312, 32320, 32323, 32329, 32331, 32347, 32354, 32360, 32364, 32365, 32371, 32372, 32373, 32379, 32411, 32413, 32415, 32431, 32439]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "error_list = []\n",
    "error_context = []\n",
    "reference_label_text = []\n",
    "reference_label_class = []\n",
    "predict_label = []\n",
    "output = pd.DataFrame()\n",
    "\n",
    "for num, i in enumerate(references):\n",
    "    # if i != predictions[num]:\n",
    "    if i == 0:\n",
    "        error_list.append(num)\n",
    "        error_context.append(test_list[num]['context'])\n",
    "        reference_label_text.append(test_list[num]['label'])\n",
    "        reference_label_class.append(i)\n",
    "        predict_label.append(predictions[num])\n",
    "                \n",
    "        # print(test_list[num])\n",
    "        # print(\"GT : \", references[num])\n",
    "        # print(predictions[num])\n",
    "        # break\n",
    "        # error_context = []\n",
    "        # predict_label = []\n",
    "        # reference_label = []\n",
    "        \n",
    "print(len(error_list))\n",
    "print(error_list)\n",
    "\n",
    "output['index'] = error_list\n",
    "output['pred_label'] = predict_label\n",
    "output['ref_class'] = reference_label_class\n",
    "output['ref'] = reference_label_text\n",
    "output['context'] = error_context\n",
    "\n",
    "# output.to_csv('list_classifier_error.csv', encoding = 'utf-8-sig')\n",
    "output.to_csv('right_spam.csv', encoding = 'utf-8-sig')\n",
    "\n",
    "    \n",
    "        # test_dataloader\n",
    "        # print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d78364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4b30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5e0c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75862e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c2ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8653fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f96539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eacf9f36",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eacacfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from commom import load_jsonl, save_jsonl\n",
    "\n",
    "from transformers import (\n",
    "    BartTokenizer,\n",
    "    BartForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "741e0b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Jsonl: datasets/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32440it [00:01, 17159.63it/s]\n"
     ]
    }
   ],
   "source": [
    "test_list = load_jsonl('datasets/test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d487bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('models/Mail_Classifier_10/epoch_5', num_labels=3) \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")  \n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if torch.cuda.device_count() >1:\n",
    "    model = nn.DataParallel(model,device_ids=[0,1])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a91ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_num = 10\n",
    "\n",
    "# subject = test_list[test_num]['subject']\n",
    "context = test_list[test_num]['context']\n",
    "label = test_list[test_num]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2b21f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "內文:  圖片 顯 現時 按此 下載 文宣 網頁 不想 收到 本行 信用卡 行銷 訊息 按此 登入 網路 銀行 信用卡 專區 進行 行銷 訊息 取消 訂閱 設定 電子 訊息 內容 包括 附件 合作金庫 銀行 股份 有限公司 傳送 電子 訊息 內容 機密性 經由 公司 授權 方可 利用 電子 訊息 指定 收件 任何人 公司 電子 訊息 內容 審閱 傳送 散佈 揭露 重製 指定 收件 通知 並請 刪除 電子 訊息 內容 謝謝您 合作 電子 訊息 內容 變更 網際網路 保證 電子 訊息 內容 完整性 公司 變更 修改 竄改 偽造 電子 訊息 內容 恕 不負 責任 網路 通訊 含有 電腦病毒 收件 應 自行 確認 郵件 內容 損害 公司 恕 負責\n",
      "label:  1 EDM\n",
      "predict:  1 EDM\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(context,\n",
    "                            max_length=512,\n",
    "                            truncation=True,\n",
    "                            return_tensors=\"pt\")\n",
    "class_number = {'SPAM':0, 'EDM':1, 'HAM':2}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input_items = {key: val.to(device) for key, val in tokenized_input.items()}\n",
    "    del input_items['token_type_ids'] ## bart不需要這個\n",
    "    \n",
    "    outputs = model(**input_items)\n",
    "    prediction = outputs.logits.argmax(dim=-1)\n",
    "    print(type(int(prediction)))\n",
    "    \n",
    "    \n",
    "    # print('主旨: ', subject)\n",
    "    print('內文: ', context)\n",
    "    print('label: ', class_number[label], label)\n",
    "    print('predict: ', int(prediction), list(class_number.keys())[list(class_number.values()).index(int(prediction))])\n",
    "    \n",
    "    # if int(prediction) == 0:\n",
    "    #     print('predict: ham')\n",
    "    # else:\n",
    "    #     print('predict: spam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff89fe0",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8925747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "acc:  0.725\n",
      "f1:  0.2801932367149758\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soc507/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/soc507/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/soc507/anaconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n        SPAM       0.00      0.00      0.00        22\\n         EDM       0.72      1.00      0.84       145\\n         HAM       0.00      0.00      0.00        33\\n\\n    accuracy                           0.73       200\\n   macro avg       0.24      0.33      0.28       200\\nweighted avg       0.53      0.72      0.61       200\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class_number = {'SPAM':0, 'EDM':1, 'HAM':2}\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for batch_index, batch_dict in enumerate(test_list[:200]):\n",
    "    input_items = {key: val.to(device) for key, val in tokenized_input.items()}\n",
    "    outputs = model(**input_items)\n",
    "\n",
    "    predictions += outputs.logits.argmax(dim=-1).tolist()\n",
    "\n",
    "    references += [class_number[batch_dict['label']]]\n",
    "    # print(reference[:10])\n",
    "print(len(predictions))\n",
    "\n",
    "accuracy = accuracy_score(references, predictions)\n",
    "f1 = f1_score(references, predictions,average='macro')\n",
    "print('acc: ', accuracy)\n",
    "print('f1: ', f1)\n",
    "\n",
    "print(\"----------\")\n",
    "classification_report(references, predictions, target_names=class_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b349f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c848836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b3ff1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71277b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93567c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_metric\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from common import load_jsonl, save_jsonl\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    "    BartTokenizer,\n",
    "    BartForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    "#     AutoModelForMaskedLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e5c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a94943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_list = load_jsonl('test-data.jsonl')\n",
    "eval_dataset = NLIDataset(eval_list)\n",
    "eval_dataset[0]\n",
    "test_dataloader = DataLoader(eval_dataset, shuffle=True, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7679eb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for batch_index, batch_dict in enumerate(test_dataloader):\n",
    "    input_items = {key: val.to(device) for key, val in batch_dict.items()}\n",
    "    outputs = model(**input_items)\n",
    "\n",
    "    predictions += outputs.logits.argmax(dim=-1).tolist()\n",
    "    references += batch_dict['labels'].tolist()\n",
    "\n",
    "accuracy = accuracy_score(references, predictions)\n",
    "f1 = f1_score(references, predictions,average='macro')\n",
    "print('acc: ', accuracy)\n",
    "print('f1: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c592cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 3\n",
    "print(\"the sentence is \")\n",
    "print()\n",
    "print(input_items[3])\n",
    "outputs = model(**input_items[3])\n",
    "predictions = outputs.logits.argmax(dim=-1).tolist()\n",
    "print(\"predict : \", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b564fa76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b85bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f2fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82237e53-f7fe-44b8-9fee-b44ef62a604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "label_list = [\"0\", \"1\"]\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"Transformers Trainer\")\n",
    "config = BertConfig.from_pretrained(\"Transformers Trainer\", finetuning_task=\"cola\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639889e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The probable hostile German reaction is unfortunate.\"  # @param {type:\"string\"}\n",
    "tokenized_input = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "outputs = model(**tokenized_input)\n",
    "print(f\"Prediction: {label_list[outputs.logits.argmax(dim=-1).item()]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
