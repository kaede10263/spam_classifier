{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "898780a9",
   "metadata": {},
   "source": [
    "## datasets prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de055d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datasets \n",
    "\n",
    "import csv\n",
    "csv.field_size_limit(500 * 1024 * 1024)\n",
    "\n",
    "data_list = []\n",
    "class_number = {'SPAM':0, 'EDM':0, 'HAM':0, 'NOTE':0}\n",
    "\n",
    "with open('spam_data_append_4class.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    # count = 0\n",
    "   \n",
    "    rows = csv.reader(csvfile)\n",
    "    for i, row in enumerate(rows):\n",
    "        if row[2] in class_number.keys():\n",
    "            pass\n",
    "        else:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa9fd8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "\n",
    "import csv\n",
    "csv.field_size_limit(500 * 1024 * 1024)\n",
    "\n",
    "data_list = []\n",
    "class_number = {'SPAM':0, 'EDM':0, 'HAM':0, 'NOTE':0}\n",
    "\n",
    "with open('spam_data_append_4class.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    \n",
    "    \n",
    "   \n",
    "    rows = csv.reader(csvfile)\n",
    "    for i, row in enumerate(rows):\n",
    "        # print(row[0])\n",
    "        if i != 0:\n",
    "            context = row[3].replace('Num','')\n",
    "            class_number[row[2]]+=1\n",
    "            data_list.append({'index': row[0], \n",
    "                              'md5sum': row[1],\n",
    "                              'label':row[2], \n",
    "                              'context':context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "088e2748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347700"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of data\n",
    "\n",
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1f36575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SPAM': 38053, 'EDM': 218647, 'HAM': 68952, 'NOTE': 22048}\n"
     ]
    }
   ],
   "source": [
    "# number of data for each class\n",
    "\n",
    "print(class_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbb0578f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': '4',\n",
       " 'md5sum': '00010a27a02be1b98537cd22e44d40a4',\n",
       " 'label': 'EDM',\n",
       " 'context': 'Read email browser 再生能源 業者 農曆 五月 五日 端午 佳節 到來 古時 稱惡 惡日 這天 驅除 瘟疫 惡運 臺灣 近期 疫情 影響 藉由 機會 驅瘟 去疫 快速 回復 自由 活動 無拘無束 生活 能源 週 呼籲 居家 追劇 喫 肉糉 盡量 外出 常備 酒精 做好 消毒 防疫 臺灣 國際 智慧 能源 週 Energy Taiwan 實體 展     虛擬 展      Tel    Email emailAddress 訂閱 取消 訂閱 Subscribe Unsubscribe'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cd5f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train : val : test = 8:1:1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, dev_test_data = train_test_split(data_list, random_state=777, train_size=0.8)\n",
    "dev_data, test_data = train_test_split(dev_test_data, random_state=777, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e5c67eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278160\n",
      "34770\n",
      "34770\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(dev_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5c0cdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to Jsonl: datasets/train_4class.jsonl\n",
      "Save to Jsonl: datasets/dev_4class.jsonl\n",
      "Save to Jsonl: datasets/test_4class.jsonl\n"
     ]
    }
   ],
   "source": [
    "from common import save_jsonl\n",
    "\n",
    "save_jsonl(train_data, 'datasets/train_4class.jsonl')\n",
    "save_jsonl(dev_data, 'datasets/dev_4class.jsonl')\n",
    "save_jsonl(test_data, 'datasets/test_4class.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef079762-b6bc-4770-bc8d-5bbb61042b77",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2712541e-ed17-4fae-a498-a962c80808fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "860e1151-9f26-4fb0-9c0a-956f89ce8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_metric\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from common import load_jsonl, save_jsonl\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    "    BertTokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c824df94-f793-411f-b1ed-a656998763f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, data_list, max_length=512, model_name=\"bert-base-multilingual-cased\"):  #bert-base-multilingual-cased\n",
    "        self.d_list = data_list\n",
    "        self.len = len(self.d_list)\n",
    "        self.max_length = max_length\n",
    "        # self.tokenizer = MBart50TokenizerFast.from_pretrained(model_name, src_lang=\"en_XX\", tgt_lang=\"ro_RO\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.label2index = {\n",
    "            'SPAM': 0,\n",
    "            'EDM': 1,\n",
    "            'HAM': 2,\n",
    "            'NOTE': 3,\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.d_list[index]\n",
    "        context = data['context']\n",
    "        label = data['label']\n",
    "        \n",
    "        processed_sample = dict()\n",
    "        processed_sample['labels'] = torch.tensor(self.label2index[label])\n",
    "        tokenized_input = self.tokenizer(context,\n",
    "                                         max_length=self.max_length,\n",
    "                                         padding='max_length', \n",
    "                                         truncation=True,\n",
    "                                         return_tensors=\"pt\")\n",
    "        \n",
    "        input_items = {key: val.squeeze() for key, val in tokenized_input.items()}\n",
    "        processed_sample.update(input_items)\n",
    "        return processed_sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b24c7d03-5cd9-4746-afa5-4d2f79506d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Jsonl: datasets/train_4class.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "278160it [00:14, 18849.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Jsonl: datasets/dev_4class.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34770it [00:01, 18877.01it/s]\n"
     ]
    }
   ],
   "source": [
    "train_list = load_jsonl('datasets/train_4class.jsonl')\n",
    "dev_list = load_jsonl('datasets/dev_4class.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65ab448b-c47a-4535-954e-a15e02356fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NLIDataset(train_list)\n",
    "dev_dataset = NLIDataset(dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd9a753c-6545-45b4-b7b4-9eaae6c15f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': '165966',\n",
       " 'md5sum': '7d224f12c9aec8a4bbcd17755f6420f3',\n",
       " 'label': 'EDM',\n",
       " 'context': ' 空氣 清淨機 排行榜 出爐 秋冬 空污 新冠肺炎 疫情 流感病毒 因素 空氣 清淨機 居家 必備 家電 行政院 消保處  抽查 檢測 市售  款 空氣 清淨機 潔淨 空氣 提供 率 CASR 排行 建議 消費者 選購 節能 標章 CASR 數值 高 產品 潔淨 空氣 提供 率 CASR 完整 內容 送 真空 蟎 吸塵器 Winix 空氣 清淨機 ZERO S   購買   APP 限時 驚喜價 Coway 濾淨 力 空氣 清淨機 AP 1216L 送 濾網    特價   3M 濾淨 型 空氣 清淨機 進階版   特價   本週 主打 送 騎士 堡 半日券 CoClean 隨身 空氣 清淨機 兒童版   特價  車用 空氣 清淨機 首選 未來 實驗室 N7 負離子 空氣 清淨機   特價  掃地 拖地 吸塵 三合一 愛迪生 三合一 智能 掃地 拖地 吸塵 機器人   特價  蹣 吸塵 機 兩用 美國 Massey 紫外線 真空 除蹣機   特價  獨家 贈三大 好禮 伊萊克斯 完美 管家 吸塵器   特價   直立 手持 兩用 丹比 DANBY 強力 旋風 有線 吸塵器   特價   質感 女神 購物節 金額 下單 抽 SOGO 禮券 累積 消費 抽  dyson 累積  單再 抽  天 免費 下午茶 大禮包 生活 分享 購多 入組 再享  回饋 購物 想 取消 訂閱 點選'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "260ed660-3f90-46e0-ae61-6b44e7650ae6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor(1),\n",
       " 'input_ids': tensor([   101,   6006,   4859,   5061,   5048,   4741,   4151,   7069,   4689,\n",
       "           2527,   5376,   5952,   2490,   6006,   4890,   4333,   2484,   6520,\n",
       "           5272,   5656,   3878,   4982,   3911,   5667,   4837,   3000,   6195,\n",
       "           6006,   4859,   5061,   5048,   4741,   3486,   3408,   3793,   2380,\n",
       "           3408,   8299,   7069,   4285,   8222,   5010,   2312,   6946,   4055,\n",
       "           4547,   4753,   5084,   3600,   2884,   4780,   6006,   4859,   5061,\n",
       "           5048,   4741,   5182,   5048,   6006,   4859,   4181,   2286,   5476,\n",
       "          92923,  11273,   4151,   7069,   3697,   7285,   5010,   7433,   6457,\n",
       "           7779,   7461,   6104,   6546,   4720,   6040,  92923,  11273,   4310,\n",
       "           2355,   8595,   5601,   2854,   5182,   5048,   6006,   4859,   4181,\n",
       "           2286,   5476,  92923,  11273,   3380,   4307,   2447,   3410,   7719,\n",
       "           5769,   6006,    100,   2800,   3135,   2970,  60987,  13274,   6006,\n",
       "           4859,   5061,   5048,   4741,    163, 104737,    156,   7461,   7431,\n",
       "          30909,  11127,   8215,   4388,   8544,   2921,   2406,  13098,  14132,\n",
       "           5232,   5048,   2594,   6006,   4859,   5061,   5048,   4741,  30909,\n",
       "          82985,  11369,   7719,   5232,   6239,   5410,   2406,    124,  11517,\n",
       "           5232,   5048,   3069,   6006,   4859,   5061,   5048,   4741,   7745,\n",
       "           8244,   5396,   5410,   2406,   4476,   7744,   2111,   4009,   7719,\n",
       "           8529,   3169,   3110,   2672,   4348,   2558,  13098,  10858,  63101,\n",
       "           8257,   7590,   6006,   4859,   5061,   5048,   4741,   2439,   6042,\n",
       "           5396,   5410,   2406,   7596,   5605,   6006,   4859,   5061,   5048,\n",
       "           4741,   8505,   7779,   4474,   2278,   3431,   8543,   3398,    151,\n",
       "          11305,   7417,   8287,   3350,   6006,   4859,   5061,   5048,   4741,\n",
       "           5410,   2406,   4146,   3035,   4069,   3035,   2800,   3135,   2077,\n",
       "           2769,   2072,   3910,   7709,   5600,   2077,   2769,   2072,   4413,\n",
       "           6546,   4146,   3035,   4069,   3035,   2800,   3135,   4741,   2970,\n",
       "           2179,   5410,   2406,    100,   2800,   3135,   4741,   2449,   5605,\n",
       "           6417,   3020,  81775,  10157,   6199,   3189,   6252,   5769,   6006,\n",
       "           8224,    100,   4741,   5410,   2406,   5467,   3408,   7465,   2077,\n",
       "           3197,   3240,   5939,   2217,   6809,   2433,   4332,   3380,   6417,\n",
       "           6098,   3408,   2800,   3135,   2970,   5410,   2406,   5759,   6033,\n",
       "           4004,   4097,   2449,   5605,   2109,   4839,  47855,  53244,  14703,\n",
       "           3729,   2594,   4341,   8445,   4461,   6252,   2800,   3135,   2970,\n",
       "           5410,   2406,   7457,   3911,   3235,   5915,   7461,   5407,   6104,\n",
       "           7911,   8399,   2079,   2928,   4055,  71020,  83821,   5939,   2558,\n",
       "           6202,   5993,   5010,   7433,   4055,  13906,  11599,   6202,   5993,\n",
       "           2928,   2475,   4055,   3198,   2435,   7433,   2079,   2670,   6739,\n",
       "           3197,   5939,   2643,   5600,   4978,   2534,   2171,   7461,   3191,\n",
       "           2446,   6209,   2475,   2171,   2999,   8482,   7461,   5407,   3898,\n",
       "           2737,   5010,   7169,   8148,   8809,   7779,    102,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad69d768-5bf4-41e4-9025-0d231d2c16a9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: 5 classes\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=4)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if torch.cuda.device_count() >1:\n",
    "    model = nn.DataParallel(model,device_ids=[0])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "acfb8614-4c87-49aa-b377-14c024b0f8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6954\n",
      "870\n"
     ]
    }
   ],
   "source": [
    "train_batch_size=40\n",
    "learning_rate=2e-5 \n",
    "train_epochs=5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "dev_dataloader = DataLoader(dev_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "print(len(train_dataloader))\n",
    "print(len(dev_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95d5fb97-7401-412c-bc22-1e57e1f0846c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([1, 0, 0, 3, 1, 1, 1, 1, 1, 0, 2, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1,\n",
      "        2, 1, 1, 1, 0, 1, 1, 2, 0, 2, 1, 1, 1, 2, 1, 1]), 'input_ids': tensor([[  101,  8148,  7290,  ...,     0,     0,     0],\n",
      "        [  101,  2774,  5611,  ...,     0,     0,     0],\n",
      "        [  101, 11520, 42492,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 12222, 10883,  ...,     0,     0,     0],\n",
      "        [  101,  8412,  5898,  ...,     0,     0,     0],\n",
      "        [  101,  4040,  2406,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for batch_index, batch_dict in enumerate(train_dataloader):\n",
    "    print(batch_dict)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe74576b-9e2b-41ac-b359-668dbfead630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ac3cd902964f448ac1d65d9bca4f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0   loss:  tensor(0.3320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0823, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0065, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.1025, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0047, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0478, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0333, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0034, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.9907103825136612\n",
      "f1:  0.9865577260484641\n",
      "epoch:  1   loss:  tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0569, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.2314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0331, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0782, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.1015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.1141, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.9922922059246477\n",
      "f1:  0.9892770313571575\n",
      "epoch:  2   loss:  tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0073, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0077, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.1184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0096, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0067, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.9927236123094622\n",
      "f1:  0.9899369831740967\n",
      "epoch:  3   loss:  tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.1939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.9928098935864251\n",
      "f1:  0.9900628767493505\n",
      "epoch:  4   loss:  tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(7.6209e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(7.5020e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.9931262582686223\n",
      "f1:  0.990482631862257\n"
     ]
    }
   ],
   "source": [
    "## 進度條\n",
    "num_training_steps = train_epochs * len(train_dataloader)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "## 設定warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "  \"linear\",\n",
    "  optimizer=optimizer,\n",
    "  num_warmup_steps=10,\n",
    "  num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "## start training\n",
    "for epoch in range(train_epochs):\n",
    "    model.train()\n",
    "    for batch_index, batch_dict in enumerate(train_dataloader):\n",
    "        \n",
    "        input_items = {key: val.to(device) for key, val in batch_dict.items()}\n",
    "        # del input_items['token_type_ids'] ## bart不需要這個\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**input_items)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        if torch.cuda.device_count() >1: ##多GPU的情況要對loss求平均\n",
    "            loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        if batch_index % 500 ==0:\n",
    "            print('epoch: ', epoch, '  loss: ', loss)\n",
    "            \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    with torch.no_grad():\n",
    "        for batch_index, batch_dict in enumerate(dev_dataloader):\n",
    "            input_items = {key: val.to(device) for key, val in batch_dict.items()}\n",
    "            outputs = model(**input_items)\n",
    "\n",
    "            predictions += outputs.logits.argmax(dim=-1).tolist()\n",
    "            references += batch_dict['labels'].tolist()\n",
    "\n",
    "    accuracy = accuracy_score(references, predictions)\n",
    "    f1 = f1_score(references, predictions,average='macro')\n",
    "    print('acc: ', accuracy)\n",
    "    print('f1: ', f1)\n",
    "    \n",
    "    ## save model\n",
    "    save_path = 'model/notice_4class_epoch_' + str(epoch+1)\n",
    "    if torch.cuda.device_count() >1:\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(save_path)\n",
    "    else:\n",
    "        model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c2c796",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "916674e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Jsonl: datasets/test_4class.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1921it [00:00, 19205.26it/s]\u001b[A\n",
      "3842it [00:00, 18814.02it/s]\u001b[A\n",
      "5724it [00:00, 18349.12it/s]\u001b[A\n",
      "7561it [00:00, 18339.80it/s]\u001b[A\n",
      "9396it [00:00, 18081.26it/s]\u001b[A\n",
      "11205it [00:00, 18048.27it/s]\u001b[A\n",
      "13136it [00:00, 18453.79it/s]\u001b[A\n",
      "14994it [00:00, 18488.27it/s]\u001b[A\n",
      "16844it [00:00, 18184.13it/s]\u001b[A\n",
      "18664it [00:01, 17958.65it/s]\u001b[A\n",
      "20636it [00:01, 18485.80it/s]\u001b[A\n",
      "22487it [00:01, 18315.68it/s]\u001b[A\n",
      "24321it [00:01, 18177.85it/s]\u001b[A\n",
      "26140it [00:01, 17926.95it/s]\u001b[A\n",
      "28057it [00:01, 18292.39it/s]\u001b[A\n",
      "29888it [00:01, 18159.59it/s]\u001b[A\n",
      "31735it [00:01, 18245.77it/s]\u001b[A\n",
      "34770it [00:01, 18240.73it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from common import load_jsonl\n",
    "test_list = load_jsonl('datasets/test_4class.jsonl')\n",
    "test_dataset = NLIDataset(test_list)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aecbc0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34770\n"
     ]
    }
   ],
   "source": [
    "print(len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f36c4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: 5 classes\n",
    "model_path = 'model/notice_4class_epoch_5'\n",
    "# model_path = 'models/Mail_Classifier/epoch_5'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=4)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")   \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1af166d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c972517eab3b49eaa91fa3c5855e530f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.9930974978429681\n",
      "f1:  0.9898595974492835\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "references = []\n",
    "num_steps = len(test_dataloader)\n",
    "progress_bar = tqdm(range(num_steps))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_index, batch_dict in enumerate(test_dataloader):\n",
    "        input_items = {key: val.to(device) for key, val in batch_dict.items()}\n",
    "        outputs = model(**input_items)\n",
    "\n",
    "        predictions += outputs.logits.argmax(dim=-1).tolist()\n",
    "        references += batch_dict['labels'].tolist()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "accuracy = accuracy_score(references, predictions)\n",
    "f1 = f1_score(references, predictions,average='macro')\n",
    "print('acc: ', accuracy)\n",
    "print('f1: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57ec3d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/notice_4class_epoch_5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      3855\n",
      "           1       1.00      1.00      1.00     22001\n",
      "           2       0.99      0.99      0.99      6788\n",
      "           3       0.99      1.00      1.00      2126\n",
      "\n",
      "    accuracy                           0.99     34770\n",
      "   macro avg       0.99      0.99      0.99     34770\n",
      "weighted avg       0.99      0.99      0.99     34770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_path)\n",
    "print(classification_report(references, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2ce634",
   "metadata": {},
   "source": [
    "## find out class 3 (note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c6190dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34770\n",
      "in testdatasets , there are 3855 SPAM messages ~\n",
      "the accurate of SPAM is : 0.95\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import operator\n",
    "class_number = {'SPAM':0, 'EDM':1, 'HAM':2, 'NOTE':3}\n",
    "\n",
    "count = 0\n",
    "wrong = 0\n",
    "right = 0\n",
    "which_class = 0\n",
    "\n",
    "print(len(references))\n",
    "\n",
    "for i, class_num in enumerate(references):\n",
    "    if class_num == which_class:\n",
    "        count += 1\n",
    "        if predictions[i] != which_class:\n",
    "            wrong += 1\n",
    "            # print(test_list[i]['context'])\n",
    "        else:\n",
    "            right += 1\n",
    "    \n",
    "print('in testdatasets , there are %d %s messages ~'%(count, [k for k, v in class_number.items() if v == which_class][0]))\n",
    "print('the accurate of %s is : %.2f'%([k for k, v in class_number.items() if v == which_class][0], (right - wrong)/count))\n",
    "       \n",
    "\n",
    "        \n",
    "# print('in testdatasets , there are %d notice messages ~')\n",
    "\n",
    "# print(test_list[i]['context'])\n",
    "# print(predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a1a3ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34770\n",
      "in testdatasets , there are 22001 EDM messages ~\n",
      "the accurate of EDM is : 0.99\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import operator\n",
    "class_number = {'SPAM':0, 'EDM':1, 'HAM':2, 'NOTE':3}\n",
    "\n",
    "count = 0\n",
    "wrong = 0\n",
    "right = 0\n",
    "which_class = 1\n",
    "\n",
    "print(len(references))\n",
    "\n",
    "for i, class_num in enumerate(references):\n",
    "    if class_num == which_class:\n",
    "        count += 1\n",
    "        if predictions[i] != which_class:\n",
    "            wrong += 1\n",
    "            # print(test_list[i]['context'])\n",
    "        else:\n",
    "            right += 1\n",
    "    \n",
    "print('in testdatasets , there are %d %s messages ~'%(count, [k for k, v in class_number.items() if v == which_class][0]))\n",
    "print('the accurate of %s is : %.2f'%([k for k, v in class_number.items() if v == which_class][0], (right - wrong)/count))\n",
    "       \n",
    "\n",
    "        \n",
    "# print('in testdatasets , there are %d notice messages ~')\n",
    "\n",
    "# print(test_list[i]['context'])\n",
    "# print(predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2942780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34770\n",
      "in testdatasets , there are 6788 HAM messages ~\n",
      "the accurate of HAM is : 0.99\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import operator\n",
    "class_number = {'SPAM':0, 'EDM':1, 'HAM':2, 'NOTE':3}\n",
    "\n",
    "count = 0\n",
    "wrong = 0\n",
    "right = 0\n",
    "which_class = 2\n",
    "\n",
    "print(len(references))\n",
    "\n",
    "for i, class_num in enumerate(references):\n",
    "    if class_num == which_class:\n",
    "        count += 1\n",
    "        if predictions[i] != which_class:\n",
    "            wrong += 1\n",
    "            # print(test_list[i]['context'])\n",
    "        else:\n",
    "            right += 1\n",
    "    \n",
    "print('in testdatasets , there are %d %s messages ~'%(count, [k for k, v in class_number.items() if v == which_class][0]))\n",
    "print('the accurate of %s is : %.2f'%([k for k, v in class_number.items() if v == which_class][0], (right - wrong)/count))\n",
    "       \n",
    "\n",
    "        \n",
    "# print('in testdatasets , there are %d notice messages ~')\n",
    "\n",
    "# print(test_list[i]['context'])\n",
    "# print(predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16f9a862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34770\n",
      "in testdatasets , there are 2126 NOTE messages ~\n",
      "the accurate of NOTE is : 1.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import operator\n",
    "class_number = {'SPAM':0, 'EDM':1, 'HAM':2, 'NOTE':3}\n",
    "\n",
    "count = 0\n",
    "wrong = 0\n",
    "right = 0\n",
    "which_class = 3\n",
    "\n",
    "print(len(references))\n",
    "\n",
    "for i, class_num in enumerate(references):\n",
    "    if class_num == which_class:\n",
    "        count += 1\n",
    "        if predictions[i] != which_class:\n",
    "            wrong += 1\n",
    "            # print(test_list[i]['context'])\n",
    "        else:\n",
    "            right += 1\n",
    "    \n",
    "print('in testdatasets , there are %d %s messages ~'%(count, [k for k, v in class_number.items() if v == which_class][0]))\n",
    "print('the accurate of %s is : %.2f'%([k for k, v in class_number.items() if v == which_class][0], (right - wrong)/count))\n",
    "       \n",
    "\n",
    "        \n",
    "# print('in testdatasets , there are %d notice messages ~')\n",
    "\n",
    "# print(test_list[i]['context'])\n",
    "# print(predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4929c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1d93b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bfc4f72",
   "metadata": {},
   "source": [
    "## find out class 4 (hacker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb50acf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3806\n",
      "this sentence 17 is incorrect ! \n",
      "this sentence 75 is incorrect ! \n",
      "this sentence 210 is incorrect ! \n",
      "this sentence 224 is incorrect ! \n",
      "this sentence 237 is incorrect ! \n",
      "this sentence 259 is incorrect ! \n",
      "this sentence 261 is incorrect ! \n",
      "this sentence 275 is incorrect ! \n",
      "this sentence 319 is incorrect ! \n",
      "this sentence 320 is incorrect ! \n",
      "this sentence 339 is incorrect ! \n",
      "this sentence 440 is incorrect ! \n",
      "this sentence 453 is incorrect ! \n",
      "this sentence 457 is incorrect ! \n",
      "this sentence 478 is incorrect ! \n",
      "this sentence 503 is incorrect ! \n",
      "this sentence 590 is incorrect ! \n",
      "this sentence 607 is incorrect ! \n",
      "this sentence 710 is incorrect ! \n",
      "this sentence 731 is incorrect ! \n",
      "this sentence 747 is incorrect ! \n",
      "this sentence 837 is incorrect ! \n",
      "this sentence 878 is incorrect ! \n",
      "this sentence 933 is incorrect ! \n",
      "this sentence 1003 is incorrect ! \n",
      "this sentence 1016 is incorrect ! \n",
      "this sentence 1119 is incorrect ! \n",
      "this sentence 1120 is incorrect ! \n",
      "this sentence 1147 is incorrect ! \n",
      "this sentence 1153 is incorrect ! \n",
      "this sentence 1200 is incorrect ! \n",
      "this sentence 1247 is incorrect ! \n",
      "this sentence 1263 is incorrect ! \n",
      "this sentence 1283 is incorrect ! \n",
      "this sentence 1293 is incorrect ! \n",
      "this sentence 1304 is incorrect ! \n",
      "this sentence 1325 is incorrect ! \n",
      "this sentence 1333 is incorrect ! \n",
      "this sentence 1334 is incorrect ! \n",
      "this sentence 1336 is incorrect ! \n",
      "this sentence 1389 is incorrect ! \n",
      "this sentence 1391 is incorrect ! \n",
      "this sentence 1404 is incorrect ! \n",
      "this sentence 1509 is incorrect ! \n",
      "this sentence 1512 is incorrect ! \n",
      "this sentence 1525 is incorrect ! \n",
      "this sentence 1528 is incorrect ! \n",
      "this sentence 1610 is incorrect ! \n",
      "this sentence 1638 is incorrect ! \n",
      "this sentence 1642 is incorrect ! \n",
      "this sentence 1658 is incorrect ! \n",
      "this sentence 1662 is incorrect ! \n",
      "this sentence 1732 is incorrect ! \n",
      "this sentence 1757 is incorrect ! \n",
      "this sentence 1775 is incorrect ! \n",
      "this sentence 1789 is incorrect ! \n",
      "this sentence 1802 is incorrect ! \n",
      "this sentence 1826 is incorrect ! \n",
      "this sentence 1884 is incorrect ! \n",
      "this sentence 1921 is incorrect ! \n",
      "this sentence 1978 is incorrect ! \n",
      "this sentence 1999 is incorrect ! \n",
      "this sentence 2029 is incorrect ! \n",
      "this sentence 2078 is incorrect ! \n",
      "this sentence 2136 is incorrect ! \n",
      "this sentence 2169 is incorrect ! \n",
      "this sentence 2173 is incorrect ! \n",
      "this sentence 2203 is incorrect ! \n",
      "this sentence 2234 is incorrect ! \n",
      "this sentence 2332 is incorrect ! \n",
      "this sentence 2373 is incorrect ! \n",
      "this sentence 2384 is incorrect ! \n",
      "this sentence 2392 is incorrect ! \n",
      "this sentence 2419 is incorrect ! \n",
      "this sentence 2421 is incorrect ! \n",
      "this sentence 2472 is incorrect ! \n",
      "this sentence 2517 is incorrect ! \n",
      "this sentence 2554 is incorrect ! \n",
      "this sentence 2577 is incorrect ! \n",
      "this sentence 2628 is incorrect ! \n",
      "this sentence 2697 is incorrect ! \n",
      "this sentence 2735 is incorrect ! \n",
      "this sentence 2740 is incorrect ! \n",
      "this sentence 2781 is incorrect ! \n",
      "this sentence 2793 is incorrect ! \n",
      "this sentence 2810 is incorrect ! \n",
      "this sentence 2834 is incorrect ! \n",
      "this sentence 2842 is incorrect ! \n",
      "this sentence 2848 is incorrect ! \n",
      "this sentence 2865 is incorrect ! \n",
      "this sentence 2868 is incorrect ! \n",
      "this sentence 2900 is incorrect ! \n",
      "this sentence 2948 is incorrect ! \n",
      "this sentence 3056 is incorrect ! \n",
      "this sentence 3141 is incorrect ! \n",
      "this sentence 3150 is incorrect ! \n",
      "this sentence 3165 is incorrect ! \n",
      "this sentence 3192 is incorrect ! \n",
      "this sentence 3272 is incorrect ! \n",
      "this sentence 3358 is incorrect ! \n",
      "this sentence 3491 is incorrect ! \n",
      "this sentence 3507 is incorrect ! \n",
      "this sentence 3616 is incorrect ! \n",
      "this sentence 3646 is incorrect ! \n",
      "this sentence 3651 is incorrect ! \n",
      "this sentence 3714 is incorrect ! \n",
      "this sentence 3726 is incorrect ! \n",
      "this sentence 3747 is incorrect ! \n",
      "this sentence 3762 is incorrect ! \n",
      "this sentence 3778 is incorrect ! \n",
      "this sentence 3791 is incorrect ! \n",
      "in testdatasets , there are 126 HACKER messages ~\n",
      "the accurate of HACKER is : -0.76\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import operator\n",
    "class_number = {'SPAM':0, 'HACKER':1}\n",
    "\n",
    "count = 0\n",
    "wrong = 0\n",
    "right = 0\n",
    "which_class = 1\n",
    "wrong_list, pred_list, ref_list = [], [], []\n",
    "\n",
    "print(len(references))\n",
    "\n",
    "for i, class_num in enumerate(references):\n",
    "    if class_num == which_class:\n",
    "        count += 1\n",
    "        if predictions[i] != which_class:\n",
    "            print(\"this sentence %s is incorrect ! \"%(i))\n",
    "            wrong += 1\n",
    "            wrong_list.append(test_list[i]['context'])\n",
    "            pred_list.append(predictions[i])\n",
    "            ref_list.append(references[i])\n",
    "            # print(test_list[i]['context'])\n",
    "        else:\n",
    "            right += 1\n",
    "\n",
    "output_dic = {}\n",
    "output_dic['pred'] = pred_list\n",
    "output_dic['label'] = ref_list\n",
    "output_dic['context'] = wrong_list\n",
    "output = pd.DataFrame(output_dic)\n",
    "output.to_csv(\"wrong_list.csv\", encoding = 'utf-8-sig')\n",
    "    \n",
    "print('in testdatasets , there are %d %s messages ~'%(count, [k for k, v in class_number.items() if v == which_class][0]))\n",
    "print('the accurate of %s is : %.2f'%([k for k, v in class_number.items() if v == which_class][0], (right - wrong)/count))\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8de76",
   "metadata": {},
   "source": [
    "## predict from batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f1f6e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soc507/anaconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 1, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import operator\n",
    "class_number = {'SPAM':0, 'EDM':1, 'HAM':2, 'NOTE':3, 'HACKER':4}\n",
    "model.eval()\n",
    "\n",
    "softmax=torch.nn.Softmax()\n",
    "csvfile = pd.read_csv('test_data/notice.csv')    # change the value up to the colab limit\n",
    "\n",
    "predict_list = []\n",
    "score_list = []\n",
    "sm_score_list = []\n",
    "for token in csvfile['context']:\n",
    "\n",
    "    tokenized_input = tokenizer(token,\n",
    "                                max_length=20,\n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        input_items = {key: val.to(device) for key, val in tokenized_input.items()}\n",
    "#         del input_items['token_type_ids'] ## bart不需要這個\n",
    "\n",
    "        outputs = model(**input_items)\n",
    "        prediction = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        prediction = int(prediction)\n",
    "        sm = softmax(outputs.logits[0])\n",
    "\n",
    "        predict_list.append(prediction)\n",
    "        score_list.append(outputs.logits[0][prediction].item())\n",
    "        sm_score_list.append(sm[prediction].item())\n",
    "        \n",
    "print(predict_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff89fe0",
   "metadata": {},
   "source": [
    "## predict from each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a2b21f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "內文:  EyeCloud  親愛的  林  淵博  設備  狀態  切換  資訊  設備  名稱  華府  總部  伺服器  型號  UR    隸屬  羣組  設備  狀態  切換  信件  EyeCloud  自動  發送  回覆  本信件  登入  EyeCloud  查看  管理員  謝謝  UrlText\n",
      "label:  3 NOTE\n",
      "predict:  3 NOTE\n"
     ]
    }
   ],
   "source": [
    "test_num = 10\n",
    "\n",
    "# subject = test_list[test_num]['subject']\n",
    "context = test_list[test_num]['context']\n",
    "label = test_list[test_num]['label']\n",
    "\n",
    "tokenized_input = tokenizer(context,\n",
    "                            max_length=512,\n",
    "                            truncation=True,\n",
    "                            return_tensors=\"pt\")\n",
    "class_number = {'SPAM':0, 'EDM':1, 'HAM':2, 'NOTE':3, 'KACKER':4}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input_items = {key: val.to(device) for key, val in tokenized_input.items()}\n",
    "    del input_items['token_type_ids'] ## bart不需要這個\n",
    "    \n",
    "    outputs = model(**input_items)\n",
    "    prediction = outputs.logits.argmax(dim=-1)\n",
    "    print(type(int(prediction)))\n",
    "    \n",
    "    \n",
    "    # print('主旨: ', subject)\n",
    "    print('內文: ', context)\n",
    "    print('label: ', class_number[label], label)\n",
    "    print('predict: ', int(prediction), list(class_number.keys())[list(class_number.values()).index(int(prediction))])\n",
    "    \n",
    "    # if int(prediction) == 0:\n",
    "    #     print('predict: ham')\n",
    "    # else:\n",
    "    #     print('predict: spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93f2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
