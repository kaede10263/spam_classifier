{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "898780a9",
   "metadata": {},
   "source": [
    "## datasets prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de055d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datasets \n",
    "\n",
    "import csv\n",
    "csv.field_size_limit(500 * 1024 * 1024)\n",
    "\n",
    "data_list = []\n",
    "class_number = {'SPAM':0, 'EDM':0, 'HAM':0, 'NOTE':0, 'HACK':0}\n",
    "\n",
    "with open('spam_data_append_v3.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    # count = 0\n",
    "   \n",
    "    rows = csv.reader(csvfile)\n",
    "    for i, row in enumerate(rows):\n",
    "        if row[2] in class_number.keys():\n",
    "            pass\n",
    "        else:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa9fd8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "\n",
    "import csv\n",
    "csv.field_size_limit(500 * 1024 * 1024)\n",
    "\n",
    "data_list = []\n",
    "class_number = {'SPAM':0, 'EDM':0, 'HAM':0, 'NOTE':0, 'HACK':0}\n",
    "\n",
    "with open('spam_data_append.csv', newline='', encoding='utf-8') as csvfile:\n",
    "    \n",
    "    \n",
    "   \n",
    "    rows = csv.reader(csvfile)\n",
    "    for i, row in enumerate(rows):\n",
    "        # print(row[0])\n",
    "        if i != 0:\n",
    "            context = row[3].replace('Num','')\n",
    "            class_number[row[2]]+=1\n",
    "            data_list.append({'index': row[0], \n",
    "                              'md5sum': row[1],\n",
    "                              'label':row[2], \n",
    "                              'context':context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "088e2748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347700"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of data\n",
    "\n",
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1f36575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SPAM': 36796, 'EDM': 218647, 'HAM': 68952, 'NOTE': 22048, 'HACK': 1257}\n"
     ]
    }
   ],
   "source": [
    "# number of data for each class\n",
    "\n",
    "print(class_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbb0578f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': '4',\n",
       " 'md5sum': '00010a27a02be1b98537cd22e44d40a4',\n",
       " 'label': 'EDM',\n",
       " 'context': 'Read email browser 再生能源 業者 農曆 五月 五日 端午 佳節 到來 古時 稱惡 惡日 這天 驅除 瘟疫 惡運 臺灣 近期 疫情 影響 藉由 機會 驅瘟 去疫 快速 回復 自由 活動 無拘無束 生活 能源 週 呼籲 居家 追劇 喫 肉糉 盡量 外出 常備 酒精 做好 消毒 防疫 臺灣 國際 智慧 能源 週 Energy Taiwan 實體 展     虛擬 展      Tel    Email emailAddress 訂閱 取消 訂閱 Subscribe Unsubscribe'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0cd5f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train : val : test = 8:1:1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, dev_test_data = train_test_split(data_list, random_state=777, train_size=0.8)\n",
    "dev_data, test_data = train_test_split(dev_test_data, random_state=777, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e5c67eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278344\n",
      "34793\n",
      "34793\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(dev_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c5c0cdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to Jsonl: datasets/train_v3.jsonl\n",
      "Save to Jsonl: datasets/dev_v3.jsonl\n",
      "Save to Jsonl: datasets/test_v3.jsonl\n"
     ]
    }
   ],
   "source": [
    "from common import save_jsonl\n",
    "\n",
    "save_jsonl(train_data, 'datasets/train_v3.jsonl')\n",
    "save_jsonl(dev_data, 'datasets/dev_v3.jsonl')\n",
    "save_jsonl(test_data, 'datasets/test_v3.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef079762-b6bc-4770-bc8d-5bbb61042b77",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2712541e-ed17-4fae-a498-a962c80808fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "860e1151-9f26-4fb0-9c0a-956f89ce8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_metric\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from common import load_jsonl, save_jsonl\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    "    BertTokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorForTokenClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c824df94-f793-411f-b1ed-a656998763f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, data_list, max_length=512, model_name=\"bert-base-multilingual-cased\"):\n",
    "        self.d_list = data_list\n",
    "        self.len = len(self.d_list)\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.label2index = {\n",
    "            'SPAM': 0,\n",
    "            'EDM': 1,\n",
    "            'HAM': 2,\n",
    "            'NOTE': 3,\n",
    "            'HACK': 4,\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.d_list[index]\n",
    "        context = data['context']\n",
    "        label = data['label']\n",
    "        \n",
    "        processed_sample = dict()\n",
    "        processed_sample['labels'] = torch.tensor(self.label2index[label])\n",
    "        tokenized_input = self.tokenizer(context,\n",
    "                                         max_length=self.max_length,\n",
    "                                         padding='max_length', \n",
    "                                         truncation=True,\n",
    "                                         return_tensors=\"pt\")\n",
    "        \n",
    "        input_items = {key: val.squeeze() for key, val in tokenized_input.items()}\n",
    "        processed_sample.update(input_items)\n",
    "        return processed_sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b24c7d03-5cd9-4746-afa5-4d2f79506d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Jsonl: datasets/train_v3.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "278344it [00:16, 16685.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Jsonl: datasets/dev_v3.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34793it [00:02, 17263.23it/s]\n"
     ]
    }
   ],
   "source": [
    "train_list = load_jsonl('datasets/train_v3.jsonl')\n",
    "dev_list = load_jsonl('datasets/dev_v3.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "65ab448b-c47a-4535-954e-a15e02356fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NLIDataset(train_list)\n",
    "dev_dataset = NLIDataset(dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd9a753c-6545-45b4-b7b4-9eaae6c15f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': '182241',\n",
       " 'md5sum': '8975620867bd9c129fcea8cfe71290fb',\n",
       " 'label': 'EDM',\n",
       " 'context': '親愛的 Email emailAddress 訂閱 生活 市集 電子報 封信 誤判為 垃圾信 點選 垃圾郵件 好康 天天 抽 機會 抽到 肯德基 蛋塔 熱門 銷售 排行榜 X TOP  熱銷 No  想給 優秀品質 兼具 親民 價格 衛生紙 非 生活 市集 莫屬 生活 市集 自有 品牌 抽取式 溶水 衛生紙 擁有 五大 保證 柔軟 觸感  原生 紙漿 含 螢光劑  臺灣 製造 溶於 水 衛生紙 搭配 可愛 夯吉 包裝 實用 療癒 物超所值   搶購  日本 幫寶適 特規 增量 尿布   搶購  Switch 主機 健身 環 遊戲組   搶購  Dyson Digital Slim 吸塵   搶購  白熊 溶水 抽取式 衛生紙   搶購  卜蜂 經典 佐 義法 嫩 雞胸肉   搶購  唯潔雅 優質 抽取式 衛生紙   搶購  康乃馨 Hi water 水 濕巾   搶購  水嫩 舒肥 雞胸 即食 隨手包   搶購  臺中 逢甲 夜市 炳 叔 烤 玉米   搶購 搶購 中 倒數  小時 熱銷 No  TZUMii 日式 空間 收納櫃 衣帽架 臺灣 製造 SGS 安心 認證 日式 沉穩 風格 獨家 設計 造型 把手 適合 居家 風格 超大 收納 空間 開放式 收納 置物 取物 一眼 明瞭 衣帽架 衣櫥 任選 高 CP值 收納 生活 環境 整齊 清爽   搶購  日本 大王 迪士尼 褲型 尿布   搶購  灰 絨毛 四層 爬架 貓 跳臺   搶購  復 易 防漏 成人 紙尿褲   搶購  抽屜式 分層 收納 隔板 架   搶購  Kiehl 激光 淡斑 精華   搶購  小妖 機全 摺疊 跑步機 M7   搶購  蝶型 防 駝背 脊椎 護腰 坐墊   搶購  無印 風 純白 抽屜 收納 層櫃   搶購  碎花 蝴蝶結 亞麻 手工 拖鞋   搶購 商品 嚴選 ON SALE 熱銷 No  日本 幫寶適 特規版 拉拉 褲 日本 原裝 進口 特製 吸收 層 快速 吸收 水分 屁屁 乾爽 舒適 穿 拉拉 褲 穿脫 穿 好動 寶寶 超自 孩子 選擇   搶購  泰凱 食堂 麻油 猴頭 杏鮑菇   搶購  4D 太空 回彈 加厚 記憶 牀墊   搶購  國際 牌高 滲透 奈米 吹風機   搶購  生活 市集 吸水 廚房 紙巾   搶購  NN yummy 份量 舒肥 雞胸   搶購  阪原 百變 桌板 折疊 收納箱   搶購  MIT 防爆 瓶 滅火器   搶購  鍋寶 金鑽 沾 經典 八件 組   搶購  3M 細滑 牙線棒 超值 分享 包   搶購 創業家 兄弟 股份 有限公司 emojiPic 版權所有  Kuobrothers Corp All Rights Reserved 郵件 系統 自動 發送 回覆 取消 訂閱 電子報'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "260ed660-3f90-46e0-ae61-6b44e7650ae6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor(1),\n",
       " 'input_ids': tensor([  101,  7150,  3910,  5718, 11289, 35240, 79515, 10738, 12969, 30743,\n",
       "          7169,  8148,  5600,  4978,  3600,  8272,  8299,  3350,  3115,  3448,\n",
       "          2316,  7224,  2549,  5287,  3066,  3039,  2316,  8809,  7779,  3066,\n",
       "          3039,  7835,  2210,  3240,  3676,  3198,  3198,  4055,  4741,  4459,\n",
       "          4055,  2555,  6516,  3789,  3099,  6980,  3124,  5351,  8129,  7959,\n",
       "          2884,  4151,  7069,  4689,   161, 98151,  5351,  7959, 10657,  3898,\n",
       "          6217,  2419,  5948,  2854,  7457,  2465,  2461,  7150,  4851,  2406,\n",
       "          4580,  7075,  5600,  6192,  8332,  5600,  4978,  3600,  8272,  6776,\n",
       "          3502,  5600,  4978,  3600,  8272,  6621,  4461,  2854,  5397,  4055,\n",
       "          2737,  3709,  5122,  4867,  7075,  5600,  6192,  4245,  4461,  2155,\n",
       "          3197,  2312,  7276,  4540,  7603,  7167,  3911,  2715,  5600,  6192,\n",
       "          5177,  2793,  7041,  2432,  2593,  6625,  5255,  7120,  7740,  5122,\n",
       "          4336,  4867,  7075,  5600,  6192,  4204,  7860,  2756,  3910,  3203,\n",
       "          2770,  2643,  7111,  3431,  5605,  5699,  5702,  5407,  7535,  3999,\n",
       "          2355,  4205,  7461,  4348,  4476,  3639,  3440,  7772,  5410,  7144,\n",
       "          3148,  7909,  3482,  3601,  4205,  7461, 80072,  2111,  4741,  2365,\n",
       "          7590,  5576,  7755,  3990,  6209,  4205,  7461, 89944, 11599, 13828,\n",
       "         66852,  2800,  3135,  4205,  7461,  5715,  5344,  5122,  4867,  4055,\n",
       "          2737,  3709,  7075,  5600,  6192,  4205,  7461,  2685,  6998,  6228,\n",
       "          2462,  2249,  6428,  4941,  3336,  8286,  6544,  6501,  4205,  7461,\n",
       "          2885,  5182,  8271,  2419,  7457,  4055,  2737,  3709,  7075,  5600,\n",
       "          6192,  4205,  7461,  3676,  2116,  8509, 20065, 12286,  4867,  5219,\n",
       "          3598,  4205,  7461,  4867,  3336,  6639,  6513,  8286,  6544,  2697,\n",
       "          8458,  8257,  4004,  2643,  4205,  7461,  6625,  2104,  7741,  5612,\n",
       "          3192,  3600,  5284,  2736,  5299,  5477,  6140,  4205,  7461,  4205,\n",
       "          7461,  2104,  2336,  4310,  3459,  4388,  5351,  7959, 10657,   157,\n",
       "         13966, 72679, 11477,  4348,  3709,  6006,  8137,  4280,  6186,  4760,\n",
       "          7080,  3627,  4532,  6625,  5255,  7120,  7740, 34415, 10731,  3378,\n",
       "          3792,  7215,  7276,  4348,  3709,  4907,  5999,  8445,  4580,  5467,\n",
       "          3408,  7184,  7171,  7740,  3069,  4033,  4004,  7772,  2769,  3486,\n",
       "          3408,  8445,  4580,  7535,  3197,  4280,  6186,  6006,  8137,  8133,\n",
       "          4284,  3709,  4280,  6186,  6406,  5407,  2737,  5407,  2072,  5776,\n",
       "          4368,  5805,  7080,  3627,  4532,  7080,  4765,  2212,  7779,  8595,\n",
       "         40070,  2355,  4280,  6186,  5600,  4978,  5576,  3139,  4307,  8821,\n",
       "          5061,  5392,  4205,  7461,  4348,  4476,  3197,  5478,  7709,  3169,\n",
       "          3479,  7129,  3069,  3482,  3601,  4205,  7461,  5260,  6218,  4844,\n",
       "          2998,  3500,  5380,  4532,  7413,  7560,  6625,  4205,  7461,  3782,\n",
       "          4370,  8198,  5159,  3975,  2179,  6192,  3482,  7129,  4205,  7461,\n",
       "          4055,   100,  3709,  2534,  3500,  4280,  6186,  8247,  4512,  4532,\n",
       "          4205,  7461, 28941, 25723, 10161,  5213,  2432,  5045,  4319,  6162,\n",
       "          6800,  4205,  7461,  3459,  3251,  4741,  2448,  4222,  5646,  7550,\n",
       "          4793,  4741,   150, 11305,  4205,  7461,  7033,  3069,  8198,  8525,\n",
       "          6529,  6553,  4650,  7288,  6583,  3047,  3142,  4205,  7461,  5318,\n",
       "          2695,  8445,  6189,  5715,  4055,   100,  4280,  6186,  3500,  4760,\n",
       "          4205,  7461,  5862,  6691,  7032,  7033,  6212,  2161,  8792,  4004,\n",
       "          3584,  4069,  8347,  4205,  7461,  2890,  2854,  2987,  7779, 49339,\n",
       "         21318, 51036,  5351,  7959, 10657,  4348,  4476,  3639,  3440,  7772,\n",
       "          5410,  7144,  5396,  4060,  4060,  7129,  4348,  4476,  2715,  7111,\n",
       "          7745,  2746,  5410,  7120,  2800,  4280,  3500,  3806,  7739,  2800,\n",
       "          4280,  4867,  2534,  3484,  3484,  2143,  5392,  6639,  7772,  6007,\n",
       "          4060,  4060,  7129,  6007,  6561,  6007,  3240,  2621,  3440,  3440,\n",
       "          7535,   102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad69d768-5bf4-41e4-9025-0d231d2c16a9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: 5 classes\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=5)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if torch.cuda.device_count() >1:\n",
    "    model = nn.DataParallel(model,device_ids=[0])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "acfb8614-4c87-49aa-b377-14c024b0f8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6959\n",
      "870\n"
     ]
    }
   ],
   "source": [
    "train_batch_size=40\n",
    "learning_rate=2e-5 \n",
    "train_epochs=5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "dev_dataloader = DataLoader(dev_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "print(len(train_dataloader))\n",
    "print(len(dev_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95d5fb97-7401-412c-bc22-1e57e1f0846c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([1, 1, 2, 1, 1, 3, 1, 1, 3, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2,\n",
      "        1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2]), 'input_ids': tensor([[ 101, 6252, 2078,  ..., 2341, 5512,  102],\n",
      "        [ 101, 8148, 7290,  ...,    0,    0,    0],\n",
      "        [ 101, 5477, 3504,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 6252, 2078,  ...,    0,    0,    0],\n",
      "        [ 101, 5351, 7454,  ...,    0,    0,    0],\n",
      "        [ 101, 4280, 4780,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for batch_index, batch_dict in enumerate(train_dataloader):\n",
    "    print(batch_dict)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe74576b-9e2b-41ac-b359-668dbfead630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34795 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0   loss:  tensor(1.6502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.2305, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0221, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0394, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.2245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.9884459517719082\n",
      "f1:  0.9510668549585333\n",
      "epoch:  1   loss:  tensor(0.0151, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0142, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.1051, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0845, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0316, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.9917224729112177\n",
      "f1:  0.9676108435930807\n",
      "epoch:  2   loss:  tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0283, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0075, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0310, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.9927571637973156\n",
      "f1:  0.9744873080652935\n",
      "epoch:  3   loss:  tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0045, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0054, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.1536, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0338, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.9929583536918346\n",
      "f1:  0.9722034516740315\n",
      "epoch:  4   loss:  tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(9.9357e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.9933319920673699\n",
      "f1:  0.9728663230696419\n"
     ]
    }
   ],
   "source": [
    "## 進度條\n",
    "num_training_steps = train_epochs * len(train_dataloader)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "## 設定warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "  \"linear\",\n",
    "  optimizer=optimizer,\n",
    "  num_warmup_steps=10,\n",
    "  num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "## start training\n",
    "for epoch in range(train_epochs):\n",
    "    model.train()\n",
    "    for batch_index, batch_dict in enumerate(train_dataloader):\n",
    "        \n",
    "        input_items = {key: val.to(device) for key, val in batch_dict.items()}\n",
    "#         del input_items['token_type_ids'] ## bart不需要這個\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**input_items)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        if torch.cuda.device_count() >1: ##多GPU的情況要對loss求平均\n",
    "            loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        if batch_index % 500 ==0:\n",
    "            print('epoch: ', epoch, '  loss: ', loss)\n",
    "            \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    with torch.no_grad():\n",
    "        for batch_index, batch_dict in enumerate(dev_dataloader):\n",
    "            input_items = {key: val.to(device) for key, val in batch_dict.items()}\n",
    "            outputs = model(**input_items)\n",
    "\n",
    "            predictions += outputs.logits.argmax(dim=-1).tolist()\n",
    "            references += batch_dict['labels'].tolist()\n",
    "\n",
    "    accuracy = accuracy_score(references, predictions)\n",
    "    f1 = f1_score(references, predictions,average='macro')\n",
    "    print('acc: ', accuracy)\n",
    "    print('f1: ', f1)\n",
    "    \n",
    "    ## save model\n",
    "    save_path = 'model/notice_v3_epoch_' + str(epoch+1)\n",
    "    if torch.cuda.device_count() >1:\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(save_path)\n",
    "    else:\n",
    "        model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c2c796",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "916674e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Jsonl: datasets/test_v2.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1528it [00:00, 15273.86it/s]\u001b[A\n",
      "3056it [00:00, 13965.51it/s]\u001b[A\n",
      "4793it [00:00, 15435.88it/s]\u001b[A\n",
      "6545it [00:00, 16233.21it/s]\u001b[A\n",
      "8178it [00:00, 16016.98it/s]\u001b[A\n",
      "9902it [00:00, 16416.35it/s]\u001b[A\n",
      "11559it [00:00, 16465.23it/s]\u001b[A\n",
      "13315it [00:00, 16809.40it/s]\u001b[A\n",
      "14999it [00:00, 16593.64it/s]\u001b[A\n",
      "16661it [00:01, 16599.27it/s]\u001b[A\n",
      "18358it [00:01, 16711.31it/s]\u001b[A\n",
      "20052it [00:01, 16779.38it/s]\u001b[A\n",
      "21731it [00:01, 16612.61it/s]\u001b[A\n",
      "23397it [00:01, 16626.13it/s]\u001b[A\n",
      "25061it [00:01, 16547.26it/s]\u001b[A\n",
      "26717it [00:01, 16543.47it/s]\u001b[A\n",
      "28372it [00:01, 16379.30it/s]\u001b[A\n",
      "30011it [00:01, 16137.34it/s]\u001b[A\n",
      "31626it [00:01, 15841.23it/s]\u001b[A\n",
      "34778it [00:02, 16188.17it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from common import load_jsonl\n",
    "test_list = load_jsonl('datasets/test_v2.jsonl')\n",
    "test_dataset = NLIDataset(test_list)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f36c4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: 5 classes\n",
    "model_path = 'model/notice_v3_epoch_5'\n",
    "# model_path = 'models/Mail_Classifier/epoch_5'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=5)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")  \n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1af166d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c95b9114d1471880113a5d218aedad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.9951981137500718\n",
      "f1:  0.9814823947571488\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "references = []\n",
    "num_steps = len(test_dataloader)\n",
    "progress_bar = tqdm(range(num_steps))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_index, batch_dict in enumerate(test_dataloader):\n",
    "        input_items = {key: val.to(device) for key, val in batch_dict.items()}\n",
    "        outputs = model(**input_items)\n",
    "\n",
    "        predictions += outputs.logits.argmax(dim=-1).tolist()\n",
    "        references += batch_dict['labels'].tolist()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "accuracy = accuracy_score(references, predictions)\n",
    "f1 = f1_score(references, predictions,average='macro')\n",
    "print('acc: ', accuracy)\n",
    "print('f1: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57ec3d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model/notice_v3_epoch_5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      3670\n",
      "           1       1.00      1.00      1.00     21985\n",
      "           2       1.00      1.00      1.00      6776\n",
      "           3       1.00      1.00      1.00      2225\n",
      "           4       0.93      0.94      0.93       122\n",
      "\n",
      "    accuracy                           1.00     34778\n",
      "   macro avg       0.98      0.98      0.98     34778\n",
      "weighted avg       1.00      1.00      1.00     34778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model_path)\n",
    "print(classification_report(references, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2ce634",
   "metadata": {},
   "source": [
    "## find out class 3 (note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c6190dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34778\n",
      "in testdatasets , there are 2225 NOT messages ~\n",
      "the accurate of NOT is : 1.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import operator\n",
    "class_number = {'SPAM':0, 'EDM':1, 'HAM':2, 'NOT':3, 'HACKER':4}\n",
    "\n",
    "count = 0\n",
    "wrong = 0\n",
    "right = 0\n",
    "which_class = 3\n",
    "\n",
    "print(len(references))\n",
    "\n",
    "for i, class_num in enumerate(references):\n",
    "    if class_num == which_class:\n",
    "        count += 1\n",
    "        if predictions[i] != which_class:\n",
    "            wrong += 1\n",
    "            # print(test_list[i]['context'])\n",
    "        else:\n",
    "            right += 1\n",
    "    \n",
    "print('in testdatasets , there are %d %s messages ~'%(count, [k for k, v in class_number.items() if v == which_class][0]))\n",
    "print('the accurate of %s is : %.2f'%([k for k, v in class_number.items() if v == which_class][0], (right - wrong)/count))\n",
    "       \n",
    "\n",
    "        \n",
    "# print('in testdatasets , there are %d notice messages ~')\n",
    "\n",
    "# print(test_list[i]['context'])\n",
    "# print(predictions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfc4f72",
   "metadata": {},
   "source": [
    "## find out class 4 (hacker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb50acf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34778\n",
      "in testdatasets , there are 122 HACKER messages ~\n",
      "the accurate of HACKER is : 0.89\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import operator\n",
    "class_number = {'SPAM':0, 'EDM':1, 'HAM':2, 'NOT':3, 'HACKER':4}\n",
    "\n",
    "count = 0\n",
    "wrong = 0\n",
    "right = 0\n",
    "which_class = 4\n",
    "\n",
    "print(len(references))\n",
    "\n",
    "for i, class_num in enumerate(references):\n",
    "    if class_num == which_class:\n",
    "        count += 1\n",
    "        if predictions[i] != which_class:\n",
    "            wrong += 1\n",
    "            # print(test_list[i]['context'])\n",
    "        else:\n",
    "            right += 1\n",
    "    \n",
    "print('in testdatasets , there are %d %s messages ~'%(count, [k for k, v in class_number.items() if v == which_class][0]))\n",
    "print('the accurate of %s is : %.2f'%([k for k, v in class_number.items() if v == which_class][0], (right - wrong)/count))\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8de76",
   "metadata": {},
   "source": [
    "## predict from batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f1f6e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soc507/anaconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 1, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import operator\n",
    "class_number = {'SPAM':0, 'EDM':1, 'HAM':2, 'NOTE':3, 'HACKER':4}\n",
    "model.eval()\n",
    "\n",
    "softmax=torch.nn.Softmax()\n",
    "csvfile = pd.read_csv('test_data/notice.csv')    # change the value up to the colab limit\n",
    "\n",
    "predict_list = []\n",
    "score_list = []\n",
    "sm_score_list = []\n",
    "for token in csvfile['context']:\n",
    "\n",
    "    tokenized_input = tokenizer(token,\n",
    "                                max_length=20,\n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        input_items = {key: val.to(device) for key, val in tokenized_input.items()}\n",
    "#         del input_items['token_type_ids'] ## bart不需要這個\n",
    "\n",
    "        outputs = model(**input_items)\n",
    "        prediction = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        prediction = int(prediction)\n",
    "        sm = softmax(outputs.logits[0])\n",
    "\n",
    "        predict_list.append(prediction)\n",
    "        score_list.append(outputs.logits[0][prediction].item())\n",
    "        sm_score_list.append(sm[prediction].item())\n",
    "        \n",
    "print(predict_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff89fe0",
   "metadata": {},
   "source": [
    "## predict from each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a2b21f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "內文:  EyeCloud  親愛的  林  淵博  設備  狀態  切換  資訊  設備  名稱  華府  總部  伺服器  型號  UR    隸屬  羣組  設備  狀態  切換  信件  EyeCloud  自動  發送  回覆  本信件  登入  EyeCloud  查看  管理員  謝謝  UrlText\n",
      "label:  3 NOTE\n",
      "predict:  3 NOTE\n"
     ]
    }
   ],
   "source": [
    "test_num = 10\n",
    "\n",
    "# subject = test_list[test_num]['subject']\n",
    "context = test_list[test_num]['context']\n",
    "label = test_list[test_num]['label']\n",
    "\n",
    "tokenized_input = tokenizer(context,\n",
    "                            max_length=512,\n",
    "                            truncation=True,\n",
    "                            return_tensors=\"pt\")\n",
    "class_number = {'SPAM':0, 'EDM':1, 'HAM':2, 'NOTE':3, 'KACKER':4}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input_items = {key: val.to(device) for key, val in tokenized_input.items()}\n",
    "    del input_items['token_type_ids'] ## bart不需要這個\n",
    "    \n",
    "    outputs = model(**input_items)\n",
    "    prediction = outputs.logits.argmax(dim=-1)\n",
    "    print(type(int(prediction)))\n",
    "    \n",
    "    \n",
    "    # print('主旨: ', subject)\n",
    "    print('內文: ', context)\n",
    "    print('label: ', class_number[label], label)\n",
    "    print('predict: ', int(prediction), list(class_number.keys())[list(class_number.values()).index(int(prediction))])\n",
    "    \n",
    "    # if int(prediction) == 0:\n",
    "    #     print('predict: ham')\n",
    "    # else:\n",
    "    #     print('predict: spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93f2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
