{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78747201-4bd9-4eb9-9edd-665432511450",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 資料前處理(處理過不用再跑)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12873c4c-75eb-4560-99c3-ade6f4ff9a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=gbk\n",
    "import os\n",
    "import codecs\n",
    "import re\n",
    "import base64\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f3af5d5-1fe0-4044-9da2-e1ea1836b581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 去掉非中文字符\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^\\u4e00-\\u9fff]\", \" \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)  # 能直接变成一个字符串 可以在txt中写成一行\n",
    "    return string.strip()\n",
    " \n",
    "def Index_File():\n",
    "    \"\"\"index文件 路径--标签 对照表\"\"\"\n",
    "    index_file = 'trec06c/full/index'\n",
    "    f = codecs.open(index_file, 'r', 'gbk', errors='ignore')\n",
    "    table = defaultdict(list)\n",
    "    for line in f:\n",
    "        label, path = line.strip().split()\n",
    "        if label == 'spam': # 是垃圾邮件\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        table['label'].append(label)\n",
    "        table['path'].append(path)\n",
    "    table = pd.DataFrame(data=table)\n",
    "    return table\n",
    " \n",
    "#提取四个特征\n",
    " \n",
    "def From_email(email):\n",
    "    # 发件人\n",
    "    # 先提取From后的所有内容\n",
    "    try:\n",
    "        From_raw = re.search(r'From: (.*)', email).group(1)\n",
    "    except:\n",
    "        From_raw = ''\n",
    "    From = ''\n",
    "    # 先看看有没有加密部分 有加密部分就给他解密\n",
    "    name = re.search(r'=\\?GB2312\\?B\\?(.*)\\?=', From_raw, re.I)  # name保存加密部分\n",
    "    if name is None:  # 没有加密部分\n",
    "        name = ''\n",
    "        # 没有加密部分 就保留串的所有内容\n",
    "        From = From_raw\n",
    "    else:  # 有加密部分\n",
    "        name = name.group(1)\n",
    "        try:\n",
    "            name = base64.b64decode(name).decode('gb2312')\n",
    "        except:\n",
    "            try:\n",
    "                name = base64.b64decode(name).decode('gbk')\n",
    "            except:\n",
    "                name = ''\n",
    "        From = name + re.search(r'\\?=(.*)', From_raw).group(1)\n",
    "    # print('From: ', From)\n",
    "    return From\n",
    " \n",
    "def To_email(email):\n",
    "    # 收件人\n",
    "    To = re.search(r'^To: (.*)', email, re.M | re.I).group(1)  # re.M 从每行文本开头的位置开始匹配\n",
    "    # print('To: ', To)\n",
    "    return To\n",
    " \n",
    "def Subject_email(email):\n",
    "    # 主题\n",
    "    Subject = re.search(r'=\\?gb2312\\?B\\?(.*)\\?=', email)\n",
    "    if Subject is None:\n",
    "        Subject = ''\n",
    "    else:  # subject 有内容\n",
    "        Subject = Subject.group(1)\n",
    "        Subject = base64.b64decode(Subject)  # 解密\n",
    "        try:\n",
    "            Subject = Subject.decode('gb2312')  # 解码\n",
    "        except:\n",
    "            try:\n",
    "                Subject = Subject.decode('gbk')  # 解码\n",
    "            except:\n",
    "                Subject = ''\n",
    "    # print('Subject: ', Subject)\n",
    "    return Subject\n",
    " \n",
    "def zhengwen_email(email):\n",
    "    # 正文\n",
    "    zhengwen = re.search(r'\\n\\n(.*)', email, re.S).group(1)\n",
    "    zhengwen = clean_str(zhengwen) # 剔除了非中文字符\n",
    "    # print('正文: \\n', zhengwen)\n",
    "    return zhengwen\n",
    " \n",
    "def data_process():\n",
    "    # 获取 路径--标签 对照表\n",
    "    table = Index_File()\n",
    "    spam = 0\n",
    "    ham = 0\n",
    "    path = 'trec06c/data'\n",
    "    mail_list = []\n",
    "    \n",
    "    dirs = os.listdir(path)# ['000','001',...]\n",
    "    progress_bar = tqdm(range(len(table)))\n",
    "    \n",
    "    for dir in dirs: # 文件夹\n",
    "        dir_path = path + '/' + dir\n",
    "        files = os.listdir(dir_path) # ['000','001',...]\n",
    "        for file in files: # 数据文件\n",
    "            file_path = dir_path + '/' + file\n",
    "            f = codecs.open(file_path, 'r', 'gbk', errors='ignore')\n",
    "            email = '' # 存储一封邮件的所有内容\n",
    "            for line in f: # 每一行\n",
    "                email += line\n",
    "            index = '../data/' + dir + '/' + file\n",
    "            # print(index)\n",
    "            # 发件人\n",
    "            From = From_email(email)\n",
    "            # 收件人\n",
    "            To = To_email(email)\n",
    "            # 主题\n",
    "            Subject = Subject_email(email)\n",
    "            # 正文\n",
    "            zhengwen = zhengwen_email(email)\n",
    " \n",
    "            # print('*'*100)\n",
    "            f.close()            \n",
    " \n",
    "            flag = table[table['path'] == index]['label'].values[0]\n",
    "            if flag == 1:\n",
    "                mail = {'from': From, 'to': To, 'subject': Subject, 'context': zhengwen, 'label': 'spam'}\n",
    "                spam += 1\n",
    "            elif flag == 0:\n",
    "                mail = {'from': From, 'to': To, 'subject': Subject, 'context': zhengwen, 'label': 'ham'}\n",
    "                ham += 1\n",
    "            \n",
    "            mail_list.append(mail)\n",
    "            progress_bar.update(1)\n",
    "            \n",
    "    return mail_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95a55242-dc19-4eca-a4ac-b32739187e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da0de8c6069a4f18a8b93040f30decd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64620 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mail_list = data_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae38f9e0-f836-4b53-a5d5-e782e9c82913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'from': '李倩娜 <mo@163.com>',\n",
       " 'to': 'chi@ccert.edu.cn',\n",
       " 'subject': '打造自己的家园',\n",
       " 'context': '',\n",
       " 'label': 'spam'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mail_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "976c723c-ff06-4e51-9d82-d3f7068b4414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to Jsonl: trec06c/full-data.jsonl\n"
     ]
    }
   ],
   "source": [
    "from common import save_jsonl\n",
    "save_jsonl(mail_list, 'trec06c/full-data.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cbbf4d0-2c4a-42c3-a02b-e337ccb32947",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mail_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27949/1212811557.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_jsonl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmail_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m## 切成訓練資料跟測試資料\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mail_list' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from common import save_jsonl\n",
    "\n",
    "random.shuffle(mail_list)\n",
    "\n",
    "## 切成訓練資料跟測試資料\n",
    "cut_lower = int(len(mail_list) * 0.75)\n",
    "cut_upper = int(len(mail_list) * 0.90)\n",
    "save_jsonl(mail_list[:cut_lower], 'trec06c/train-dataset.jsonl')\n",
    "save_jsonl(mail_list[cut_lower:cut_upper], 'trec06c/val-dataset.jsonl')\n",
    "save_jsonl(mail_list[cut_upper:], 'trec06c/test-dataset.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef079762-b6bc-4770-bc8d-5bbb61042b77",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2712541e-ed17-4fae-a498-a962c80808fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860e1151-9f26-4fb0-9c0a-956f89ce8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_metric\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from common import load_jsonl, save_jsonl\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    "    BartTokenizer,\n",
    "    BartForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    "#     AutoModelForMaskedLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c824df94-f793-411f-b1ed-a656998763f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIDataset(Dataset):\n",
    "    # def __init__(self, data_list, max_length=512, model_name='fnlp/bart-large-chinese'):\n",
    "    def __init__(self, data_list, max_length=512, model_name=\"bert-base-multilingual-cased\"):\n",
    "        self.d_list = data_list\n",
    "        self.len = len(self.d_list)\n",
    "        self.max_length = max_length\n",
    "#         self.tokenizer = BertTokenizer.from_pretrained(model_name)  ##模型作者指定用BertTokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.label2index = {\n",
    "            'ham': 0,\n",
    "            'spam': 1\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.d_list[index]\n",
    "        subject = data['subject']\n",
    "        context = data['context']\n",
    "        label = data['label']\n",
    "        \n",
    "        processed_sample = dict()\n",
    "        processed_sample['labels'] = torch.tensor(self.label2index[label])\n",
    "        tokenized_input = self.tokenizer(subject, context,\n",
    "                                                  max_length=self.max_length,\n",
    "                                                  padding='max_length', \n",
    "                                                  truncation=True,\n",
    "                                                  return_tensors=\"pt\")\n",
    "        \n",
    "        input_items = {key: val.squeeze() for key, val in tokenized_input.items()}\n",
    "        processed_sample.update(input_items)\n",
    "        return processed_sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b24c7d03-5cd9-4746-afa5-4d2f79506d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Jsonl: trec06c/train-dataset.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48465it [00:02, 16739.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Jsonl: trec06c/val-dataset.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9693it [00:00, 16305.18it/s]\n"
     ]
    }
   ],
   "source": [
    "train_list = load_jsonl('trec06c/train-dataset.jsonl')\n",
    "eval_list = load_jsonl('trec06c/val-dataset.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ab448b-c47a-4535-954e-a15e02356fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a55df5361ca4422a3385e1a524c4678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f28750d38644418ae3f58517ac3657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6d04ab4e1943a98b43ce0396cf2ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b09c55fb1a04b97884283bb4addf19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = NLIDataset(train_list)\n",
    "eval_dataset = NLIDataset(eval_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd9a753c-6545-45b4-b7b4-9eaae6c15f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'from': 'ling@cernet.com',\n",
       " 'to': 'hong@ccert.edu.cn',\n",
       " 'subject': 'Re: 大家觉得建立一个IT职业者联盟如何',\n",
       " 'context': '你看人 都有那种组织的 定期弄些劳资谈判什么的 缓和劳资矛盾 而我们所处的 行业 唉 我感觉是一个比较尴尬的行业 一方面说待遇好 一方面又是有种吃青春饭的感觉 没有保障 当然我所说的是大多数人 那么我们是否有必要建立一个类似工会一样的组织来维护我们这个行业的就业人员的共同利益呢 只是我们这个是一个行业工会 全 行业的员工都可以成为联盟的成员 至于联盟的权利 那就很多了 最基础的一个我觉得就是帮助建立一个合理的劳资合同 比如可以确立最低劳资保障 最低合同年限等 另外还可以有就业经验交流之类的活动 具体还要集思广益',\n",
       " 'label': 'ham'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "260ed660-3f90-46e0-ae61-6b44e7650ae6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor(0),\n",
       " 'input_ids': tensor([  101, 20304,   131,  3197,  3408,  7162,  3775,  3697,  6033,  2072,\n",
       "          2102, 26956,  6478,  2090,  6457,  6479,  5749,  3241,  2253,   102,\n",
       "          2262,  5765,  2179,  7838,  4461,  7802,  5953,  6328,  6331,  5718,\n",
       "          3388,  4470,  3705,  2159,  2608,  7507,  7366,  2549,  2181,  2118,\n",
       "          5718,  6374,  2833,  2608,  7507,  5813,  5762,  6459,  3976,  2206,\n",
       "          3999,  3182,  5718,  7069,  2090,   100,  3976,  3911,  7162,  4380,\n",
       "          2072,  2102,  4839,  7653,  3473,  3470,  5718,  7069,  2090,  2072,\n",
       "          4335,  8335,  7354,  3765,  7754,  3240,  2072,  4335,  8335,  2728,\n",
       "          4380,  4461,  5953,  2767,  8325,  4376,  8489,  5718,  3911,  7162,\n",
       "          4917,  4461,  2312,  8253,  3740,  5322,  3976,  3999,  7354,  5718,\n",
       "          4380,  3197,  3191,  4305,  2179,  7802,  2118,  3976,  2206,  4380,\n",
       "          2789,  4461,  3793,  7139,  3697,  6033,  2072,  2102,  6141,  2240,\n",
       "          3584,  2226,  2072,  4577,  5718,  6328,  6331,  4501,  6361,  4046,\n",
       "          3976,  2206,  7700,  2102,  7069,  2090,  5718,  3472,  2090,  2179,\n",
       "          2816,  5718,  2456,  2773,  2551,  5736,  2820,  2751,  4380,  3976,\n",
       "          2206,  7700,  2102,  4380,  2072,  2102,  7069,  2090,  3584,  2226,\n",
       "          2448,  7069,  2090,  5718,  2816,  3584,  7838,  2756,  2204,  3975,\n",
       "          2110,  6479,  5749,  5718,  3975,  2816,  6623,  2151,  6479,  5749,\n",
       "          5718,  4486,  2551,  7802,  3472,  3767,  3191,  2146,  4458,  3099,\n",
       "          5845,  5718,  2072,  2102,  3976,  7162,  3775,  3472,  4380,  3620,\n",
       "          2602,  3697,  6033,  2072,  2102,  2769,  5515,  5718,  2608,  7507,\n",
       "          2769,  2773,  4839,  3241,  2756,  2204,  5854,  6033,  4458,  2247,\n",
       "          2608,  7507,  2312,  8253,  4458,  2247,  2769,  2773,  3642,  8215,\n",
       "          6069,  2749,  3189,  7699,  2756,  2204,  4461,  3472,  2090,  6336,\n",
       "          8573,  2165,  4982,  2120,  6141,  5718,  4978,  2601,  2461,  2251,\n",
       "          7699,  7139,  8272,  3820,  3651,  5736,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad69d768-5bf4-41e4-9025-0d231d2c16a9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416a6dd9429c465f98146253b022ff9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = BartForSequenceClassification.from_pretrained(\"fnlp/bart-large-chinese\", num_labels=2) ## 2分類\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if torch.cuda.device_count() >1:\n",
    "    model = nn.DataParallel(model,device_ids=[0])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acfb8614-4c87-49aa-b377-14c024b0f8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9693\n",
      "1939\n"
     ]
    }
   ],
   "source": [
    "train_batch_size=5\n",
    "learning_rate=2e-5 \n",
    "train_epochs=5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "eval_dataloader = DataLoader(eval_dataset, shuffle=True, batch_size=train_batch_size)\n",
    "print(len(train_dataloader))\n",
    "print(len(eval_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d5fb97-7401-412c-bc22-1e57e1f0846c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([1, 1, 1, 1, 1]), 'input_ids': tensor([[ 101, 4037, 7507,  ...,    0,    0,    0],\n",
      "        [ 101, 7535, 6309,  ..., 2196, 2206,  102],\n",
      "        [ 101, 5410, 2211,  ...,    0,    0,    0],\n",
      "        [ 101, 2090, 2599,  ...,    0,    0,    0],\n",
      "        [ 101,  102, 3620,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for batch_index, batch_dict in enumerate(train_dataloader):\n",
    "    print(batch_dict)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe74576b-9e2b-41ac-b359-668dbfead630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3755aa5ad7c442fb82b7d0957b982265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48465 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0   loss:  tensor(0.6812, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  0   loss:  tensor(0.0621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.997730320850098\n",
      "f1:  0.9974414430411945\n",
      "epoch:  1   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0076, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  1   loss:  tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.9987619931909626\n",
      "f1:  0.9986052944421566\n",
      "epoch:  2   loss:  tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  2   loss:  tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.9989683276591355\n",
      "f1:  0.9988384684857675\n",
      "epoch:  3   loss:  tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(7.1880e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(9.6315e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(5.8315e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(5.9078e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(4.6991e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(3.4856e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(7.2548e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(5.4954e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  3   loss:  tensor(3.5571e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "acc:  0.9994841638295677\n",
      "f1:  0.9994190988155645\n",
      "epoch:  4   loss:  tensor(3.7193e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(2.5582e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(3.2281e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(1.9073e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(2.0551e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(2.0766e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(1.5926e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(1.5163e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(1.6999e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(1.3733e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(1.4424e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(1.4901e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(1.2755e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(1.3304e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(1.2922e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(1.3065e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(1.4806e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(1.3542e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "epoch:  4   loss:  tensor(1.3757e-05, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.9993809965954813\n",
      "f1:  0.9993027558396207\n"
     ]
    }
   ],
   "source": [
    "## 進度條\n",
    "num_training_steps = train_epochs * len(train_dataloader)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "## 設定warmup\n",
    "lr_scheduler = get_scheduler(\n",
    "  \"linear\",\n",
    "  optimizer=optimizer,\n",
    "  num_warmup_steps=10,\n",
    "  num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "## start training\n",
    "for epoch in range(train_epochs):\n",
    "    model.train()\n",
    "    for batch_index, batch_dict in enumerate(train_dataloader):\n",
    "        \n",
    "        input_items = {key: val.to(device) for key, val in batch_dict.items()}\n",
    "#         del input_items['token_type_ids'] ## bart不需要這個\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**input_items)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        if torch.cuda.device_count() >1: ##多GPU的情況要對loss求平均\n",
    "            loss = loss.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        if batch_index % 500 ==0:\n",
    "            print('epoch: ', epoch, '  loss: ', loss)\n",
    "            \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    with torch.no_grad():\n",
    "        for batch_index, batch_dict in enumerate(eval_dataloader):\n",
    "            input_items = {key: val.to(device) for key, val in batch_dict.items()}\n",
    "            outputs = model(**input_items)\n",
    "\n",
    "            predictions += outputs.logits.argmax(dim=-1).tolist()\n",
    "            references += batch_dict['labels'].tolist()\n",
    "\n",
    "    accuracy = accuracy_score(references, predictions)\n",
    "    f1 = f1_score(references, predictions,average='macro')\n",
    "    print('acc: ', accuracy)\n",
    "    print('f1: ', f1)\n",
    "    \n",
    "    ## save model\n",
    "    save_path = 'model/epoch_' + str(epoch+1)\n",
    "    if torch.cuda.device_count() >1:\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(save_path)\n",
    "    else:\n",
    "        model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf9f36",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eacacfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from common import load_jsonl, save_jsonl\n",
    "\n",
    "from transformers import (\n",
    "    BartTokenizer,\n",
    "    BartForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "741e0b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Jsonl: test-data.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "2683it [00:00, 26815.77it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "5371it [00:00, 26846.33it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "8056it [00:00, 26769.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "12924it [00:00, 26328.51it/s]\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "test_list = load_jsonl('test-data.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d487bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('model/epoch_5', num_labels=2) ## 2分類\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")  ##模型作者指定用BertTokenizer\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if torch.cuda.device_count() >1:\n",
    "    model = nn.DataParallel(model,device_ids=[0,1])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a91ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_num = 10\n",
    "\n",
    "subject = test_list[test_num]['subject']\n",
    "context = test_list[test_num]['context']\n",
    "label = test_list[test_num]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2b21f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主旨:  商业联络！\n",
      "內文:  尊敬的负责人 经理 财务 您好 我是深圳伟和商贸发展有限公司 在我公司持续稳定的发展中实业雄厚 有着 一定的社会关系 在我公司每月进项多销项少 每月有余额发票 对外联系些业务 贵公司在平时作帐及销售方面是需要用些票据可在我公司代开 如商品销售 地税 建筑安装 其它服务 运输 广告发票等 收费 左右 还可代办海关缴款书 在我公司成立多年一直坚持以信用 所开绝对真票 更希望贵公司共同合作快乐 本公司的承诺 以最优惠的价格和最及时的服务满足你的要求 提供到位 如 贵公司有些担心 可在电脑网上查证或确认后再付款 以真诚的服务合作一次 必成永久的朋友 欢迎来电联系 联 系 人 杨 生 电 话\n",
      "label:  spam\n",
      "predict: spam\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(subject, context,\n",
    "                            max_length=512,\n",
    "                            truncation=True,\n",
    "                            return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input_items = {key: val.to(device) for key, val in tokenized_input.items()}\n",
    "    del input_items['token_type_ids'] ## bart不需要這個\n",
    "    \n",
    "    outputs = model(**input_items)\n",
    "    prediction = outputs.logits.argmax(dim=-1)\n",
    "    \n",
    "    \n",
    "    print('主旨: ', subject)\n",
    "    print('內文: ', context)\n",
    "    print('label: ', label)\n",
    "    if int(prediction) == 0:\n",
    "        print('predict: ham')\n",
    "    else:\n",
    "        print('predict: spam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff89fe0",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8925747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b349f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c848836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b3ff1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71277b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93567c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_metric\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from common import load_jsonl, save_jsonl\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    "    BartTokenizer,\n",
    "    BartForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    "#     AutoModelForMaskedLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "875e5c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a94943a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Jsonl: test-data.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1645it [00:00, 16417.49it/s]\u001b[A\n",
      "3417it [00:00, 17177.40it/s]\u001b[A\n",
      "5268it [00:00, 17785.21it/s]\u001b[A\n",
      "7124it [00:00, 18073.65it/s]\u001b[A\n",
      "8932it [00:00, 17672.30it/s]\u001b[A\n",
      "10738it [00:00, 17801.64it/s]\u001b[A\n",
      "12924it [00:00, 17537.28it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "eval_list = load_jsonl('test-data.jsonl')\n",
    "eval_dataset = NLIDataset(eval_list)\n",
    "eval_dataset[0]\n",
    "test_dataloader = DataLoader(eval_dataset, shuffle=True, batch_size=train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7679eb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.9996904982977406\n",
      "f1:  0.9996526132173413\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for batch_index, batch_dict in enumerate(test_dataloader):\n",
    "    input_items = {key: val.to(device) for key, val in batch_dict.items()}\n",
    "    outputs = model(**input_items)\n",
    "\n",
    "    predictions += outputs.logits.argmax(dim=-1).tolist()\n",
    "    references += batch_dict['labels'].tolist()\n",
    "\n",
    "accuracy = accuracy_score(references, predictions)\n",
    "f1 = f1_score(references, predictions,average='macro')\n",
    "print('acc: ', accuracy)\n",
    "print('f1: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4c592cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sentence is \n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6849/2097110609.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"the sentence is \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_items\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minput_items\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 3"
     ]
    }
   ],
   "source": [
    "num = 3\n",
    "print(\"the sentence is \")\n",
    "print()\n",
    "print(input_items[3])\n",
    "outputs = model(**input_items[3])\n",
    "predictions = outputs.logits.argmax(dim=-1).tolist()\n",
    "print(\"predict : \", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b564fa76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b85bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f2fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82237e53-f7fe-44b8-9fee-b44ef62a604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "label_list = [\"0\", \"1\"]\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"Transformers Trainer\")\n",
    "config = BertConfig.from_pretrained(\"Transformers Trainer\", finetuning_task=\"cola\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639889e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The probable hostile German reaction is unfortunate.\"  # @param {type:\"string\"}\n",
    "tokenized_input = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "outputs = model(**tokenized_input)\n",
    "print(f\"Prediction: {label_list[outputs.logits.argmax(dim=-1).item()]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
