{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef079762-b6bc-4770-bc8d-5bbb61042b77",
   "metadata": {},
   "source": [
    "### Expand application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2712541e-ed17-4fae-a498-a962c80808fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "860e1151-9f26-4fb0-9c0a-956f89ce8e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_metric\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from commom import load_jsonl, save_jsonl\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    "    BertTokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c824df94-f793-411f-b1ed-a656998763f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, data_list, max_length=512, model_name=\"bert-base-multilingual-cased\"):\n",
    "        self.d_list = data_list\n",
    "        self.len = len(self.d_list)\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.label2index = {\n",
    "            'SPAM': 0,\n",
    "            'EDM': 1,\n",
    "            'HAM': 2,\n",
    "            'NOTE':3,\n",
    "            \"HACK\":4\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.d_list[index]\n",
    "        context = data['context']\n",
    "        label = data['label']\n",
    "        \n",
    "        processed_sample = dict()\n",
    "        processed_sample['labels'] = torch.tensor(self.label2index[label])\n",
    "        tokenized_input = self.tokenizer(context,\n",
    "                                         max_length=self.max_length,\n",
    "                                         padding='max_length', \n",
    "                                         truncation=True,\n",
    "                                         return_tensors=\"pt\")\n",
    "        \n",
    "        input_items = {key: val.squeeze() for key, val in tokenized_input.items()}\n",
    "        processed_sample.update(input_items)\n",
    "        return processed_sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf9f36",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d487bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('models/Mail_Classifier_10/epoch_5', num_labels=3) \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")  \n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "if torch.cuda.device_count() >1:\n",
    "    model = nn.DataParallel(model,device_ids=[0,1])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbefdee1",
   "metadata": {},
   "source": [
    "## example token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4daa0378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token predict\n",
    "\n",
    "edm_1=['新 登入 資訊 確認 FedEx 帳號  XXXXX4045 Alice 感謝 支持 FedEx 登入 整合 計畫 UrlText 登入 資訊 成功 登入 FedEx 酬賓 廣場  平臺 這讓 得以 UrlText 用戶 識別碼  FedEx 酬賓 廣場 安排 管理 到府 取件 管理 線上 帳單 視 查詢 特定 運送 時間 登入 FedEx 酬賓 廣場 查看 紅利 積分 回饋 獎品 更多 功能 如需 進一步 UrlText 帳號 登入 FedEx 酬賓 廣場 網站 請參閱  保護 帳號 顯示  碼 FedEx 帳號  碼  UrlText 登入 帳號 註冊 FedEx 酬賓 廣場 FedEx 帳號 受 FedEx 酬賓 廣場 活動 條款 細則 關注 FEDEX 活動 訊息 通知 發送至 emailAddress FedEx 不定期 寄發 活動 訊息 至此 信箱 取消 訂閱 更新 資料 如本 電子郵件 涉及 個人資料 無意 接受 類似 訊息 按此 回覆 妥善處理 荷蘭商 聯邦快遞 國際 股份 有限公司 臺灣 分公司  臺北市 中山北路 二段  號  樓 emojiPic  FedEx 本信件 受 美國 國際 著作權 商標法 保護 參考 隱私權 聲明 保留 權利 瀏覽 網路版']\n",
    "edm_2=['xem b ng tr nh duy dear qu kh ch h ng sau th gian ch n novaworld l p n novaland ph tri n ch nh th c c tri n khai v r nhi u ch ng tr nh u h p n nh cho qu kh ch h ng v nh u c h sinh c ng nh u n hi u qu nh  qu kh ch h ng quan n vui l ng li n h bpkd qua n tho     ho c l th']\n",
    "edm_3=['export target africa market max lewis 09th power energy africa  october   carnivore expo ground nairobi kenya gateway east africa power energy market ding zing chemical product co ltd dear sir east africa invite come explore vast  million consumer market mega']\n",
    "edm_4=['米思米 官網 首頁 WOS 網上 報價 訂購 賬戶 目錄 申請 忘記 密碼 煩惱 懂 想要 解決 乾貨 滿滿的 知識點 咯  批量 導入 商品 清單 採購 輕鬆 便捷 告別 手動 輸入 高效 採購 時代 終於 訂購 大批量 商品 一份 採購 BOM 導入 上傳 即可  商品 庫存']\n",
    "edm_5=['special enrollment period end soon advertisement insurance carrier agent broker affiliate plan endorsed government entity agency connect consumer insurance carrier affiliate  main street suite  el segundo ca  emojipic health plan america  unsubscribeview webpage']\n",
    "edm_6=['dear user update database e mail account also upgrade user effect delete unused e mail account create space new account ensure experience service disruption period need click customer support state kindly customer support validate upgrade mailbox quota also']\n",
    "edm_7=['limited time offer  prefer receive future email unsubscribe  e 21st street los angeles ca ']\n",
    "edm_8=['image click jfd training date come  february  february date imca diver medic technician refresher fully book date  feb   feb  find urltext classroom course combine dive rov system audit assurance awareness date  feb   feb  find urltext online course march date imca']\n",
    "edm_9=['alleviate neck pain home neckmassager must easy solution chronic neck relaxation problem even though technology complex advance simple use save lot pain neckmassager design make feel good possible sit new smoke people modern work force day suffer unnecessary headache due extremely stiff neck neckmassagerhas medical professional go crazy effectiveness problem continue read key feature neckmassager neck pain headache simply disappear  different massage method say goodbye stress worry balance nervous system improve oxygen supply tissue cell nutrition relieve joint pain muscle tension increase blood circulation click neck massager  free shipping like update problem pleaseunsubscribe  clinton street little rock ar  79d 364db television show u take lot emotional issue first two season least death husband father jack season popular drama series ing another controversial important topic obesity fertility u follow life five family include kate woman obese face weight relate stigma throughout initial episode season kate husband visit fertility specialist discus option undergo vitro fertilization ivf struggle overcome emotionally devastate miscarriage season two couple hope ivf help kate pregnant fertility specialist initially refuse take kate patient e embryo say']\n",
    "edm_10=['親愛的 富邦 信用卡 客戶 感謝 本行 信用卡 持有 信用卡 到期 末  碼  行將 次 掛號 郵寄 方式 寄送 新卡 留存 本行 通訊 地址 臺北市 中山區 長 變更 地址 請於    前 本行 客服 專線 本行 保留 最終 審核 續卡 各項 條件 權利 致電 本行 客服 專線    將有 專員 竭誠 服務 臺北 富邦 銀行 敬啟']\n",
    "\n",
    "\n",
    "SPAM_1=['chang natapong adul thaebtong thai oil clean fuel project']         \n",
    "SPAM_2=['Dear Mr Ms 簡瑜瑩 小姐 If message displayed properly please click']\n",
    "SPAM_3=['下載 達美樂 APP 登入 會員 訂餐 訂單 狀態 達美樂 社? 不定期 好康 送給 Line FB Instagram 封信 系統 發出 信件 回覆 產品 實物 優惠 合併 達美樂 保有 調整 活動 辦法 優惠 內容 權利 取消 訂閱']\n",
    "SPAM_4=['湘喻 文樂 數學  班 中 張貼 預先 排程 作業 新 作業 截止 日期     課堂 進度 繳交 請於 隔天 中午   前 上傳 方喬飛 p1  潘潔琳 p37  蘇子 琦 p1  孟巧 璇 p99  開啟 張貼者 陳 採靈 張貼 日期   Google LLC  Amphitheatre Parkway Mountain View CA  USA']\n",
    "SPAM_5=['want change receive email update preference unsubscribe list']\n",
    "SPAM_6=['read online newsletter february  free subscription qf plus option face attendance new work location feature timetec ta timetec ban jailbroken root phone start timetec ta app timetec renew iso   certification separate contractor normal visitor timetec vms change']\n",
    "SPAM_7=['航 eMall 哩程 折抵 購物金 超值 公開 父親節 禮物 eMall 買 超優 華航 eMall 哩程 折抵 購物金   折抵 NT  元 每樣 商品 折抵 哩程 睡著 詳情 參考 哩程 折抵 活動 辦法 華夏 會員 享有 更多 優惠 新增 全 哩程 兌換 優惠 館 原價  折起 幫 省 荷包']\n",
    "SPAM_8=['votre bien tre notre priorit si ce message ne affiche pa correctement cliquez ici pour l ouvrir dans votre navigateur si vous ne souhaitez plus recevoir email de la part des laboratoires olliscience cliquez ici anti ge fin de l offre de lancement ce soir ch lectrice j des']\n",
    "SPAM_9=['湘喻 潘俞 岑在 新 來義 數學  班 中 張貼 新 公告 新 公告 上課 上線 時間 李喬     高郡成     馬育 澤     開啟 張貼者 潘俞 岑 張貼 日期   Google LLC  Amphitheatre Parkway Mountain View CA  USA 不想 收到 Classroom 傳送 電子郵件 取消 訂閱']\n",
    "SPAM_10=['直播 預告 利用 NanEye 快速 開發 一次性 醫療 內窺鏡 顯示 點擊 在線 查看 直播 預告 利用 NanEye 快速 開發 一次性 醫療 內窺鏡 一次性 內窺鏡 醫療 內窺鏡 市場 成? 主流 趨勢 降低 交叉感染 風險 尺寸 內窺鏡 泌尿 鏡 支氣管鏡 人體 進行 無創 手術 診斷 治療 一種 最佳 方式 一次性 內窺鏡 市場 需求 ams OSRAM 提供 NanEye 系列 微 攝像頭 模組 客戶 快速 開發出 市場 競爭力 一次性 內窺鏡 產品 本次 研討會 將會 解  醫療 內窺鏡 市場趨勢  NanEye 系列 微型 攝像頭 產品 優勢 ']\n",
    "\n",
    "HAM_1=['發送 郵件 退回 相關 信息 發件人 emailAddress 主題 Payment account 原文 郵件 原文 包含 附件 中 時間       CST 發送到 emailAddress 系統 應答  退信 原因 發件 賬號 無權 限給 郵件 列表 發送 郵件 導致 郵件 系統 退回 解決 建議 確認 郵件 列表']\n",
    "HAM_2=['hi sure get early message figure probably slip crack help many company increase sale want show something would like know available thanks carter smith ecommerce specialist emailaddress carter smith emailaddress date wed jun     cest  emailaddress emailaddress']\n",
    "HAM_3=['Hi 現場 還在 佈 光纖 網線 接 Wifi AP 落伍 透過 網狀 網路 Mesh 結構 無線 AP 連線 建立 備援 架構 輕鬆 建立 單一 無線 系統 改善 角落 訊號 業界 首發 WA512GM Mesh Wifi AP 提供 IP67 導軌 式 設計 戶外 Wifi 傳輸 困擾 Womater 提供 長 距離 Wifi 方案 Tim Chen 陳德 智 Sales developing Div 展業 部 UrlText Phone   ext  MP  emailAddress']\n",
    "HAM_4=['urltext']\n",
    "HAM_5=['awesome thank much alex wonderful day well aaron dyke chief executive officer p   e emailaddress urltext office  washington mall  boston  something stand success move dwayne rock johnson tue feb     emailaddress write hello aaron hope great weekend track late']\n",
    "HAM_6=['message summit web portal      receive send back sale order po  part c67711t7a003101 r please recheck click urltext']\n",
    "HAM_7=['Dear friend 參考 黃埔 價格 用到   號 建華 USD700  陽明 USD750  中聯 基隆 USD950  中聯 臺中 高雄 USD950  臺塑 RMB6300 RMB14500 南沙 建華 USD850   價格 用到   號 中聯 基隆 USD950  價格 用到   號 中聯 臺中 高雄 USD950  價格 用到   號']\n",
    "HAM_8=['正確 顯示 點選 查看 知識 學院 網站 企業 包班 企業 二代 高?會 物料 控制 倉儲 管理 操作實務 課前 導讀 影片 連結 何種 製造業 物料 倉儲 管理 工作 迄今 困擾 管理層 問題所在 倉儲 管理 做 不好 良品 廢料 損失 備料 費時 發錯 料 影響 生產 工作 料帳 不準 更是 企業 心頭 之痛 不必要 呆料 停工待料 導致 交期 延誤 庫存 管理模式 攸關 投產 供料 成敗 ERP 系統  生 部門  物管 部門  採購 部門  製造 部門 廠長 製造 現場 相關 主管  IT 部門 相關 人員 中高階 主管 深入探討 物料 管理 本質 肇因 簡捷 系統化 制度化 方法 予以 防治 消除 課程 大綱 課程內容 教學方式 物料 管理 基本概念  物料 管理 意義  物料 管理 演進  物料 管理 目標  物料 管理 常見 解決 對策  物料 管理 二大 主軸  物管 人員 應有 職責 特質  物料 管理 績效 衡量 指標 PPT 講授 案例 分享 小組討論 選用 合適 存量 管制 模式  傳統 物料 計畫 迷思  庫存 存量 管制 方式  庫存 存量 管制 方式  庫存 存量 管制 方式 實務 演練 庫存 物料 需求 計算  Time Phasing 存量 管制 方式  ABC 分析 設定 存量 管制 模式  存量 管制 模式  訂單 批量 法 By Order  複 倉法  定期 訂購 法  訂購 點法 ROP 實務 演練 訂購 點 計算 方式  存量 法  物料 需求 計畫 MRP 實務 演練 貴公司 原物料 存量 管制 方式 物料 需求 計畫 MRP  製造業 ERP 作業 流程 簡介  MRP 三大 要素 主 生產 排程 MPS 用料 清單 BOM 庫存 管理 Inventory  物料 編號 原則  BOM 建立 ECN 管理  ERP 系統 導入 步驟 成功 要訣 案例 ERP 製造業 營運 管理 效益 倉儲 管理 實施 要點  倉儲 管理 目標  倉儲 管理 改善 作法  儲位 規劃 管理  做好 倉庫 5S 管理  收 發料 作業 管理 實例 收發 料 作業 流程  退料 作業 管理 實例 退料 作業 流程  先進先出 實施 技巧  料帳 合一 關鍵因素  做好 盤點 作業  盤點 目的  盤點 種類 料帳 實地  盤點 前 工作  盤點 作業 執行 步驟  ERP 進行 盤點 作業  呆 廢料  呆料 比率 高 原因  呆料 防治 解決 對策  廢料 發生 原因  廢料 原則 主辦單位 得視 報名 情況 保留 開課日期 講師 題綱 變更 權利 最新 訊息 上網 UrlText 陳 錦河 講師 講師 簡介 現 企業 競爭力 提升 顧問 專業 講師 北京大學 清華大學 四川大學 特聘 講師 安傳 電子 股份 有限公司 品管 課長 三光 惟達 股份 有限公司 生 專員 工業 技術 研究院 電子 物管 主管 華智 資訊 管理 顧問 有限公司 ERP 專案 經理 總經理 工業 技術 研究院 量測 中心 品保 課長 鼎沛 股份 有限公司 協理 副總經理 蘇州 華傑 電子 有限公司 總經理 康可 電子 股份 有限公司 副總經理 天路 管理 顧問 有限公司 總經理 廈門 福友 企 管理 顧問 有限公司 副總經理 榮 譽 榮獲 臺灣 經濟部 品質 推廣 服務獎 證 品管 技術師 合格  ISO  實驗室 品質 系統 認證 主任 評審員 考試合格 中階 幹部 管理 技能 MTP 合格 講師 專 長 企業 體質 再造 流程 優化 企業 規範化 管理 生產 管理系統 物料 管理系統 品質 管理系統 5S 目視 管理 IE 工作 改善 ERP 系統 設計規劃 目標 管理 績效考覈 人力資源 管理系統 中 基層幹部 管理 能力 提升 訓練 臺灣 區 電機 電子 工業 同業公會 品質 技術諮詢 顧問 企業 化 小組 召集人 臺灣 經濟部 中小企業 榮譽 指導員 職訓局 職業 訓練 研究 發展 中心 企業 訓練 輔導團 顧問 區域 上課 日期 上課 時間 地址 線上 報名 臺北場       鼎新 臺北 總公司 新北市 新店區 中興路 一段  號 臺中場       鼎新 臺中 分公司 臺中市 大里區 中興路 一段  號 課程 費用   元整 包含 稅 講義 中餐 課程 優惠 方案 線上 報名 優惠 方案 一律 享有  折 優惠 早鳥票 開課 一個月 前 報名 享有  折 優惠 團體  享有  折 優惠 Ps 人工 報名 恕 提供 優惠 課程 洽詢    分機  分機  知識 學院 網站 企業 包班 企業 二代 高?會 Facebook YouTube 客服 信箱 隱私權 聲明 取消 訂閱 鼎新 電腦 版權所有 Copyright emojiPic Data Systems Consulting Co Ltd All rights reserved 不想 收到 電子報 點選 入口 取消 訂閱']\n",
    "HAM_9=['airplus engineering co ltd    f   e mail emailaddress website urltext']\n",
    "HAM_10=['dear tram plz provide debit note feb within tmr thanks advance wai yan naing lym purchasing every import shipment need provide co certificate origin shipment show make country cargo']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e64bc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Jsonl: datasets/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32440it [00:01, 17599.72it/s]\n"
     ]
    }
   ],
   "source": [
    "test_list = load_jsonl('datasets/test.jsonl')\n",
    "context = test_list[5]['context']\n",
    "# label = test_list[test_num]['label']\n",
    "# tokenized_input = tokenizer(context,\n",
    "#                             max_length=512,\n",
    "#                             truncation=True,\n",
    "#                             return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10e62fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = test_list[5]['context']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b228ab",
   "metadata": {},
   "source": [
    "## single token inference \n",
    "### Classification for each token and statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0869ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['品味', '達人', '慶', '下單', '金額', '送', '精美', '好禮', '單筆', '消費', '抽', 'UNIQLO', '情侶', '穿搭', '累積', '消費', '抽禾聯', '冷氣', 'UNIQLO', '一週', '穿搭', '購物', '想', '取消', '訂閱', '點選']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soc507/anaconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0, 1, 1, 1]\n",
      "[4.921854019165039, 5.987586498260498, 5.541514873504639, 4.328450679779053, 4.019975185394287, 5.5087995529174805, 6.440144062042236, 4.585875988006592, 2.9259495735168457, 2.3199374675750732, 4.1942596435546875, 0.941717803478241, 5.460469722747803, 2.8076400756835938, 4.915834903717041, 2.3199374675750732, 4.992317199707031, 3.638474702835083, 0.941717803478241, 5.671134948730469, 2.8076400756835938, 1.2411834001541138, 6.3661065101623535, 4.863460540771484, 4.160599708557129, 1.8418810367584229]\n",
      "[0.9988424181938171, 0.999745786190033, 0.9995414018630981, 0.9965644478797913, 0.9921736717224121, 0.999538779258728, 0.9998648166656494, 0.9977588653564453, 0.9422842264175415, 0.906727135181427, 0.9960610270500183, 0.5965257883071899, 0.999306321144104, 0.9659648537635803, 0.9981787204742432, 0.906727135181427, 0.9989342093467712, 0.9924593567848206, 0.5965257883071899, 0.9995481371879578, 0.9659648537635803, 0.7616497278213501, 0.9998514652252197, 0.9976269602775574, 0.9884810447692871, 0.6408308148384094]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32445/183363947.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# choose SPAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtest_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0medm_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0medm_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "softmax=torch.nn.Softmax()\n",
    "# predict_list = []\n",
    "# score_list = []\n",
    "# sm_score_list = []\n",
    "\n",
    "edm_dict = {}\n",
    "predict_list = []\n",
    "score_list = []\n",
    "sm_score_list = []\n",
    "test_token = context   # set which token\n",
    "\n",
    "print(test_token.split())\n",
    "for token_list in test_token.split():\n",
    "    tokenized_input = tokenizer(token_list,\n",
    "                                max_length=20,\n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        input_items = {key: val.to(device) for key, val in tokenized_input.items()}\n",
    "#         del input_items['token_type_ids'] ## bart不需要這個\n",
    "\n",
    "        outputs = model(**input_items)\n",
    "        prediction = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        prediction = int(prediction)\n",
    "        sm = softmax(outputs.logits[0])\n",
    "\n",
    "        predict_list.append(prediction)\n",
    "        score_list.append(outputs.logits[0][prediction].item())\n",
    "        sm_score_list.append(sm[prediction].item())\n",
    "\n",
    "\n",
    "# dic = {\"abc\":0}\n",
    "# for key in predict_list:\n",
    "#     dic[key] = dic.get(key, 0) + 1\n",
    "# print(dic)\n",
    "\n",
    "print(predict_list)\n",
    "print(score_list)\n",
    "print(sm_score_list)\n",
    "for num, i in enumerate(predict_list):\n",
    "    if i == 0:   # choose SPAM\n",
    "        if test_token[0].split()[num] not in edm_dict.keys():\n",
    "            edm_dict[str(test_token[0].split()[num])] = 1\n",
    "        else:\n",
    "            edm_dict[str(test_token[0].split()[num])] += 1\n",
    "print(\"--------\")\n",
    "print(edm_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7297ff0",
   "metadata": {},
   "source": [
    "## EDM statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94778874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soc507/anaconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'免運': 70, '訂閱': 14, '取消': 10, '限時': 6, '線上': 5, '購物': 4, '點選': 4, '洗衣機': 3, '按此': 3, '閱讀': 2, '銷售': 2, '顯示': 2, 'Subscribe': 2, 'Unsubscribe': 2, '誤判為': 2, '消費': 2, '提供': 2, '顯示器': 2, '請點此': 1, '本活動': 1, '去除': 1, '攝影機': 1, '資生堂百優': 1, '外貿協會': 1, '推出': 1, '轉介': 1, 'X10': 1, 'Panasonic': 1, 'XB12': 1, '抽取': 1, '星級飯店': 1, '曬衣架': 1, '刪除': 1, '電腦病毒': 1}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import operator\n",
    "class_number = {'SPAM':0, 'EDM':1, 'HAM':2}\n",
    "model.eval()\n",
    "\n",
    "softmax=torch.nn.Softmax()\n",
    "# predict_list = []\n",
    "# score_list = []\n",
    "# sm_score_list = []\n",
    "\n",
    "edm_dict = {}\n",
    "\n",
    "# with open('right_edm.csv', newline='', encoding='utf-8') as csvfile:\n",
    "csvfile = pd.read_csv('right_edm.csv')[:10]    # change the value up to the colab limit\n",
    "    # rows = csv.reader(csvfile)\n",
    "for i, token_list in enumerate(csvfile['context']):\n",
    "    token_list = token_list.split()\n",
    "    predict_list = []\n",
    "    score_list = []\n",
    "    sm_score_list = []\n",
    "    for token in token_list:\n",
    "\n",
    "        tokenized_input = tokenizer(token,\n",
    "                                    max_length=20,\n",
    "                                    truncation=True,\n",
    "                                    return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            input_items = {key: val.to(device) for key, val in tokenized_input.items()}\n",
    "    #         del input_items['token_type_ids'] ## bart不需要這個\n",
    "\n",
    "            outputs = model(**input_items)\n",
    "            prediction = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "            prediction = int(prediction)\n",
    "            sm = softmax(outputs.logits[0])\n",
    "\n",
    "            predict_list.append(prediction)\n",
    "            score_list.append(outputs.logits[0][prediction].item())\n",
    "            sm_score_list.append(sm[prediction].item())\n",
    "\n",
    "\n",
    "    dic = {}\n",
    "    for key in predict_list:\n",
    "        dic[key] = dic.get(key, 0) + 1\n",
    "    # print(dic)\n",
    "\n",
    "    # print(predict_list)\n",
    "    # print(score_list)\n",
    "    # print(sm_score_list)\n",
    "    for num, j in enumerate(predict_list):\n",
    "        if j == 1:\n",
    "            # print(token_list[num])\n",
    "            if token_list[num] not in edm_dict.keys():\n",
    "                edm_dict[str(token_list[num])] = 1\n",
    "            else:\n",
    "                edm_dict[str(token_list[num])] += 1\n",
    "    # print(\"--------\")\n",
    "\n",
    "print(dict(sorted(edm_dict.items(), key=operator.itemgetter(1),reverse=True)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2157850b",
   "metadata": {},
   "source": [
    "# HAM statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9494bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soc507/anaconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'求職者': 22, '待遇': 14, '交易': 13, '信件': 6, 'Windows': 6, '回覆': 5, '信用卡': 5, '股份': 5, '收件人': 5, 'sender': 5, '謝謝': 5, '待業': 5, '個人資料': 4, '請款': 4, '收到': 4, '寄件人': 4, 'virus': 4, '寫': 4, 'email': 4, '登入': 3, '新臺幣': 3, '費用': 3, '信用': 3, 'recipient': 3, '檢視': 3, '持有': 3, '接待': 3, '簽約': 2, '損害': 2, 'please': 2, 'notify': 2, 'communications': 2, '薪資': 2, '主任': 2, '作業系統': 2, 'XP': 2, '收取': 2, '提醒您': 2, '客運': 2, 'HKC110292': 2, 'subject': 2, 'pm': 2, 'shall': 2, 'customer': 2, 'statement': 2, '主旨': 1, '寄件': 1, '21KK279981352': 1, '產品編號': 1, '產品名稱': 1, '訂購': 1, '若需': 1, '旅客': 1, '客戶': 1, 'NT': 1, '如非': 1, '扣繳': 1, '需為': 1, '新增': 1, '並請': 1, 'distribution': 1, 'using': 1, 'damages': 1, 'transmitted': 1, '行政助理': 1, '基金會': 1, '總務組長': 1, 'Android': 1, '接受': 1, '小客車': 1, '食物': 1, '入帳日': 1, '1ESHKITT011555': 1, 'notification': 1, 'transaction': 1, 'result': 1, 'message': 1, 'send': 1, 'PM': 1, '請於': 1, '寄件者': 1, '涉及': 1, '有價證券': 1, 'addressee': 1, 'may': 1, 'contain': 1, 'received': 1, 'error': 1, 'products': 1, '業務人員': 1, '居住地': 1, '待人接物': 1, '收信': 1, '中國信託': 1, '商業銀行': 1, '請以': 1, 'used': 1, 'Thanks': 1}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import operator\n",
    "class_number = {'SPAM':0, 'EDM':1, 'HAM':2}\n",
    "model.eval()\n",
    "\n",
    "softmax=torch.nn.Softmax()\n",
    "# predict_list = []\n",
    "# score_list = []\n",
    "# sm_score_list = []\n",
    "\n",
    "edm_dict = {}\n",
    "\n",
    "# with open('right_edm.csv', newline='', encoding='utf-8') as csvfile:\n",
    "csvfile = pd.read_csv('right_ham.csv')[:10]  # change the value up to the colab limit\n",
    "    # rows = csv.reader(csvfile)\n",
    "for i, token_list in enumerate(csvfile['context']):\n",
    "    token_list = token_list.split()\n",
    "    predict_list = []\n",
    "    score_list = []\n",
    "    sm_score_list = []\n",
    "    for token in token_list:\n",
    "\n",
    "        tokenized_input = tokenizer(token,\n",
    "                                    max_length=20,\n",
    "                                    truncation=True,\n",
    "                                    return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            input_items = {key: val.to(device) for key, val in tokenized_input.items()}\n",
    "    #         del input_items['token_type_ids'] ## bart不需要這個\n",
    "\n",
    "            outputs = model(**input_items)\n",
    "            prediction = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "            prediction = int(prediction)\n",
    "            sm = softmax(outputs.logits[0])\n",
    "\n",
    "            predict_list.append(prediction)\n",
    "            score_list.append(outputs.logits[0][prediction].item())\n",
    "            sm_score_list.append(sm[prediction].item())\n",
    "\n",
    "\n",
    "    dic = {}\n",
    "    for key in predict_list:\n",
    "        dic[key] = dic.get(key, 0) + 1\n",
    "    # print(dic)\n",
    "\n",
    "    # print(predict_list)\n",
    "    # print(score_list)\n",
    "    # print(sm_score_list)\n",
    "    for num, j in enumerate(predict_list):\n",
    "        if j == 2:\n",
    "            # print(token_list[num])\n",
    "            if token_list[num] not in edm_dict.keys():\n",
    "                edm_dict[str(token_list[num])] = 1\n",
    "            else:\n",
    "                edm_dict[str(token_list[num])] += 1\n",
    "    # print(\"--------\")\n",
    "\n",
    "print(dict(sorted(edm_dict.items(), key=operator.itemgetter(1),reverse=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661ce0b",
   "metadata": {},
   "source": [
    "# SPAM statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb64c6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soc507/anaconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'思維導圖': 70, 'UrlText': 67, 'cnn': 58, 'emailAddress': 46, '案例': 35, '分析': 35, '數據': 32, '管理': 31, '表達': 30, '課程': 28, 'old': 27, '工作': 26, '邏輯': 24, 'Excel': 24, 'videoplayer': 23, '財務': 22, 'like': 21, '金字塔': 21, 'I': 21, 'gabeira': 21, '筆記': 20, 'PPT': 20, 'wave': 20, 'year': 18, '講': 18, '圖表': 18, 'm': 17, 'big': 17, '企業': 17, 'Y': 17, 'say': 16, 'He': 16, 'Smeeth': 16, 'boys': 16, 'fellows': 16, 'going': 16, 'Heart': 16, 'Sheldy': 16, '思維': 15, '做': 15, 'videopinner': 15, '結構': 14, 'know': 14, 'type': 14, 'playerinstance': 14, 'turn': 13, '原理': 13, '講師': 13, 'get': 13, 'currentvideocollection': 13, 'mobilepinnedview': 13, 'world': 12, '清晰': 12, '基礎': 12, '中': 12, '設計': 12, '成本': 12, 'men': 12, 'tell': 12, 'nazar': 12, 'way': 11, '高效': 11, '認證': 11, '學士': 11, '表': 11, 'sport': 11, 'use': 11, 'jquery': 11, '控制': 11, 'make': 10, 'give': 10, 'go': 10, 'come': 10, '構建': 10, 'C': 10, '方法': 10, '繪製': 10, '關鍵詞': 10, '軟件': 10, '演示': 10, '技巧': 10, '圖': 10, '預算': 10, '透視': 10, 'dinner': 10, 'man': 10, 'explain': 10, 'day': 10, 'typeof': 10, 'length': 10, 'true': 10, 'modernizr': 10, 'watch': 9, '實戰': 9, '工具': 9, '回': 9, 'heard': 9, 'll': 9, 'looked': 9, 'eyes': 9, 'find': 9, 'dataobj': 9, 'var': 9, 'width': 9, 'height': 9, 'autostartvideo': 9, 'nextvideourl': 9, 'smartwatch': 8, 'air': 8, 'first': 8, 'time': 8, 'take': 8, '內容': 8, '標準': 8, 'P': 8, '經驗': 8, '功能': 8, '會議': 8, '模板': 8, '目標': 8, '原則': 8, '郵': 8, '箱': 8, '條件': 8, 'liked': 8, 'Busy': 8, 'Folks': 8, 'Bible': 8, 'Class': 8, 'composed': 8, 'mature': 8, 'women': 8, 'addressed': 8, 'school': 8, 'physician': 8, 'Dr': 8, 'T': 8, 'Atkins': 8, 'Jordan': 8, 'sparkling': 8, 'style': 8, 'comparable': 8, 'refined': 8, 'humorous': 8, 'speakers': 8, 'went': 8, 'junior': 8, 'classes': 8, 'disconcerted': 8, 'Sheldon': 8, 'educational': 8, 'director': 8, 'M': 8, 'leader': 8, 'church': 8, 'choir': 8, 'pale': 8, 'strenuous': 8, 'young': 8, 'curly': 8, 'hair': 8, 'smile': 8, 'teaching': 8, 'sixteen': 8, 'lovingly': 8, 'admonished': 8, 'Now': 8, 'Talk': 8, 'Evening': 8, 'house': 8, 'next': 8, 'Thursday': 8, 'We': 8, 'frank': 8, 'Secret': 8, 'Worries': 8, 'You': 8, 'anything': 8, 'frankly': 8, 'horrible': 8, 'practises': 8, 'kiddy': 8, 'falls': 8, 'unless': 8, 'guided': 8, 'Big': 8, 'Brother': 8, 'perils': 8, 'glory': 8, 'Sex': 8, 'Old': 8, 'beamed': 8, 'damply': 8, 'ashamed': 8, 'Babbitt': 8, 'didn': 8, 'embarrassed': 8, 'age': 8, 'see': 8, 'slate': 8, 'companion': 8, 'new': 7, 'one': 7, '上海': 7, '深圳': 7, '效果': 7, '專業': 7, '核心': 7, '計劃': 7, '聯想': 7, '操作': 7, '公司': 7, '郵箱': 7, '呈現': 7, '採購': 7, '圖片': 7, '幻燈片': 7, '快速': 7, 'INDEX': 7, 'MATCH': 7, '柱狀圖': 7, 'february': 7, 'much': 7, 'else': 7, 'jpg': 7, 'uri': 7, 'contentid': 7, '選擇': 6, 'life': 6, 'start': 6, 'thing': 6, 'different': 6, 'many': 6, '北京': 6, '廣州': 6, '諮詢': 6, '領導': 6, '三種': 6, '法': 6, '步驟': 6, '回顧': 6, '小組': 6, '優化': 6, '結論': 6, '先行': 6, '記': 6, '提升': 6, '時間': 6, '理解': 6, '視覺': 6, '信息': 6, '表格': 6, '能力': 6, '可視化': 6, '微軟': 6, '系統': 6, 'praia': 6, 'norte': 6, 'ever': 6, 'really': 6, 'challenge': 6, 'felt': 6, 'dog': 6, 'parent': 6, 'endslate': 6, 'cvpid': 6, 'updatecompanionlayout': 6, 'D': 6, 'idea': 6, 'chen': 6, '報表': 6, 'water': 5, 'always': 5, 'click': 5, '成都': 5, '元': 5, '建構': 5, '訓練': 5, '專家': 5, '類型': 5, '思考': 5, '優勢': 5, '技術': 5, '分類': 5, '進行': 5, '報': 5, '執': 5, '提高': 5, '動畫': 5, '數據分析': 5, '微信': 5, 'foot': 5, 'thought': 5, 'last': 5, 'also': 5, 'surf': 5, 'story': 5, 'surfer': 5, 'lot': 5, 'home': 5, 'fact': 5, 'ocean': 5, 'feel': 5, 'work': 5, 'well': 5, 'swell': 5, 'every': 5, 'remembertime': 5, 'unmutecta': 5, 'unmute': 5, 'fave': 5, 'getelementbyid': 5, 'videndslate': 5, 'configobj': 5, 'movetonexttimeout': 5, 'nextvideoid': 5, 'currentvideoid': 5, 'videoid': 5, 'tablet': 5, 'pinnedplayer': 5, 'playerid': 5, 'play': 5, 'secret': 5, 'said': 5, 'karim': 5, 'youtube': 5, '成功': 4, 'head': 4, 'look': 4, 'feature': 4, 'company': 4, 'already': 4, 'think': 4, 'move': 4, 'people': 4, 'nothing': 4, 'around': 4, 'become': 4, '定製': 4, '內訓': 4, '第一': 4, '討論': 4, '詳解': 4, 'S': 4, '三講': 4, '歸類': 4, '分組': 4, 'MECE': 4, '統下': 4, '順序': 4, '重要性': 4, '常用': 4, '第五': 4, '背景': 4, '工商企業': 4, '東尼': 4, '博贊': 4, '管理師': 4, '四大': 4, '高級': 4, '定義': 4, '一張': 4, '四個': 4, '記筆記': 4, '5W2H': 4, '電腦操作': 4, '現場': 4, '發送': 4, '電': 4, '話': 4, '發': 4, '送': 4, '勿': 4, '直': 4, '接': 4, '復': 4, '收益': 4, '規範': 4, '放映': 4, '演講': 4, '字體': 4, '形狀': 4, '柱形圖': 4, '成爲': 4, '影響': 4, '價值': 4, '學會': 4, '填充': 4, '格式': 4, 'SUBTOTAL': 4, '培訓': 4, '授課': 4, '學員': 4, 'white': 4, 'portugal': 4, 'almost': 4, 'accident': 4, 'imageobject': 4, 'description': 4, 'something': 4, 'large': 4, 'energy': 4, 'create': 4, 'fear': 4, 'scar': 4, 'realize': 4, 'happen': 4, 'sense': 4, 'eat': 4, 'drop': 4, 'videx': 4, 'playerpropertyobj': 4, 'n': 4, 'window': 4, 'mute': 4, 'active': 4, 'j': 4, 'eq': 4, 'autoplayvideoexist': 4, 'markupid': 4, 'videoendslateimpl': 4, 'vidobj': 4, 'undefined': 4, 'wife': 4, 'Artagnan': 4, 'suzannemiLLer': 4, 'found': 4, 'vision': 4, 'keep': 4, 'party': 4, 'testosterone': 4, 'hurley': 4, '運作': 4, '現金流量': 4, '存貨': 4, '改善': 4, '週轉率': 4, '順豐': 3, '業務': 3, '發票': 3, 'late': 3, 'thousand': 3, 'important': 3, 'heart': 3, 'st': 3, 'need': 3, 'bar': 3, 'early': 3, '來電': 3, '故事': 3, '文字': 3, '總分': 3, '排序': 3, '介紹': 3, '老師': 3, '結構化': 3, '版權': 3, '中國電信': 3, '在職場': 3, '投資': 3, '要素': 3, '前': 3, '三個': 3, '創意': 3, '關聯性': 3, '姓名': 3, '職位': 3, '電話': 3, '商務': 3, '價格': 3, '中國': 3, '層次': 3, '展示': 3, '大綱': 3, '平行四邊形': 3, '轉換': 3, '型': 3, '對齊': 3, '頁面': 3, '巧用': 3, '技能': 3, '學習': 3, '產品': 3, '背後': 3, '規則': 3, '計算': 3, 'IF': 3, '查找': 3, 'SUMPRODUCT': 3, '篩選': 3, '溫度計': 3, '式': 3, 'baomingzixun': 3, '添加': 3, '微信號': 3, '退訂': 3, 'unsubscribetd': 3, '郵件': 3, '至此': 3, 'two': 3, 'set': 3, 'site': 3, 'fellow': 3, 'able': 3, 'difficulty': 3, 'deal': 3, 'level': 3, 'health': 3, 'follow': 3, 'laureus': 3, 'berlin': 3, 'action': 3, 'sportsperson': 3, 'guy': 3, 'drive': 3, 'gabiera': 3, 'compete': 3, 'tow': 3, 'somebody': 3, 'push': 3, 'photographer': 3, 'capture': 3, 'giant': 3, 'october': 3, 'change': 3, 'learn': 3, 'bed': 3, 'nightmare': 3, 'january': 3, 'online': 3, 'wsl': 3, 'month': 3, 'back': 3, 'strict': 3, 'isplayermute': 3, 'playerproperties': 3, 'removeclass': 3, 'addclass': 3, 'inactive': 3, 'showflashslate': 3, 'pandemic': 3, 'body': 3, 'containerel': 3, 'findnextvideo': 3, 'videourl': 3, 'jsmd': 3, 'either': 3, 'pause': 3, 'player': 3, 'metadata': 3, 'duration': 3, 'id': 3, 'another': 3, 'ad': 3, 'cleartimeout': 3, 'hidespinner': 3, 'setisplaying': 3, 'exchange': 3, 'ritual': 3, 'decide': 3, '經理': 3, '知名': 3, '應': 3, 'letter': 3, 'r': 3, 'com': 3, 'flower': 3, 'petal': 3, 'cat': 3, 'power': 3, 'today': 3, 'lead': 3, 'silent': 3, 'diabetes': 3, '資金': 3, '解讀': 3, '績效': 3, '現金': 3, '總監': 3, '行業': 3, '感謝': 2, '速運並': 2, '開票': 2, '電子': 2, '附件': 2, 'invoice': 2, '下載': 2, 'simple': 2, 'fun': 2, 'europe': 2, 'launch': 2, 'real': 2, 'three': 2, 'less': 2, 'satisfy': 2, 'popular': 2, 'tracker': 2, 'high': 2, 'easy': 2, 'light': 2, 'ring': 2, 'personal': 2, 'etc': 2, 'nearly': 2, 'u': 2, 'date': 2, 'notice': 2, 'pack': 2, 'update': 2, 'street': 2, 'sometimes': 2, 'bad': 2, 'madness': 2, 'manager': 2, 'recover': 2, 'monday': 2, 'possible': 2, 'improve': 2, 'seem': 2, 'towards': 2, 'four': 2, 'place': 2, 'wash': 2, 'liquid': 2, '課': 2, '程': 2, '測試': 2, '作者': 2, '想': 2, '場景': 2, '演練': 2, '彙報': 2, '版本': 2, '漏斗': 2, '子結構': 2, '水平': 2, '首要': 2, '因素': 2, '市長': 2, '懸念': 2, '序言': 2, 'Situation': 2, 'Complication': 2, 'Q': 2, 'Question': 2, 'Answer': 2, '向下': 2, '分解': 2, '自上而下': 2, '總彙': 2, '報時': 2, '情景': 2, '結構圖': 2, 'TOPS': 2, '三項': 2, '口頭': 2, '實用工具': 2, 'PRES': 2, 'Point': 2, 'R': 2, 'Reason': 2, 'E': 2, 'Example': 2, 'Summary': 2, '判斷題': 2, 'NPA': 2, 'N': 2, 'Necessity': 2, 'Prospect': 2, 'Analysis': 2, '選擇題': 2, 'POP': 2, 'Position': 2, 'O': 2, 'Option': 2, 'Proposal': 2, '吳': 2, '南昌大學': 2, '雙證': 2, 'MBA': 2, '結構性': 2, '英國': 2, '官方': 2, '英語': 2, '漢語言': 2, '文學': 2, '人力資源': 2, '基礎知識': 2, '概述': 2, '六個': 2, '誕生': 2, '由來': 2, '用途': 2, '一份': 2, '三步曲': 2, '導圖': 2, '傳統': 2, '行列式': 2, '六種': 2, '兩種': 2, '核心技術': 2, '抓取': 2, '找': 2, '區別': 2, '視頻': 2, '模擬': 2, '注意事項': 2, '工作效率': 2, '法則': 2, '設定': 2, 'SMART': 2, '問': 2, '象限': 2, '目的': 2, '接龍': 2, '開花': 2, '腦袋': 2, '卡住': 2, '着手': 2, '與生俱來': 2, '五感': 2, '策劃': 2, 'iMindMap': 2, 'MindMananger': 2, '第六': 2, '席位': 2, '報名': 2, 'singupxuexi': 2, '聯': 2, '系': 2, '信': 2, '之道': 2, '降低': 2, '各項': 2, '時': 2, '說服力': 2, '第一章': 2, '字': 2, '感覺': 2, '正確': 2, '搜索': 2, '商業': 2, '帶來': 2, '動態': 2, '佈局': 2, '好圖': 2, '收集': 2, '降噪': 2, '設置': 2, '並列': 2, '重複': 2, '元素': 2, '自定義': 2, '傳遞': 2, '圖示': 2, '展現': 2, '過程': 2, '觀衆': 2, '美化': 2, '更具': 2, '第二章': 2, '目錄': 2, '過渡': 2, '頁': 2, '第三章': 2, '六大': 2, '對象': 2, '簡單': 2, '非': 2, '重點': 2, '第四章': 2, '成果': 2, '一頁': 2, '場合': 2, '辦公': 2, '高階': 2, '強化': 2, '刺激': 2, '記憶': 2, '編制': 2, '實踐': 2, '超級': 2, '提醒': 2, '預警': 2, '日期': 2, '綜合': 2, '另類': 2, '提成': 2, '隱藏': 2, '狀態': 2, '切片': 2, '線': 2, '柱狀': 2, 'MVP': 2, '設計師': 2, '教育': 2, '年度': 2, '教學': 2, '擅長': 2, 'Office': 2, '任職': 2, '運': 2, '課程名稱': 2, '場次': 2, '手機': 2, '聯繫人': 2, 'Steven': 2, 'Zhou': 2, 'gongkaikezixun': 2, 'los': 2, 'angeles': 2, 'forward': 2, 'ankle': 2, 'night': 2, 'pa': 2, 'documentary': 2, 'ounce': 2, 'catch': 2, 'surfed': 2, 'five': 2, 'drown': 2, 'piano': 2, 'doctor': 2, 'continue': 2, 'clyde': 2, 'reflect': 2, 'split': 2, 'let': 2, 'alone': 2, 'north': 2, 'view': 2, 'show': 2, 'everything': 2, 'organize': 2, 'never': 2, 'priority': 2, 'nominated': 2, 'brazil': 2, 'serious': 2, 'quickly': 2, 'feeling': 2, 'admire': 2, 'stabilize': 2, 'survival': 2, 'still': 2, 'step': 2, 'ground': 2, 'american': 2, 'hard': 2, 'crazy': 2, 'everyone': 2, 'quite': 2, 'uncertain': 2, 'wait': 2, 'rid': 2, 'enough': 2, 'live': 2, 'nice': 2, 'area': 2, 'domestic': 2, 'often': 2, 'recent': 2, 'instance': 2, 'comfortable': 2, 'disparate': 2, 'needed': 2, 'advantage': 2, 'half': 2, 'guinness': 2, 'establish': 2, 'travel': 2, 'care': 2, 'suck': 2, 'near': 2, 'handleunmuteplayer': 2, 'unmuteidselector': 2, 'getlibraryname': 2, 'getinstance': 2, 'utils': 2, 'billy': 2, 'kemper': 2, 'champion': 2, 'covid': 2, 'spt': 2, 'intl': 2, 'profile': 2, 'article': 2, 'medium': 2, 'isvideoreplayclicked': 2, 'callbackobj': 2, 'currentvideocollectionid': 2, 'isliveplayer': 2, 'muteplayerenabled': 2, 'turnonflashmessaging': 2, 'isloggedinvideocheck': 2, 'v': 2, 'navigatetonextvideo': 2, 'nextvideoplaytimeout': 2, 'js': 2, 'pg': 2, 'transition': 2, 'hideui': 2, 'showui': 2, 'handlemobilepinnedplayerstates': 2, 'endslatelen': 2, 'videosourceutils': 2, 'token': 2, 'mode': 2, 'blockid': 2, 'adtype': 2, 'dismiss': 2, 'dismissmobilepinnedplayer': 2, 'animatedown': 2, 'isadpause': 2, 'employee': 2, 'bee': 2, 'african': 2, 'manhood': 2, 'elongation': 2, 'oh': 2, 'tribe': 2, 'incredible': 2, 'Advanced': 2, 'Applications': 2, 'Mind': 2, 'Map': 2, 'In': 2, 'Workplace': 2, 'Pyramid': 2, 'Principle': 2, 'Logical': 2, 'Thinking': 2, 'Effective': 2, 'Expression': 2, '增強': 2, '推廣': 2, '班': 2, '研發': 2, '解決': 2, 'summons': 2, 'without': 2, 'Tours': 2, 'w': 2, 'must': 2, 'b': 2, 'eparada': 2, 'far': 2, 'd': 2, 'hos': 2, 'heqing': 2, 'xyzf': 2, 'cold': 2, 'e': 2, 'czfkzyfbro': 2, 'good': 2, 'sky': 2, 'p': 2, 'terrible': 2, 'crush': 2, 'swallow': 2, 'repair': 2, 'friend': 2, 'certain': 2, 'reverse': 2, 'mary': 2, 'full': 2, 'finally': 2, 'happy': 2, 'sock': 2, 'dryer': 2, 'match': 2, 'win': 2, 'somehow': 2, 'bring': 2, 'memory': 2, 'tomorrow': 2, 'food': 2, 'best': 2, 'understand': 2, 'taste': 2, 'paul': 2, 'service': 2, 'could': 2, 'killer': 2, 'low': 2, 'link': 2, 'develop': 2, 'yet': 2, 'natural': 2, 'boost': 2, 'sexual': 2, 'bedroom': 2, 'study': 2, 'university': 2, 'share': 2, '部門': 2, '狀況': 2, '風險': 2, '意識': 2, '公司財務': 2, '角度': 2, '意義': 2, '利潤': 2, '利潤率': 2, '收賬': 2, '款': 2, '業績': 2, '週轉': 2, '危機': 2, '增加': 2, '關鍵點': 2, '新': 2, '上市公司': 2, '華': 2, '消費品': 2, 'Sf': 1, 'Delivery': 1, 'Dec': 1, '尊敬': 1, '速': 1, '戶': 1, '開具': 1, '詳情': 1, 'The': 1, 'electronic': 1, 'successfully': 1, 'Please': 1, 'attachment': 1, 'details': 1, 'View': 1, 'Receipt': 1, 'Best': 1, 'Regards': 1, 'emojiPic': 1, 'Sfexpress': 1, 'Team': 1, 'mail': 1, 'ref': 1, 'Lynn': 1, 'technology': 1, 'top': 1, 'put': 1, 'amazing': 1, 'abundance': 1, 'maker': 1, 'run': 1, 'money': 1, 'convinced': 1, 'flaship': 1, 'era': 1, 'costing': 1, 'competition': 1, 'demand': 1, 'focus': 1, 'fitness': 1, 'sleek': 1, 'quality': 1, 'read': 1, 'display': 1, 'accurate': 1, 'tracking': 1, 'rate': 1, 'rhythm': 1, 'resistant': 1, 'menstrual': 1, 'cycle': 1, 'sleep': 1, 'monitor': 1, 'activity': 1, 'progress': 1, 'inspire': 1, 'sit': 1, 'voice': 1, 'assistant': 1, 'arrange': 1, 'appointment': 1, 'safe': 1, 'clever': 1, 'extras': 1, 'west': 1, 'carpenter': 1, 'plattsburgh': 1, 'ny': 1, 'completely': 1, 'as': 1, 'laugh': 1, 'realise': 1, 'wolverhampton': 1, 'wanderer': 1, 'nuno': 1, 'espirito': 1, 'santo': 1, 'goalkeeper': 1, 'nickel': 1, 'rui': 1, 'patricio': 1, 'totally': 1, 'conscious': 1, 'transfer': 1, 'sustain': 1, 'counter': 1, 'liverpool': 1, 'barcelona': 1, 'ronald': 1, 'unwelcome': 1, 'koeman': 1, 'lionel': 1, 'messi': 1, 'round': 1, 'history': 1, 'lend': 1, 'argentine': 1, '767th': 1, 'appearance': 1, 'packaging': 1, 'catalan': 1, 'amity': 1, 'john': 1, 'travolta': 1, 'fairly': 1, 'unhappy': 1, 'protege': 1, 'bell': 1, 'joe': 1, 'frazier': 1, 'bob': 1, 'muhammad': 1, 'ali': 1, 'approximately': 1, 'verify': 1, '20th': 1, 'norm': 1, 'century': 1, 'influential': 1, 'figure': 1, 'except': 1, 'struggle': 1, 'geography': 1, 'sure': 1, 'somewhere': 1, 'virtual': 1, 'stadiumteam': 1, 'unconfirmed': 1, 'game': 1, 'titanium': 1, 'arhancet': 1, 'akin': 1, 'own': 1, 'literacy': 1, 'prison': 1, 'coffin': 1, 'bf6a': 1, '投': 1, '資': 1, 'Lecturer': 1, '蘇州': 1, '左側': 1, '預留': 1, '袁傑華': 1, 'Mark': 1, 'Yuan': 1, '微': 1, '號': 1, '1980RMB': 1, '新常態': 1, '各類': 1, '發生': 1, '改變': 1, '產能': 1, '過剩': 1, '競爭': 1, '壓力': 1, '越來越': 1, '現狀': 1, '恰當': 1, '提出': 1, '我方': 1, '虛高': 1, '當我': 1, '方爲': 1, '弱勢': 1, '原材料': 1, '招標': 1, '業內': 1, '同行': 1, '關注': 1, '焦點': 1, '議題': 1, '靈活': 1, '母版': 1, '美觀': 1, '完美': 1, '風采': 1, '漢字': 1, '色彩': 1, '藝術': 1, '多行': 1, '文本': 1, '相關': 1, '包括': 1, '安裝': 1, '加載': 1, '可用': 1, '作用': 1, '長方形': 1, '給人': 1, '穩定': 1, '圓圈': 1, '聚焦': 1, '三角形': 1, '編輯': 1, '配色': 1, '哪裏': 1, '透明': 1, '高清': 1, '互聯網': 1, '搜索詞': 1, '所選': 1, '摳': 1, '裁剪': 1, '壓縮': 1, '裁': 1, '視覺化': 1, '透明度': 1, '虛化': 1, '融合': 1, '強大': 1, '圖形': 1, '主題': 1, '階梯型': 1, '環繞': 1, '表現形式': 1, '切換': 1, '缺少': 1, '流程': 1, '傳達': 1, '準確': 1, '突出重點': 1, '萬能': 1, '套路': 1, '變異': 1, 'EXCEL': 1, '同步': 1, '更新': 1, '封面': 1, '內頁': 1, '結尾': 1, '篇章': 1, '包含': 1, '轉折': 1, '劃': 1, '模塊': 1, '分': 1, '化': 1, '形象': 1, '出發點': 1, '排版': 1, '無憂': 1, '美感': 1, '一致性': 1, '親密': 1, '應有': 1, '留白': 1, '大腦': 1, '超愛': 1, '簡約': 1, '弱化': 1, '流暢': 1, '信手拈來': 1, '自如': 1, '快捷鍵': 1, '視圖': 1, '告別': 1, '忘': 1, '臺詞': 1, '尷尬': 1, '風格': 1, '要點': 1, '聽衆': 1, '尋找': 1, '適合': 1, '材料': 1, '完整': 1, '演講詞': 1, '開場白': 1, '結束語': 1, '輔助': 1, '2500RMB': 1, 'excel': 1, '廣泛性': 1, '學好': 1, '職場': 1, '選修課': 1, '必修課': 1, '職員': 1, '未來': 1, '組建': 1, '團隊': 1, '決策者': 1, '都會': 1, '持續': 1, '翅膀': 1, '掣肘': 1, '需求': 1, '訓練營': 1, '人員': 1, '進階': 1, '開發': 1, '工具集': 1, '常說': 1, '一圖勝': 1, '千言': 1, '直觀': 1, '地向': 1, '還可': 1, '以爲': 1, '感官': 1, '有助於': 1, '加深': 1, '過目不忘': 1, '資源': 1, '不止於': 1, '探尋': 1, '思路': 1, '幫': 1, '助講': 1, '着': 1, '更好': 1, '祕密': 1, '講述': 1, '經典': 1, '理念': 1, '美學': 1, '並進': 1, '洞察': 1, '之美': 1, '訪問': 1, '工具欄': 1, '文件': 1, '自動': 1, '保存': 1, '恢復': 1, '啓用': 1, '開發工具': 1, '選項': 1, '卡': 1, '專屬': 1, '界面': 1, '坑': 1, '真': 1, '區域': 1, '二維': 1, '錶': 1, '一維': 1, '大數據': 1, '規範化': 1, '分列': 1, '條': 1, '色階': 1, '圖標': 1, '集': 1, '公式': 1, '漲紅': 1, '跌綠': 1, '合同': 1, '到期': 1, '花名冊': 1, 'VLOOKUP': 1, 'COLUMN': 1, 'DATEDIF': 1, '年齡': 1, '工齡': 1, '家族': 1, '三段式': 1, 'AND': 1, 'OR': 1, '判斷': 1, '最值': 1, 'MAXIFS': 1, 'MINIFS': 1, '雙劍合': 1, '璧': 1, '常規': 1, '精確': 1, '一對': 1, '目標值': 1, '含有': 1, '通配': 1, '符': 1, '返回': 1, '分數': 1, '等級': 1, '用法': 1, 'MAX': 1, '下限': 1, 'LARGE': 1, 'SMALL': 1, 'COUNTIF': 1, 'COUNTIFS': 1, '老實': 1, 'SUMPRODUCTSUMPRODUCT': 1, '演講比賽': 1, '評分': 1, '求和': 1, '行': 1, '生成': 1, '連續': 1, '序號': 1, '第五章': 1, '刷新': 1, '器': 1, '表之': 1, '透視圖': 1, '器分': 1, '析': 1, '第六章': 1, '初探': 1, '更改': 1, '調用': 1, '圓餅': 1, '變化': 1, '雙': 1, '軸': 1, '懸浮': 1, '長': 1, '標籤': 1, '條形': 1, '圓環': 1, '正負': 1, '趙': 1, 'Adobe': 1, 'ACCD': 1, '深圳市': 1, '技工': 1, '職業培訓': 1, '優秀教師': 1, 'IT': 1, '資深': 1, '平面': 1, '平面設計': 1, '熟練掌握': 1, 'Power': 1, 'BI': 1, 'Photoshop': 1, 'Illustrator': 1, 'InDesign': 1, 'CorelDraw': 1, '桌面': 1, '資質': 1, '證書': 1, '教學風格': 1, '紮實': 1, '幽默': 1, '風趣': 1, '多年': 1, '課堂': 1, '理論': 1, '相結合': 1, '條理': 1, '情況': 1, '調整': 1, '方案': 1, '解答': 1, '耐心': 1, '細緻': 1, '每次': 1, '收穫': 1, '頗豐': 1, '好評': 1, '戀': 1, '高話': 1, '撿': 1, '漸訣': 1, '賄廉互': 1, '加交壩銘': 1, '兩鏈': 1, '貢半骨': 1, '煉': 1, '陸機': 1, '恨門': 1, '呼敗': 1, '對鬥': 1, '賈鱗老': 1, '飢闌齡': 1, '難': 1, '僅姐': 1, '編賦': 1, '鋼亢': 1, '飢鐐': 1, '邏': 1, '堤困': 1, '貳': 1, '類輯': 1, '凍': 1, '昆顆': 1, '綁': 1, '訃': 1, '螞潮': 1, '枯': 1, '僥花廚': 1, '擯': 1, '次美囪': 1, '競': 1, 'reuters': 1, 'lakers': 1, 'lebron': 1, 'james': 1, 'vocal': 1, 'reinjured': 1, 'right': 1, 'sunday': 1, 'unclear': 1, 'butler': 1, 'denver': 1, 'nugget': 1, 'slower': 1, 'eight': 1, 'sassy': 1, 'whiz': 1, 'kid': 1, 'nourish': 1, 'camera': 1, 'accompany': 1, 'catchy': 1, 'tune': 1, 'cornea': 1, 'netflix': 1, 'nazi': 1, 'short': 1, 'olympia': 1, 'maya': 1, 'carve': 1, 'ripple': 1, 'across': 1, 'face': 1, 'windsor': 1, 'flash': 1, 'mind': 1, 'preferably': 1, 'horny': 1, 'naming': 1, 'detox': 1, 'measure': 1, 'raleigh': 1, 'meter': 1, 'surpass': 1, 'past': 1, 'aladdin': 1, 'statistic': 1, 'enrich': 1, 'footage': 1, 'part': 1, 'sandal': 1, 'endure': 1, 'tart': 1, 'surgery': 1, 'toast': 1, 'painstaking': 1, 'rehabilitation': 1, 'recorded': 1, 'avalanche': 1, 'samurai': 1, 'occur': 1, 'patriotism': 1, 'point': 1, 'biological': 1, 'glitter': 1, 'knock': 1, 'unconscious': 1, 'trapped': 1, 'texture': 1, 'swirl': 1, 'jet': 1, 'ski': 1, 'rider': 1, 'carlos': 1, 'burle': 1, 'conclusively': 1, 'depression': 1, 'cpr': 1, 'soho': 1, 'stop': 1, 'showroom': 1, 'identification': 1, 'uncertainty': 1, 'jazzy': 1, 'future': 1, 'line': 1, 'recovery': 1, 'bronx': 1, 'wildfire': 1, 'testament': 1, 'officer': 1, 'resilience': 1, 'aerospace': 1, 'lisbon': 1, 'lie': 1, 'renowned': 1, 'tulsa': 1, 'hone': 1, 'rich': 1, 'specifically': 1, 'humble': 1, 'rack': 1, 'leftover': 1, 'lucrative': 1, 'control': 1, 'cabinet': 1, 'worm': 1, 'detached': 1, 'achievement': 1, 'beloved': 1, 'funny': 1, 'medieval': 1, 'alcazar': 1, 'disgust': 1, 'extensively': 1, 'physic': 1, 'angel': 1, 'indifferent': 1, 'similarly': 1, 'training': 1, 'moth': 1, 'nominate': 1, 'compression': 1, 'nightlife': 1, 'electoral': 1, 'botanical': 1, 'lump': 1, 'grow': 1, 'fourteen': 1, 'dancer': 1, 'mako': 1, 'despite': 1, 'family': 1, 'wog': 1, 'connection': 1, 'postal': 1, 'hook': 1, 'hula': 1, 'sheer': 1, 'addictive': 1, 'lifestyle': 1, 'bail': 1, 'sensation': 1, 'frequent': 1, 'apparel': 1, 'precursor': 1, 'guadalajara': 1, 'boy': 1, 'solidly': 1, 'wag': 1, 'admired': 1, 'bravery': 1, 'radiation': 1, 'seoul': 1, 'portray': 1, 'suffix': 1, 'unconsciously': 1, 'midnight': 1, 'broom': 1, 'marsh': 1, 'wallet': 1, 'entrepreneur': 1, 'decoration': 1, 'hawaii': 1, 'pursue': 1, 'career': 1, 'virgin': 1, 'richards': 1, 'currently': 1, 'prepare': 1, 'indonesia': 1, 'kern': 1, 'hunting': 1, 'unwanted': 1, 'melt': 1, 'forecast': 1, 'corner': 1, 'torte': 1, 'descend': 1, 'myth': 1, 'brazilian': 1, 'rodrigo': 1, 'koxa': 1, 'diagram': 1, 'octet': 1, 'prune': 1, 'garrett': 1, 'mcnamara': 1, 'jewelry': 1, 'dynasty': 1, 'expect': 1, 'amati': 1, 'nerve': 1, 'pile': 1, 'kelly': 1, 'sheffield': 1, 'naval': 1, 'heartwarming': 1, 'map': 1, 'tito': 1, 'gossip': 1, 'consciously': 1, 'nervous': 1, 'writer': 1, 'turnover': 1, 'mohammed': 1, 'nervousness': 1, 'excitement': 1, 'pisa': 1, 'impulse': 1, 'aberdeen': 1, 'essential': 1, 'ingredient': 1, 'psychiatry': 1, 'discipline': 1, 'involve': 1, 'long': 1, 'orifice': 1, 'massive': 1, 'materialize': 1, 'consequent': 1, 'factor': 1, 'paddy': 1, 'beard': 1, 'honestly': 1, 'navigate': 1, 'territory': 1, 'lidar': 1, 'assist': 1, 'raw': 1, 'habitude': 1, 'babylon': 1, 'divide': 1, 'alpha': 1, 'immune': 1, 'pronounced': 1, 'illuminate': 1, 'percy': 1, 'bey': 1, 'explains': 1, 'marvel': 1, 'commercially': 1, 'athlete': 1, 'wrong': 1, 'grudge': 1, 'close': 1, 'choke': 1, 'patagonia': 1, 'position': 1, 'oates': 1, 'aligns': 1, 'tableau': 1, 'everywhere': 1, 'ishita': 1, 'malaviya': 1, 'india': 1, 'professional': 1, 'mailbox': 1, 'country': 1, 'perception': 1, 'neatly': 1, 'rote': 1, 'ultimo': 1, 'obstruction': 1, 'compensate': 1, 'batter': 1, 'trace': 1, 'lesson': 1, 'ensure': 1, 'alignment': 1, 'whenever': 1, 'grapefruit': 1, 'ahead': 1, 'civilian': 1, 'fill': 1, 'calm': 1, 'avo': 1, 'wake': 1, 'blaster': 1, 'binghamton': 1, 'clinton': 1, 'lady': 1, 'scenario': 1, 'miracle': 1, 'reason': 1, 'disadvantage': 1, 'comeback': 1, 'debit': 1, 'least': 1, 'connect': 1, 'metric': 1, 'globally': 1, 'lust': 1, 'warmth': 1, 'hopefully': 1, 'ph': 1, 'battling': 1, 'recognitionit': 1, 'transpire': 1, 'trade': 1, 'battle': 1, 'lengthy': 1, 'lieutenant': 1, 'confirmed': 1, 'franc': 1, 'accrue': 1, 'martinique': 1, 'checksum': 1, 'persuade': 1, 'orld': 1, 'protective': 1, 'gwr': 1, '1900s': 1, 'morally': 1, 'headquarters': 1, 'excise': 1, 'promise': 1, 'support': 1, 'istanbul': 1, 'unanswered': 1, 'teflon': 1, 'effort': 1, 'vain': 1, 'attack': 1, 'ebitda': 1, 'huge': 1, 'aries': 1, 'legally': 1, 'epstein': 1, 'surveillance': 1, 'book': 1, 'refresher': 1, 'personally': 1, 'croatian': 1, 'evacuation': 1, 'cement': 1, 'thickness': 1, 'humiliate': 1, 'kind': 1, 'manner': 1, 'especially': 1, 'satirical': 1, 'revolutionary': 1, 'process': 1, 'benny': 1, 'airfare': 1, 'injector': 1, 'executefeature': 1, 'native': 1, 'ancestral': 1, 'belle': 1, 'cnnvideomanager': 1, 'getplayerbycontainer': 1, 'videoinstance': 1, 'cvp': 1, 'swarm': 1, 'boolean': 1, 'strained': 1, 'untouched': 1, 'contentplayed': 1, 'wound': 1, 'tempo': 1, 'storelocalvalue': 1, 'x': 1, 'cornu': 1, 'softly': 1, 'l': 1, 'html': 1, 'hesitant': 1, 'spectrum': 1, 'thumb': 1, 'none': 1, 'xxx': 1, 'section': 1, 'expansion': 1, 'theoplayer': 1, 'allownativefullscreen': 1, 'adsection': 1, 'const': 1, 'inpage': 1, 'framewidth': 1, 'frameheight': 1, 'posterimageoverride': 1, 'mini': 1, 'xsmall': 1, 'small': 1, 'full16x9': 1, 'absurd': 1, 'mediametadatacallbacks': 1, 'bloodstream': 1, 'seventeen': 1, 'honor': 1, 'commission': 1, 'recoup': 1, 'autostart': 1, 'negotiation': 1, 'enableautoplayblock': 1, 'setplayerproperties': 1, 'ares': 1, 'setfirstvideoincollection': 1, 'videoendslate': 1, 'graphite': 1, 'isarray': 1, 'warwick': 1, 'shoestring': 1, 'stomp': 1, 'dirt': 1, 'caramel': 1, 'plead': 1, 'flare': 1, 'evar60': 1, 'replace': 1, 'playlist': 1, 'pang': 1, 'gandhi': 1, 'serra': 1, 'settimeout': 1, 'location': 1, 'href': 1, 'formally': 1, 'confidently': 1, 'showendslateforcontainer': 1, 'onplayerready': 1, 'handleinitialexpandablevideostate': 1, 'handleadoncvpvisibilitychange': 1, 'utica': 1, 'pagevis': 1, 'isdocumentvisible': 1, 'enablemobilewebfloatingplayer': 1, 'dodo': 1, 'contentmodel': 1, 'pagetype': 1, 'enable': 1, 'mobilewebfloatingplayer': 1, 'onpin': 1, 'roof': 1, 'onunpin': 1, 'onplayerclick': 1, 'valentine': 1, 'enterfullscreen': 1, 'ondismiss': 1, 'store': 1, 'pinned': 1, 'beat': 1, 'helpful': 1, 'single': 1, 'pin': 1, 'disregard': 1, 'brighten': 1, 'lantern': 1, 'init': 1, 'hidethumbnail': 1, 'oncontententryload': 1, 'isqueue': 1, 'unknown': 1, 'showspinner': 1, 'oncontentpause': 1, 'canon': 1, 'oncontentmetadata': 1, 'gratification': 1, 'rend': 1, 'updatesource': 1, 'fetchandshowrecommendedvideos': 1, 'onadplay': 1, 'enigmatic': 1, 'bull': 1, 'bimbo': 1, 'onadpause': 1, 'stand': 1, 'ontrackingfullscreen': 1, 'montreal': 1, 'handlefullscreenchange': 1, 'generosity': 1, 'os': 1, 'io': 1, 'fullscreen': 1, 'scrolltop': 1, 'getscrollposition': 1, 'oncontentplay': 1, 'catcher': 1, 'prevvideoid': 1, 'cnncompanion': 1, 'formation': 1, 'restoreepicads': 1, 'associate': 1, 'delicate': 1, 'fork': 1, 'oncontentreplayrequest': 1, 'dependant': 1, 'tiered': 1, 'relax': 1, 'oncontentbegin': 1, 'vigorously': 1, 'rash': 1, 'muteplayer': 1, 'removeepicads': 1, 'clearsource': 1, 'triggervideocontentstarted': 1, 'oncontentcomplete': 1, 'reconsider': 1, 'restorefreewheel': 1, 'oncontentend': 1, 'outage': 1, 'offer': 1, 'tribesman': 1, 'gift': 1, 'god': 1, 'dude': 1, 'summer': 1, 'pay': 1, 'sacred': 1, 'africa': 1, 'legendary': 1, 'sombas': 1, 'elites': 1, 'scientific': 1, 'special': 1, 'decade': 1, 'try': 1, 'insight': 1, 'luck': 1, 'unthinkable': 1, 'growth': 1, 'whole': 1, 'careful': 1, 'wisely': 1, 'grows': 1, 'member': 1, 'inch': 1, 'week': 1, 'monster': 1, 'wonder': 1, 'score': 1, 'chick': 1, 'answer': 1, 'yes': 1, 'flg': 1, '立體': 1, '思維能力': 1, '層次性': 1, '性': 1, '羣策羣力': 1, '高手': 1, '面對': 1, '難題': 1, '遊刃有餘': 1, '創新能力': 1, '推進': 1, '項目': 1, '條理清晰': 1, '有條不紊': 1, '煥發': 1, '創造': 1, '靈感': 1, '智多星': 1, '牢固': 1, '事實': 1, '二種': 1, '脫穎而出': 1, '致勝': 1, '四項': 1, '招式': 1, '現任': 1, '中國電信集團': 1, '江西': 1, '權威': 1, '業界': 1, '授權': 1, '專注': 1, '系列': 1, '研究': 1, '導': 1, '訓練班': 1, '思考力': 1, '擔任': 1, '江西省': 1, '公司員工': 1, '成長': 1, '輔導': 1, '多套': 1, '全省': 1, '獲': 1, '百佳': 1, '優秀企業': 1, '內訓師': 1, '榮譽稱號': 1, '自主': 1, '效能': 1, '卓越': 1, '譽爲': 1, '國內': 1, '落地': 1, '版': 1, '解決問題': 1, '不清': 1, '商界': 1, '政界': 1, '學界': 1, '人士': 1, '表達能力': 1, '高': 1, '基層人員': 1, '尚地': 1, 'ecret': 1, 'tenderly': 1, 'wh': 1, 'Will': 1, 'rende': 1, 'etired': 1, 'speaking': 1, 'understa': 1, 'afzfkzL': 1, 'xyznever': 1, 'denied': 1, 'personalbrav': 1, 'patrick': 1, 'manLey': 1, 'xyzhet': 1, 'receive': 1, 'sevenhun': 1, 'tagnan': 1, 'cameas': 1, 'usual': 1, 'h': 1, 'beating': 1, 'viLLhqxqos': 1, 'cnowed': 1, 'mistress': 1, 'Por': 1, 'xyzerie': 1, 'answered': 1, 'ot': 1, 'approving': 1, 'xyzerson': 1, 'useless': 1, 'pleuri': 1, 'prudencewhich': 1, 'tamccraysr': 1, 'xyzt': 1, 'clean': 1, 'ew': 1, 'sort': 1, 'kee': 1, 'stevenmquinLan': 1, 'xyzot': 1, 'thisBastille': 1, 'My': 1, 'frankLuera': 1, 'xyzed': 1, 'chamber': 1, 'closed': 1, 'c': 1, 'xyze': 1, 'Lordd': 1, 'ved': 1, 'withfresh': 1, 'eggs': 1, 'xyztimental': 1, 'hecould': 1, 'assu': 1, 'Athos': 1, 'disdainful': 1, 'aLphamega': 1, 'xyzecial': 1, 'ab': 1, 'LingLk': 1, 'comn': 1, 'Mad': 1, 'er': 1, 'seal': 1, 'denotes': 1, 'ith': 1, 'appearanc': 1, 'addressi': 1, 'cover': 1, 'ts': 1, 'Milady': 1, 'onbefore': 1, 'urge': 1, 'continues': 1, 'mena': 1, 'Tomorrow': 1, 'Live': 1, 'Musketeers': 1, 'xyzou': 1, 'willthis': 1, 'instant': 1, 'deliv': 1, 'Can': 1, 'ole': 1, 'regiment': 1, 'doyou': 1, 'sai': 1, 'passing': 1, 'repassing': 1, 'xyzl': 1, 'honored': 1, 'specia': 1, 'ainexpressions': 1, 'pauLvogt': 1, 'xyzreusewhich': 1, 'singularly': 1, 'fatigued': 1, 'f': 1, 'campson': 1, 'xyzds': 1, 'Aramis': 1, 'Porthos': 1, 'little': 1, 'sha': 1, 'xyznd': 1, 'conversation': 1, 're': 1, 'thoopes': 1, 'xyzheardthe': 1, 'whistling': 1, 'Buckinghamand': 1, 'English': 1, 'cnemies': 1, 'yo': 1, 'commandington': 1, 'ncoadc': 1, 'xyzwere': 1, 'frequently': 1, 'supported': 1, 'xyzom': 1, 'friends': 1, 'But': 1, 'mo': 1, 'rime': 1, 'inFrance': 1, 'thecrummer': 1, 'adangerou': 1, 'hand': 1, 'manojparuLekar': 1, 'xyzle': 1, 'thata': 1, 'confes': 1, 'thos': 1, 'talked': 1, 'discre': 1, 'tbwatkins': 1, 'xyzhat': 1, 'inhis': 1, 'projects': 1, 'ambi': 1, 'jLsmith': 1, 'xyzDe': 1, 'Wardes': 1, 'cr': 1, 'nsieur': 1, 'de': 1, 'Buckingham': 1, 'donkk': 1, 'xyzecks': 1, 'horses': 1, 'Wel': 1, 'Madame': 1, 'Bonacieux': 1, 'rele': 1, 'gnan': 1, 'rec': 1, 'law': 1, 'Monsieur': 1, 'Desses': 1, 'xyzward': 1, 'peixun8': 1, 'xyzning': 1, 'papers': 1, 'peared': 1, 'egan': 1, 'lay': 1, 'hold': 1, 'upon': 1, 'ersistence': 1, 'proved': 1, 'sh': 1, 'executionerin': 1, 'cutting': 1, 'menare': 1, 'att': 1, 'wansktn': 1, 'cneling': 1, 'simplifying': 1, 'min': 1, 'des': 1, 'Bazinis': 1, 'ambitious': 1, 'bezwoft': 1, 'xyztted': 1, 'notcome': 1, 'heads': 1, 'qizhig': 1, 'comreaching': 1, 'secondsoldie': 1, 'replied': 1, 'Artagna': 1, 'cnIt': 1, 'however': 1, 'ed': 1, 'byhis': 1, 'eternal': 1, 'infideli': 1, 'xyzD': 1, 'residence': 1, 'met': 1, 'ordajdeurb': 1, 'xyznher': 1, 'drawing': 1, 'arm': 1, 'cure': 1, 'damage': 1, 'eyesight': 1, 'rob': 1, 'porter': 1, 'burley': 1, 'idaho': 1, 'driver': 1, 'license': 1, 'sarah': 1, 'clawson': 1, 'alabama': 1, 'healthy': 1, 'avoid': 1, 'reading': 1, 'glass': 1, 'macular': 1, 'degeneration': 1, 'blind': 1, 'decline': 1, 'matter': 1, 'alderwood': 1, 'augusta': 1, 'sc': 1, 'rainbow': 1, 'bridge': 1, 'rain': 1, 'heed': 1, 'warn': 1, 'surprisingly': 1, 'seat': 1, 'vacant': 1, 'positive': 1, 'sixth': 1, 'actually': 1, 'seventh': 1, 'legos': 1, 'sudden': 1, 'rainstorm': 1, 'crocodile': 1, 'mutiny': 1, 'queen': 1, 'delight': 1, 'sea': 1, 'cucumber': 1, 'juice': 1, 'understood': 1, 'grief': 1, 'love': 1, 'cloud': 1, 'form': 1, 'beautiful': 1, 'animal': 1, 'eventually': 1, 'tornado': 1, 'wreak': 1, 'havoc': 1, 'walk': 1, 'along': 1, 'gutter': 1, 'facemasks': 1, 'butt': 1, 'interest': 1, 'perspective': 1, 'flat': 1, 'bitter': 1, 'radish': 1, 'curious': 1, 'blimp': 1, 'appear': 1, 'overnight': 1, 'wore': 1, 'paragraph': 1, 'beauty': 1, 'sunset': 1, 'obscure': 1, 'industrial': 1, 'crane': 1, 'million': 1, 'bathing': 1, 'suit': 1, 'wear': 1, 'son': 1, 'quip': 1, 'candy': 1, 'door': 1, 'swung': 1, 'open': 1, 'reveal': 1, 'giraffe': 1, 'elephant': 1, 'gloves': 1, 'excess': 1, 'geriatric': 1, 'yr': 1, 'ball': 1, 'hummingbird': 1, 'wing': 1, 'blur': 1, 'eagerly': 1, 'sip': 1, 'sugar': 1, 'feeder': 1, 'warm': 1, 'squirrel': 1, 'rustle': 1, 'gym': 1, 'bag': 1, 'generally': 1, 'approve': 1, 'pick': 1, 'trash': 1, 'spare': 1, 'dump': 1, 'neighbor': 1, 'yard': 1, 'extinct': 1, 'stairway': 1, 'heaven': 1, 'highway': 1, 'hell': 1, 'sound': 1, 'list': 1, 'pat': 1, 'order': 1, 'ghost': 1, 'pepper': 1, 'pie': 1, 'cheat': 1, 'bird': 1, 'ride': 1, 'urgent': 1, 'center': 1, 'flood': 1, 'deadly': 1, 'public': 1, 'surprised': 1, 'immense': 1, 'laziness': 1, 'inspirational': 1, 'others': 1, 'green': 1, 'smell': 1, 'tranquil': 1, 'rotten': 1, 'group': 1, 'toxic': 1, 'waste': 1, 'effective': 1, 'barrier': 1, 'zombie': 1, 'brick': 1, 'hi': 1, 'hello': 1, 'xiamen': 1, 'china': 1, 'hope': 1, 'manufacturer': 1, 'plastic': 1, 'extrusion': 1, 'inform': 1, 'america': 1, 'ge': 1, 'walmart': 1, 'ikea': 1, 'free': 1, 'competitive': 1, 'price': 1, 'pvc': 1, 'usd2': 1, 'kgs': 1, 'excellent': 1, 'size': 1, 'color': 1, 'logo': 1, 'customize': 1, 'appreciate': 1, 'assign': 1, 'responsible': 1, 'hear': 1, 'soon': 1, 'regard': 1, 'inside': 1, 'harm': 1, 'limb': 1, 'organ': 1, 'cell': 1, 'slowly': 1, 'murder': 1, 'cause': 1, 'faster': 1, 'normal': 1, 'cripple': 1, 'assassin': 1, 'raise': 1, 'stress': 1, 'anxiety': 1, 'drain': 1, 'strength': 1, 'weaken': 1, 'erection': 1, 'muscle': 1, 'mass': 1, 'soft': 1, 'squishy': 1, 'flab': 1, 'away': 1, 'aspect': 1, 'parasite': 1, 'rampant': 1, 'epidemic': 1, 'among': 1, 'research': 1, 'forget': 1, 'injection': 1, 'pill': 1, 'cream': 1, 'second': 1, 'skyrocket': 1, 'restores': 1, 'truly': 1, 'behind': 1, 'discover': 1, 'even': 1, 'amaze': 1, 'happiness': 1, 'rave': 1, 'jam': 1, 'k': 1, 'kansas': 1, 'city': 1, 'symptom': 1, 'vanish': 1, 'couple': 1, 'frisky': 1, 'teenager': 1, 'miss': 1, 'utube': 1, 'steve': 1, 'chad': 1, 'jawed': 1, 'trio': 1, 'paypal': 1, 'enriched': 1, 'buy': 1, 'ebay': 1, 'design': 1, 'indiana': 1, 'pennsylvania': 1, 'science': 1, 'together': 1, 'illinois': 1, 'urbana': 1, 'champaign': 1, 'multiple': 1, 'accord': 1, 'experience': 1, 'shot': 1, 'apartment': 1, 'san': 1, 'francisco': 1, 'attend': 1, 'deny': 1, 'occurred': 1, 'comment': 1, 'probably': 1, 'strengthen': 1, 'market': 1, 'digestible': 1, 'inspiration': 1, 'janet': 1, 'jackson': 1, 'role': 1, 'super': 1, 'bowl': 1, 'incident': 1, 'breast': 1, 'expose': 1, 'performance': 1, 'later': 1, 'indian': 1, 'tsunami': 1, 'easily': 1, 'clip': 1, 'original': 1, 'version': 1, 'influence': 1, 'website': 1, 'hot': 1, 'created': 1, 'post': 1, 'craigslist': 1, 'attractive': 1, 'upload': 1, 'reward': 1, 'dating': 1, 'plan': 1, 'founder': 1, 'accept': 1, 'uploads': 1, 'Background': 1, '背': 1, '景': 1, '介': 1, '紹': 1, '知識': 1, '經營': 1, '財務人員': 1, '統一': 1, '平臺': 1, '達成': 1, '溝通': 1, '建立起': 1, '現金流': 1, '效率': 1, '規避': 1, '管理工具': 1, '內部管理': 1, 'Outline': 1, '綱': 1, '整體': 1, '觀': 1, '決策': 1, '數字': 1, '審視': 1, '資產': 1, '負債表': 1, '利潤表': 1, '表對': 1, '日常': 1, '現實': 1, '稅收': 1, '比率': 1, '壞賬': 1, '減值': 1, '固定資產': 1, '折舊': 1, '試試': 1, '會計': 1, '切入點': 1, '原因': 1, '表中': 1, '途徑': 1, '佔用': 1, '手段': 1, '指標': 1, '解析': 1, '越多越': 1, '應付賬款': 1, '內涵': 1, '付款': 1, '週期': 1, '越長': 1, '正': 1, '能量': 1, '獲利': 1, '職業': 1, '經理人': 1, '回報': 1, '分紅': 1, '激勵': 1, '最大化': 1, '矛盾': 1, '責任': 1, '中心': 1, '零基': 1, '跟蹤': 1, '事前': 1, '事': 1, '事後': 1, '利用': 1, '建立': 1, '完善': 1, '體系': 1, '間接': 1, '固定成本': 1, '變動': 1, '保本': 1, '點': 1, '不善': 1, '價值鏈': 1, '定價': 1, '策略': 1, '日本': 1, '豐田': 1, '汽車': 1, 'Speaker': 1, 'William': 1, 'Wu': 1, '管理學': 1, '金融': 1, '碩士': 1, '經歷': 1, '現': 1, '香港': 1, '曾任': 1, '多家': 1, '知名企業': 1, '中國區': 1, '歐洲': 1, '工廠': 1, '管理方面': 1, '擁有': 1, '外企': 1, '管理工作': 1, '世界': 1, '強的': 1, '歐美': 1, '新加坡': 1, '包裝': 1, '石油': 1, '能源行業': 1, '億年': 1, '產值': 1, '進出口': 1, '貿易': 1, '耐用': 1, 'OEM': 1, '加工': 1, '第一家': 1, '新興': 1, '醫學': 1, '實驗室': 1, '服務性': 1, '現代農業': 1, '稅法': 1, '國際': 1, '會計準則': 1, 'IFRS': 1, '企業財務': 1, '內部': 1, '成本覈算': 1, '運營': 1, '管理效率': 1, '並對': 1, '生產型': 1, '外部': 1, 'Registration': 1, '預訂': 1, '填寫': 1, '回執': 1, '李': 1, 'Andy': 1, 'Li': 1, '手機號': 1}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import operator\n",
    "class_number = {'SPAM':0, 'EDM':1, 'HAM':2}\n",
    "model.eval()\n",
    "\n",
    "softmax=torch.nn.Softmax()\n",
    "# predict_list = []\n",
    "# score_list = []\n",
    "# sm_score_list = []\n",
    "\n",
    "edm_dict = {}\n",
    "\n",
    "# with open('right_edm.csv', newline='', encoding='utf-8') as csvfile:\n",
    "csvfile = pd.read_csv('right_spam.csv')[:10]   # change the value up to the colab limit\n",
    "    # rows = csv.reader(csvfile)\n",
    "for i, token_list in enumerate(csvfile['context']):\n",
    "    # print(token_list)\n",
    "    token_list = token_list.split()\n",
    "    predict_list = []\n",
    "    score_list = []\n",
    "    sm_score_list = []\n",
    "    for token in token_list:\n",
    "\n",
    "        tokenized_input = tokenizer(token,\n",
    "                                    max_length=20,\n",
    "                                    truncation=True,\n",
    "                                    return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            input_items = {key: val.to(device) for key, val in tokenized_input.items()}\n",
    "    #         del input_items['token_type_ids'] ## bart不需要這個\n",
    "\n",
    "            outputs = model(**input_items)\n",
    "            prediction = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "            prediction = int(prediction)\n",
    "            sm = softmax(outputs.logits[0])\n",
    "\n",
    "            predict_list.append(prediction)\n",
    "            score_list.append(outputs.logits[0][prediction].item())\n",
    "            sm_score_list.append(sm[prediction].item())\n",
    "\n",
    "\n",
    "    dic = {}\n",
    "    for key in predict_list:\n",
    "        dic[key] = dic.get(key, 0) + 1\n",
    "    # print(dic)\n",
    "\n",
    "    # print(predict_list)\n",
    "    # print(score_list)\n",
    "    # print(sm_score_list)\n",
    "    for num, j in enumerate(predict_list):\n",
    "        if j == 0:\n",
    "            # print(token_list[num])\n",
    "            if token_list[num] not in edm_dict.keys():\n",
    "                edm_dict[str(token_list[num])] = 1\n",
    "            else:\n",
    "                edm_dict[str(token_list[num])] += 1\n",
    "    # print(\"--------\")\n",
    "\n",
    "print(dict(sorted(edm_dict.items(), key=operator.itemgetter(1),reverse=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1821f05f",
   "metadata": {},
   "source": [
    "## token clissifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0000b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "產品 中試 管理 樣品 量產 Product pilot management sample mass production    上海    深圳     北京  元 課程 背景 企業 提供 研發 管理 諮詢服務 過程 中 發現 企業 新 產品開發 樣機 量產 過程 中 產品化 過程 着 新品 中試 中試 時間 短 製造 部門 戲稱 研發 新品 三無 產品 生產 文件 工裝 生產 現場 沒人管 轉產 標準 研發 想 快點 轉產 生產 產品 不願 接收 希望 研發 解決 轉過來 市場 催得 急 被迫 接收 長此以往 導致 研發 生產 矛盾激化 企業 成立 中試 部門 希望 中試 階段 產品質量 解決掉 中試 位與 運作 困惑 發生 質量 進度 衝突 時 取捨 平衡 研發 製造 矛盾轉化 研發 中試 中試 生產 矛盾 中試 成 矛盾 集散 中心 市場 壓力 並不因 中試 減少 中試 哪些方面 努力 產品 質量 進度 中試 業務 面向 研發 面向 製造 兼而有之 量產 發現 產品 製造 性差 成品率 低 返工 影響 發貨 產品 生產 後還 發生 設計變更 產品 客戶 手中 還冒出 各種各樣 研發 人員 救火 課程 多年 實踐 長期 研發 諮詢 積累 一套 理論 實踐 相結合 操作 方法 配以 案例 指導 研發 試產 製造 部門 主管 高效 產品 樣品 走向 量產 參加 對象 企業 CEO 總經理 研發 總經理 副總 測試 部經理 中試 試產 部經理 製造 部經理 工藝 工程 部經理 質量 部經理 項目 經理 產品 經理 高級 製造 工程師 培訓 收益 業界 公司 發展 階段 產品 中試 管理模式 實踐 面向 製造 系統 產品設計 DFM 方法 實施 過程 面向 生產 測試 產品設計 DFT 方法 實施 過程 面向 製造 系統 新 產品 驗證 過程 方法 質量 標準 前提 縮短 產品 試製 週期 方法 技巧 建立 樣品 量產 管理機制 課程 大綱 案例 研討 樣品 量產 概述 企業 追求 技術 樣品 產品 商品 研發 製造 矛盾  製造 系統 面對 研發 三無 產品  研發 面對 製造 系統 越來越 高 門檻 研發 製造矛盾 激化 中試 成爲 中試 位與 發展  研發 RD 中試 D P 生產 P 關係  中試 使命  中試 定位  中試 發展 大而全 b 專業化 分工 c 產品線 劃分 共享 平臺 d 中試 人員 發展 定位 廣度 深度 中試 業務範圍  中試 業務 新 產品 導入 NPI  承上 面向 產品 研發  啓 面向 產品 製造  橋樑 中試 作爲 連接 研發 製造 橋樑 獨木橋 陽關 道 演練 討論  企業 情況 建立 發展 中試 職能 新 產品 導入 團隊 新 產品 導入 團隊  工藝 工程  設備 工程  測試 工程  工業 工程  產品 驗證  試生產 計劃 生產 質量 新 產品 導入 團隊 職責 新 產品 導入 團隊 產品開發 團隊 關係  開發 模式 演變 串行 變 並行  並行工程 產品開發 中 體現  新 產品 導入 團隊 提前 介入 研發 提前 介入 b 提前 介入 c 提前 介入 做  新 產品 導入 團隊 管理 新 產品 導入 團隊 產品開發 團隊 職能部門 溝通 b 新 產品 導入 團隊 成員 彙報 考覈 管理機制 演練 討論  企業 情況 研討 建立 新 產品 導入 團隊 時機 面向 製造 系統 產品設計 DFM 產品設計 開發 過程 中 進行 製造 性 設計  製造 角度 產品設計  工藝 人員 介入 產品開發 過程 切入點 立項  工藝 管理 三個 階段 工藝 設計 工藝 調製 驗證 工藝 管制  工藝 設計 提出 製造 性需求 b 典型 工藝 規範 c 製造 性需求 落實 產品設計 方案 中 d 工藝 設計 產品設計 並行 e 產品 工藝流程 設計 f 電裝 整裝 包裝 物流 製造 性 設計 分析 g 確保 製造 性需求 產品開發 中 h 工藝 評審 操作 工裝 j 開發 過程 中 同步 輸出 工藝 文件 生產 操作 指導 文件  工藝 調製 驗證 工藝 驗證 時機 b 工藝 驗證 方案 包括 內容 c 實施 工藝 驗證 d 工藝 驗證 報告 內容 e 推動 工藝 驗證 解決 f 研發 人員 配合 新 產品 工藝 驗證 g 製造 包 模式 工藝 驗證  工藝 管制 工藝 管制 困惑 救火 盡頭 b 工藝 轉產 評審 標準 流程 責任 c 量產 過程 中 例行 監控 異常 管理 演練 討論  分析 學員 企業 工藝 管理工作 程度 差距 工藝 管理 平臺 建設  負責 工藝 平臺 建設  工藝 委員會 責任 運作 模式  進行 工藝 規劃  基礎 工藝 研究  支撐 工藝 管理 平臺 四大 規範 品質 規範 b 設備 規範 c 工藝 規範 d 設計規劃  工藝 管理 部門 推動 DFM 業務  工藝 體系 組織 發展 演變  工藝 人員 培養 技能 提升 面向 生產 測試 產品設計 DFT 產品 生命週期 全 流程 測試 策略  研發 測試 Alpha 試驗局 測試 Beta 生產 測試 研發 測試 Alpha BETA 測試  測試人員 介入 產品開發 過程 時機 提可 測試性 需求 時機  測試性 需求 內容 示例  單元測試 模塊 測試 系統集成 測試 專業化 測試 BETA 測試 重點 分析  產品開發 過程 中 測試 業務流程 分析  企業 發展 階段 測試 相關 工作 短平快 項目 測試 工作 面向 生產 測試 業務 產品設計 開發  生產 測試 業務流程 分析  典型 部品 測試 整機 測試方法 介紹  開發 生產 測試 工裝 條件 分析  生產 測試 工裝 開發 管理  產品開發 過程 中 實施 面向 生產 測試 產品設計 提出 測試性 需求 b 測試性 需求 落實 產品設計 方案 中 c 研發 面對 衆多 需求 取捨 測試性 需求 優先級 分析 d 產品開發 過程 中 同步 開發 生產 測試 工裝 e 產品開發 過程 中 同步 輸出 生產 測試 操作 指導 文件 f 進行 測試 工裝 驗證 g 推動 測試 驗證 解決  推動 測試性 設計 DFT 業務  進行 測試 平臺 建設 演練 討論  分析 學員 企業 DFT 工作 程度 差距 改進 產品 試製 驗證 管理 影響 產品 試製 週期 因素 分析 研發 人員 試製 提供 支持 試製 團隊 職責 定位 設置 試製 部門 時機 優缺點 分析 試製 人員 介入 產品開發 過程 時機  進行 試製 要素 示例 面向 製造 系統 驗證  研發 人員 試製 過程 中 進行 產品設計 優化  製造 系統 驗證 策略 計劃  製造 系統 驗證 方案  實施 製造 系統 驗證 工藝 驗證 工藝流程 工藝 路線 單板 工藝 整機 工藝 包裝 工藝 物流 工藝 b 工裝 驗證 裝配 工裝 測試 工裝 生產 設備 c 結構 驗證 d 產品 數據 驗證 BOM 驗證 製造 文檔 驗證 e 產品 試製 驗證 質量 效率 成本  批次 驗證 報告 驗證 批才 合適  推動 驗證 解決 轉產 評審  研發 人員 支持 新 產品 轉產 工作  轉產 評審 評審 組織  評審 標準  判定 轉產  評審 流程 運作 機制 產品 轉 產後 管理  新 產品 試製 效果 評價  新 產品 質量 目標 達成 情況  工程 變更 管理  缺陷 管理  質量 審計 演練 討論  分析 學員 企業 產品 試製 驗證 過程 分析 差距 提出 改進 建議 講師 介紹 Charles 老師 研發 諮詢 資深 顧問 國家 發改委 創新 管理 培訓中心 特邀 講師 清華大學 國際 工程項目 管理 研究院 特邀 講師 專業 背景  高科技 企業 從業 背景 產品策劃 產品 研發 產品 中試 產品 服務 領域 實踐 管理 經驗 產品設計 開發 NPI 項目 經理 產品 經理 研發 管理部 經理 企業 管理 顧問 職務 國內 著名 通信 設備 公司 工作   期間 國際 頂尖 諮詢 顧問 工作 作爲 核心成員 全程 參與 推動 公司 研發 管理體系 變革 工作 作爲 產品 經理 主導 產品線 多個 大型項目 產品設計 開發 中試 轉產 上市 工作 報 回 執 報名 填寫 回執 發送到 郵箱 微信 課程名稱 課程 場次 公司 姓名 職位 手機 郵箱 電話 聯 系 李 華 Andy Li 電 話    郵 箱 baomingzixun  com 報 回 執 發 送 郵 箱 勿 直 接 回 復 微信 諮詢 添加 微信號 微信 手機號 退訂 unsubscribetd  com 發送 郵件 至此 郵箱\n"
     ]
    }
   ],
   "source": [
    "test_num = 10\n",
    "test_list = pd.read_csv('right_spam.csv')\n",
    "# subject = test_list[test_num]['subject']\n",
    "print(test_list['context'][10])\n",
    "context = test_list['context'][test_num]\n",
    "label = test_list['ref'][test_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2b21f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "內文:  產品 中試 管理 樣品 量產 Product pilot management sample mass production    上海    深圳     北京  元 課程 背景 企業 提供 研發 管理 諮詢服務 過程 中 發現 企業 新 產品開發 樣機 量產 過程 中 產品化 過程 着 新品 中試 中試 時間 短 製造 部門 戲稱 研發 新品 三無 產品 生產 文件 工裝 生產 現場 沒人管 轉產 標準 研發 想 快點 轉產 生產 產品 不願 接收 希望 研發 解決 轉過來 市場 催得 急 被迫 接收 長此以往 導致 研發 生產 矛盾激化 企業 成立 中試 部門 希望 中試 階段 產品質量 解決掉 中試 位與 運作 困惑 發生 質量 進度 衝突 時 取捨 平衡 研發 製造 矛盾轉化 研發 中試 中試 生產 矛盾 中試 成 矛盾 集散 中心 市場 壓力 並不因 中試 減少 中試 哪些方面 努力 產品 質量 進度 中試 業務 面向 研發 面向 製造 兼而有之 量產 發現 產品 製造 性差 成品率 低 返工 影響 發貨 產品 生產 後還 發生 設計變更 產品 客戶 手中 還冒出 各種各樣 研發 人員 救火 課程 多年 實踐 長期 研發 諮詢 積累 一套 理論 實踐 相結合 操作 方法 配以 案例 指導 研發 試產 製造 部門 主管 高效 產品 樣品 走向 量產 參加 對象 企業 CEO 總經理 研發 總經理 副總 測試 部經理 中試 試產 部經理 製造 部經理 工藝 工程 部經理 質量 部經理 項目 經理 產品 經理 高級 製造 工程師 培訓 收益 業界 公司 發展 階段 產品 中試 管理模式 實踐 面向 製造 系統 產品設計 DFM 方法 實施 過程 面向 生產 測試 產品設計 DFT 方法 實施 過程 面向 製造 系統 新 產品 驗證 過程 方法 質量 標準 前提 縮短 產品 試製 週期 方法 技巧 建立 樣品 量產 管理機制 課程 大綱 案例 研討 樣品 量產 概述 企業 追求 技術 樣品 產品 商品 研發 製造 矛盾  製造 系統 面對 研發 三無 產品  研發 面對 製造 系統 越來越 高 門檻 研發 製造矛盾 激化 中試 成爲 中試 位與 發展  研發 RD 中試 D P 生產 P 關係  中試 使命  中試 定位  中試 發展 大而全 b 專業化 分工 c 產品線 劃分 共享 平臺 d 中試 人員 發展 定位 廣度 深度 中試 業務範圍  中試 業務 新 產品 導入 NPI  承上 面向 產品 研發  啓 面向 產品 製造  橋樑 中試 作爲 連接 研發 製造 橋樑 獨木橋 陽關 道 演練 討論  企業 情況 建立 發展 中試 職能 新 產品 導入 團隊 新 產品 導入 團隊  工藝 工程  設備 工程  測試 工程  工業 工程  產品 驗證  試生產 計劃 生產 質量 新 產品 導入 團隊 職責 新 產品 導入 團隊 產品開發 團隊 關係  開發 模式 演變 串行 變 並行  並行工程 產品開發 中 體現  新 產品 導入 團隊 提前 介入 研發 提前 介入 b 提前 介入 c 提前 介入 做  新 產品 導入 團隊 管理 新 產品 導入 團隊 產品開發 團隊 職能部門 溝通 b 新 產品 導入 團隊 成員 彙報 考覈 管理機制 演練 討論  企業 情況 研討 建立 新 產品 導入 團隊 時機 面向 製造 系統 產品設計 DFM 產品設計 開發 過程 中 進行 製造 性 設計  製造 角度 產品設計  工藝 人員 介入 產品開發 過程 切入點 立項  工藝 管理 三個 階段 工藝 設計 工藝 調製 驗證 工藝 管制  工藝 設計 提出 製造 性需求 b 典型 工藝 規範 c 製造 性需求 落實 產品設計 方案 中 d 工藝 設計 產品設計 並行 e 產品 工藝流程 設計 f 電裝 整裝 包裝 物流 製造 性 設計 分析 g 確保 製造 性需求 產品開發 中 h 工藝 評審 操作 工裝 j 開發 過程 中 同步 輸出 工藝 文件 生產 操作 指導 文件  工藝 調製 驗證 工藝 驗證 時機 b 工藝 驗證 方案 包括 內容 c 實施 工藝 驗證 d 工藝 驗證 報告 內容 e 推動 工藝 驗證 解決 f 研發 人員 配合 新 產品 工藝 驗證 g 製造 包 模式 工藝 驗證  工藝 管制 工藝 管制 困惑 救火 盡頭 b 工藝 轉產 評審 標準 流程 責任 c 量產 過程 中 例行 監控 異常 管理 演練 討論  分析 學員 企業 工藝 管理工作 程度 差距 工藝 管理 平臺 建設  負責 工藝 平臺 建設  工藝 委員會 責任 運作 模式  進行 工藝 規劃  基礎 工藝 研究  支撐 工藝 管理 平臺 四大 規範 品質 規範 b 設備 規範 c 工藝 規範 d 設計規劃  工藝 管理 部門 推動 DFM 業務  工藝 體系 組織 發展 演變  工藝 人員 培養 技能 提升 面向 生產 測試 產品設計 DFT 產品 生命週期 全 流程 測試 策略  研發 測試 Alpha 試驗局 測試 Beta 生產 測試 研發 測試 Alpha BETA 測試  測試人員 介入 產品開發 過程 時機 提可 測試性 需求 時機  測試性 需求 內容 示例  單元測試 模塊 測試 系統集成 測試 專業化 測試 BETA 測試 重點 分析  產品開發 過程 中 測試 業務流程 分析  企業 發展 階段 測試 相關 工作 短平快 項目 測試 工作 面向 生產 測試 業務 產品設計 開發  生產 測試 業務流程 分析  典型 部品 測試 整機 測試方法 介紹  開發 生產 測試 工裝 條件 分析  生產 測試 工裝 開發 管理  產品開發 過程 中 實施 面向 生產 測試 產品設計 提出 測試性 需求 b 測試性 需求 落實 產品設計 方案 中 c 研發 面對 衆多 需求 取捨 測試性 需求 優先級 分析 d 產品開發 過程 中 同步 開發 生產 測試 工裝 e 產品開發 過程 中 同步 輸出 生產 測試 操作 指導 文件 f 進行 測試 工裝 驗證 g 推動 測試 驗證 解決  推動 測試性 設計 DFT 業務  進行 測試 平臺 建設 演練 討論  分析 學員 企業 DFT 工作 程度 差距 改進 產品 試製 驗證 管理 影響 產品 試製 週期 因素 分析 研發 人員 試製 提供 支持 試製 團隊 職責 定位 設置 試製 部門 時機 優缺點 分析 試製 人員 介入 產品開發 過程 時機  進行 試製 要素 示例 面向 製造 系統 驗證  研發 人員 試製 過程 中 進行 產品設計 優化  製造 系統 驗證 策略 計劃  製造 系統 驗證 方案  實施 製造 系統 驗證 工藝 驗證 工藝流程 工藝 路線 單板 工藝 整機 工藝 包裝 工藝 物流 工藝 b 工裝 驗證 裝配 工裝 測試 工裝 生產 設備 c 結構 驗證 d 產品 數據 驗證 BOM 驗證 製造 文檔 驗證 e 產品 試製 驗證 質量 效率 成本  批次 驗證 報告 驗證 批才 合適  推動 驗證 解決 轉產 評審  研發 人員 支持 新 產品 轉產 工作  轉產 評審 評審 組織  評審 標準  判定 轉產  評審 流程 運作 機制 產品 轉 產後 管理  新 產品 試製 效果 評價  新 產品 質量 目標 達成 情況  工程 變更 管理  缺陷 管理  質量 審計 演練 討論  分析 學員 企業 產品 試製 驗證 過程 分析 差距 提出 改進 建議 講師 介紹 Charles 老師 研發 諮詢 資深 顧問 國家 發改委 創新 管理 培訓中心 特邀 講師 清華大學 國際 工程項目 管理 研究院 特邀 講師 專業 背景  高科技 企業 從業 背景 產品策劃 產品 研發 產品 中試 產品 服務 領域 實踐 管理 經驗 產品設計 開發 NPI 項目 經理 產品 經理 研發 管理部 經理 企業 管理 顧問 職務 國內 著名 通信 設備 公司 工作   期間 國際 頂尖 諮詢 顧問 工作 作爲 核心成員 全程 參與 推動 公司 研發 管理體系 變革 工作 作爲 產品 經理 主導 產品線 多個 大型項目 產品設計 開發 中試 轉產 上市 工作 報 回 執 報名 填寫 回執 發送到 郵箱 微信 課程名稱 課程 場次 公司 姓名 職位 手機 郵箱 電話 聯 系 李 華 Andy Li 電 話    郵 箱 baomingzixun  com 報 回 執 發 送 郵 箱 勿 直 接 回 復 微信 諮詢 添加 微信號 微信 手機號 退訂 unsubscribetd  com 發送 郵件 至此 郵箱\n",
      "label:  0 SPAM\n",
      "predict:  0 SPAM\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(context,\n",
    "                            max_length=512,\n",
    "                            truncation=True,\n",
    "                            return_tensors=\"pt\")\n",
    "class_number = {'SPAM':0, 'EDM':1, 'HAM':2}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input_items = {key: val.to(device) for key, val in tokenized_input.items()}\n",
    "    del input_items['token_type_ids'] ## bart不需要這個\n",
    "    \n",
    "    outputs = model(**input_items)\n",
    "    prediction = outputs.logits.argmax(dim=-1)\n",
    "    print(type(int(prediction)))\n",
    "    \n",
    "    \n",
    "    # print('主旨: ', subjzect)\n",
    "    print('內文: ', context)\n",
    "    print('label: ', class_number[label], label)\n",
    "    print('predict: ', int(prediction), list(class_number.keys())[list(class_number.values()).index(int(prediction))])\n",
    "    \n",
    "    # if int(prediction) == 0:\n",
    "    #     print('predict: ham')\n",
    "    # else:\n",
    "    #     print('predict: spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a741e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
